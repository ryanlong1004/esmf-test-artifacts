build time -- 2022-03-27 01:26:07.374531
20220327 015824.554 INFO             PET0 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220327 015824.554 INFO             PET0 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220327 015824.554 INFO             PET0 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220327 015824.554 INFO             PET0 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220327 015824.554 INFO             PET0 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220327 015824.554 INFO             PET0 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220327 015824.554 INFO             PET0 Running with ESMF Version   : v8.3.0b10-105-ga5295e34ae
20220327 015824.554 INFO             PET0 ESMF library build date/time: "Mar 27 2022" "01:23:54"
20220327 015824.554 INFO             PET0 ESMF library build location : /gpfsm/dnb04/projects/p98/mpotts/esmf/intel_19.1.3_intelmpi_O_develop
20220327 015824.554 INFO             PET0 ESMF_COMM                   : intelmpi
20220327 015824.554 INFO             PET0 ESMF_MOAB                   : enabled
20220327 015824.554 INFO             PET0 ESMF_LAPACK                 : enabled
20220327 015824.554 INFO             PET0 ESMF_NETCDF                 : enabled
20220327 015824.554 INFO             PET0 ESMF_PNETCDF                : disabled
20220327 015824.554 INFO             PET0 ESMF_PIO                    : enabled
20220327 015824.554 INFO             PET0 ESMF_YAMLCPP                : enabled
20220327 015824.555 INFO             PET0 --- VMK::logSystem() start -------------------------------
20220327 015824.555 INFO             PET0 esmfComm=intelmpi
20220327 015824.555 INFO             PET0 isPthreadsEnabled=1
20220327 015824.555 INFO             PET0 isOpenMPEnabled=1
20220327 015824.555 INFO             PET0 isOpenACCEnabled=0
20220327 015824.556 INFO             PET0 isSsiSharedMemoryEnabled=1
20220327 015824.556 INFO             PET0 ssiCount=1 peCount=6
20220327 015824.556 INFO             PET0 PE=0 SSI=0 SSIPE=0
20220327 015824.556 INFO             PET0 PE=1 SSI=0 SSIPE=1
20220327 015824.556 INFO             PET0 PE=2 SSI=0 SSIPE=2
20220327 015824.556 INFO             PET0 PE=3 SSI=0 SSIPE=3
20220327 015824.556 INFO             PET0 PE=4 SSI=0 SSIPE=4
20220327 015824.556 INFO             PET0 PE=5 SSI=0 SSIPE=5
20220327 015824.556 INFO             PET0 --- VMK::logSystem() MPI Control Variables ---------------
20220327 015824.556 INFO             PET0 index=   0                                          I_MPI_DEBUG_OUTPUT : @Default:# not defined
20220327 015824.556 INFO             PET0 index=   1                                        I_MPI_DEBUG_COREDUMP : @Default:# 1
20220327 015824.556 INFO             PET0 index=   2                                          I_MPI_LIBRARY_KIND : @Default:# not defined
20220327 015824.556 INFO             PET0 index=   3                                  I_MPI_OFI_LIBRARY_INTERNAL : @Default:# not defined
20220327 015824.556 INFO             PET0 index=   4                                            I_MPI_CC_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET0 index=   5                                           I_MPI_CXX_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET0 index=   6                                            I_MPI_FC_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET0 index=   7                                           I_MPI_F77_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET0 index=   8                                           I_MPI_F90_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET0 index=   9                                         I_MPI_TRACE_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  10                                         I_MPI_CHECK_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  11                                        I_MPI_CHECK_COMPILER : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  12                                                    I_MPI_CC : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  13                                                   I_MPI_CXX : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  14                                                    I_MPI_FC : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  15                                                   I_MPI_F90 : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  16                                                   I_MPI_F77 : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  17                                                  I_MPI_ROOT : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  18                                           I_MPI_ONEAPI_ROOT : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  19                                           MPIR_CVAR_VT_ROOT : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  20                                   I_MPI_COMPILER_CONFIG_DIR : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  21                                                  I_MPI_LINK : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  22                                      I_MPI_DEBUG_INFO_STRIP : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  23                                                I_MPI_CFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  24                                              I_MPI_CXXFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  25                                               I_MPI_FCFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  26                                                I_MPI_FFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  27                                               I_MPI_LDFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  28                                             I_MPI_FORT_BIND : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  29                                           I_MPI_AUTH_METHOD : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  30                               I_MPI_HYDRA_COLLECTIVE_LAUNCH : @Default:# 1
20220327 015824.556 INFO             PET0 index=  31                                  I_MPI_HYDRA_UNIQUE_PROXIES : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  32                                        I_MPI_FAULT_CONTINUE : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  33                                   I_MPI_FAULT_NODE_CONTINUE : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  34                                                I_MPI_MPIRUN : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  35                                            I_MPI_BIND_ORDER : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  36                                             I_MPI_BIND_NUMA : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  37                                     I_MPI_BIND_WIN_ALLOCATE : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  38                                      I_MPI_HYDRA_NAMESERVER : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  39                                        I_MPI_JOB_CHECK_LIBS : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  40                                    I_MPI_HYDRA_SERVICE_PORT : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  41                                           I_MPI_HYDRA_DEBUG : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  42                                             I_MPI_HYDRA_ENV : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  43                                           I_MPI_JOB_TIMEOUT : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  44                                       I_MPI_MPIEXEC_TIMEOUT : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  45                                   I_MPI_JOB_STARTUP_TIMEOUT : Set this environment variable to make mpiexec.hydra terminate$ the job in <timeout> seconds if some processes are not launched. @Default:# -1
20220327 015824.556 INFO             PET0 index=  46                                    I_MPI_JOB_TIMEOUT_SIGNAL : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  47                                      I_MPI_JOB_ABORT_SIGNAL : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  48                                I_MPI_JOB_SIGNAL_PROPAGATION : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  49                                  I_MPI_HYDRA_BOOTSTRAP_EXEC : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  50                       I_MPI_HYDRA_BOOTSTRAP_EXEC_EXTRA_ARGS : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  51                              I_MPI_HYDRA_BOOTSTRAP_AUTOFORK : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  52                                             I_MPI_HYDRA_RMK : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  53                                     I_MPI_HYDRA_PMI_CONNECT : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  54                                         I_MPI_HYDRA_TOPOLIB : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  55                                            I_MPI_PORT_RANGE : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  56                         I_MPI_JOB_RESPECT_PROCESS_PLACEMENT : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  57                                                I_MPI_TMPDIR : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  58                                           I_MPI_HYDRA_DEMUX : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  59                                           I_MPI_HYDRA_IFACE : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  60                                I_MPI_HYDRA_GDB_REMOTE_SHELL : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  61                                   I_MPI_HYDRA_PMI_AGGREGATE : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  62                                        I_MPI_JOB_TRACE_LIBS : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  63                                       I_MPI_HYDRA_HOST_FILE : Set the host file to run the application.$ Syntax$ I_MPI_HYDRA_HOST_FILE=<arg>$ Arguments$ <arg> - String parameter$ -----------------------------------------------------------------------$ <hostsfile> - The full or relative path to the host file$ ----------------------------------------------------------------------- @Default:# not defined
20220327 015824.556 INFO             PET0 index=  64                                     I_MPI_HYDRA_HOSTS_GROUP : This environment variable allows to set node ranges using brackets,$ commas, and dashes (like in Slurm* Workload Manager). @Default:# not defined
20220327 015824.556 INFO             PET0 index=  65                                               I_MPI_PERHOST : Define the default behavior for the -perhost option of the mpiexec.hydra$ command.$ Syntax$ I_MPI_PERHOST=<value>$ Arguments$ <value> - Define a value used for -perhost by default$ -----------------------------------------------------------------------$ <integer > 0> - Exact value for the option$ <all>         - All logical CPUs on the node$ <allcores>    - All cores (physical CPUs) on the node. This is the$  default value.$ ----------------------------------------------------------------------- @Default:# not defined
20220327 015824.556 INFO             PET0 index=  66                                                 I_MPI_GTOOL : Specify the tools to be launched for selected ranks. An alternative to$ this variable is the -gtool option$ Syntax$ I_MPI_GTOOL="<command line for a tool 1>:<ranks set 1>[=exclusive]$ [@arch 1];<command line for a tool 2>:<ranks set 2>[=exclusive]$ [@arch 2]; â€¦ ;<command line for a tool n>:<ranks set n>[=exclusive]$ [@arch n]"$ Arguments$ <arg> - Specify a tool launch command, including parameters.$ -----------------------------------------------------------------------$ <command line>     - Specify tool launch command, including parameters$ <rank set>         - Specify the range of ranks that are involved in the$  tool execution. Separate ranks with a comma or use the '-' symbol for$ a set ofcontiguous ranks. To run the tool forall ranks, use the all$  argument.$ [=exclusive]       - All cores (physical CPUs) on the node. This is$ the default value.$ [@arch]            - Specify the architecture on which the tool runs$  optional). For a given <rank set>, if you specify this argument,$ the tool is launched
20220327 015824.556 INFO             PET0 index=  67                                    I_MPI_HYDRA_BRANCH_COUNT : Set this environment variable to restrict the number of child management$ processes launched by the mpiexec.hydra operation or by each pmi_proxy$ anagement process.$ Syntax$ I_MPI_HYDRA_BRANCH_COUNT=<num>$ Arguments$ <value> - Number$ -----------------------------------------------------------------------$ <n> >= 0 - The default value is -1 if less than 128 nodes are used. $ This value also means that there is no hierarchical structure$ The default value is 32 if more than 127 nodes are used$ ----------------------------------------------------------------------- @Default:# not defined
20220327 015824.556 INFO             PET0 index=  68                                       I_MPI_HYDRA_BOOTSTRAP : Set this environment variable to specify the bootstrap server Syntax$ I_MPI_HYDRA_BOOTSTRAP=<arg>$ Arguments$ <arg> - String parameter$ -----------------------------------------------------------------------$ <ssh>     - Use secure shell. This is the default value$ <rsh>     - Use remote shell$ <pdsh>    - Use parallel distributed shell$ <pbsdsh>  - Use Torque* and PBS* pbsdsh command$ <fork>    - Use fork call$ <slurm>   - Use SLURM* srun command$ <ll>      - Use LoadLeveler* llspawn.stdio command$ <lsf>     - Use LSF* blaunch command$ <sge>     - Use Univa* Grid Engine* qrsh command$ ----------------------------------------------------------------------- @Default:# not defined
20220327 015824.556 INFO             PET0 index=  69                                              I_MPI_PIN_UNIT : @Default:# not defined
20220327 015824.556 INFO             PET0 index=  70                                                 I_MPI_STATS :
20220327 015824.556 INFO             PET0 index=  71                                             I_MPI_TIMER_ART : @Default:# 1
20220327 015824.556 INFO             PET0 index=  72                                   I_MPI_OFFLOAD_DOMAIN_SIZE : Control the number of devices/tiles per MPI rank
20220327 015824.556 INFO             PET0 index=  73                                          I_MPI_OFFLOAD_CELL : Variable to choose the base unit: tile or device
20220327 015824.556 INFO             PET0 index=  74                                       I_MPI_OFFLOAD_DEVICES : Variable to select available devices
20220327 015824.556 INFO             PET0 index=  75                                   I_MPI_OFFLOAD_DEVICE_LIST : A comma-separated list of tiles and/or ranges of tiles.$ The process with the i-th rank is pinned to the i-th tile in the list.$ If I_MPI_OFFLOAD_CELL=device then it is comma-separated list of devices.
20220327 015824.556 INFO             PET0 index=  76                                        I_MPI_OFFLOAD_DOMAIN : Define domains through the comma separated list of hexadecimal numbers (domain masks).
20220327 015824.556 INFO             PET0 index=  77                               I_MPI_OFFLOAD_INFO_SET_NTILES : Set the number of tiles
20220327 015824.556 INFO             PET0 index=  78                                I_MPI_OFFLOAD_INFO_SET_NGPUS : Set the number of gpus
20220327 015824.556 INFO             PET0 index=  79                           I_MPI_OFFLOAD_INFO_SET_NNUMANODES : Set the number of numanodes
20220327 015824.557 INFO             PET0 index=  80                               I_MPI_OFFLOAD_INFO_SET_GPU_ID : Set gpu id for each tile
20220327 015824.557 INFO             PET0 index=  81                 I_MPI_OFFLOAD_INFO_SET_NUMANODE_ID_FOR_GPUS : Set numanode id for each gpu
20220327 015824.557 INFO             PET0 index=  82                I_MPI_OFFLOAD_INFO_SET_NUMANODE_ID_FOR_RANKS : Set numanode id for each rank
20220327 015824.557 INFO             PET0 index=  83                            I_MPI_OFFLOAD_INFO_SET_VENDOR_ID : Set vendor id for each device
20220327 015824.557 INFO             PET0 index=  84                                         MPIR_CVAR_INTEL_PIN : Turn on/off process pinning.$ Syntax$ I_MPI_PIN=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 - Enable process pinning$ > disable | no | off | 0 - Disable processes pinning$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN @Default:# on
20220327 015824.557 INFO             PET0 index=  85                          MPIR_CVAR_INTEL_PIN_SHOW_REAL_MASK : Turn on/off real masks pinning print.$ Syntax$ I_MPI_PIN_SHOW_REAL_MASK=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 - Enable real pinning print$ > disable | no | off | 0 - Disable real pinning print$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_SHOW_REAL_MASK @Default:# on
20220327 015824.557 INFO             PET0 index=  86                          MPIR_CVAR_INTEL_PIN_PROCESSOR_LIST : Define a processor subset and the mapping rules for MPI processes within$ this subset.$ Syntax$ I_MPI_PIN_PROCESSOR_LIST=<value>$ The environment variable value has the following syntax forms:$ 1. <proclist>$ 2. [<procset>][:[grain=<grain>][,shift=<shift>]$ [,preoffset=<preoffset>][,postoffset=<postoffset>]$ 3. [<procset>][:map=<map>]$ @Alias:# I_MPI_PIN_PROCESSOR_LIST @Default:# not defined
20220327 015824.557 INFO             PET0 index=  87                  MPIR_CVAR_INTEL_PIN_PROCESSOR_EXCLUDE_LIST : Define a subset of logical processors to be excluded for the pinning$ capability on the intended hosts.$ Syntax$ I_MPI_PIN_PROCESSOR_EXCLUDE_LIST=<proclist>$ Arguments$ <proclist> - A comma-separated list of logical processor numbers$ and/or ranges of processors.$ -----------------------------------------------------------------------$ > <l> - Processor with logical number <l>.$ > <l>-<m> - Range of processors with logical numbers from <l> to <m>.$ > <k>,<l>-<m> - Processors <k>, as well as <l> through <m>.$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_PROCESSOR_EXCLUDE_LIST @Default:# not defined
20220327 015824.557 INFO             PET0 index=  88                                    MPIR_CVAR_INTEL_PIN_CELL : Set this environment variable to define the pinning resolution$ granularity. I_MPI_PIN_CELL specifies the minimal processor cell$ allocated when an MPI process is running.$ Syntax$ I_MPI_PIN_CELL=<cell>$ Arguments$ <cell> - Specify the resolution granularity$ -----------------------------------------------------------------------$ > unit - Basic processor unit (logical CPU)$ > core - Physical processor core$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_CELL @Default:# unit
20220327 015824.557 INFO             PET0 index=  89                          MPIR_CVAR_INTEL_PIN_RESPECT_CPUSET : Respect the process affinity mask.$ Syntax$ I_MPI_PIN_RESPECT_CPUSET=<value>$ Arguments$ <value> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 - Respect the process affinity mask$ > disable | no | off | 0 - Do not respect the process affinity mask$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_RESPECT_CPUSET @Default:# on
20220327 015824.557 INFO             PET0 index=  90                             MPIR_CVAR_INTEL_PIN_RESPECT_HCA : In the presence of Intel(R) Omni-Path Architecture (Intel(R) OPA) or$ Infiniband architecture* host channel adapter (IBA* HCA),$ adjust the pinning according to the location of adapter.$ Syntax$ I_MPI_PIN_RESPECT_HCA=<value>$ Arguments$ <value> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 Use the location of IBA HCA if available$ > disable | no | off | 0 Do not use the location of IBA HCA$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_RESPECT_HCA @Default:# on
20220327 015824.557 INFO             PET0 index=  91                                  MPIR_CVAR_INTEL_PIN_DOMAIN : Intel(R) MPI Library provides environment variable to control process pinning for hybrid MPI/OpenMP* applications. This environment variable is used to define a number of non-overlapping subsets (domains) of logical processors on a node, and a set of rules on how MPI processes are bound to these domains by the following formula: one MPI process per one domain. Multi-core Shape:$ I_MPI_PIN_DOMAIN=<mc-shape>$ <mc-shape> - Define domains through multi-core terms.$ -----------------------------------------------------------------------$ > core - Each domain consists of the logical processors that share a$ particular core. The number of domains on a node is equal to the number$ of cores on the node.$ > socket | sock - Each domain consists of the logical processors that$ share a particular socket. The number of domains on a node is equal to$ the number of sockets on the node.$ > numa - Each domain consists of the logical processors that share a$ particular NUMA node. The number of domains on a machine is equal to$
20220327 015824.557 INFO             PET0 index=  92                                   MPIR_CVAR_INTEL_PIN_ORDER : Set this environment variable to define the mapping order for MPI$ processes to domains as specified by the$ I_MPI_PIN_DOMAIN environment variable.$ Syntax$ I_MPI_PIN_ORDER=<order>$ <order> - Specify the ranking order$ -----------------------------------------------------------------------$ > range - The domains are ordered according to the processor's BIOS$ numbering. This is a platform dependent numbering$ > scatter - The domains are ordered so that adjacent domains have$ minimal sharing of common resources$ > compact - The domains are ordered so that adjacent domains share$ common resources as much as possible. This is the default value$ > spread - The domains are ordered consecutively with the possibility$ not to share common resources$ > bunch - The processes are mapped proportionally to sockets and the$ domains are ordered as close as possible on the sockets$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_ORDER @Default:# compact
20220327 015824.557 INFO             PET0 index=  93                                   MPIR_CVAR_IMPI_HBW_POLICY : @Alias:# I_MPI_HBW_POLICY
20220327 015824.557 INFO             PET0 index=  94                          MPIR_CVAR_IMPI_INTERNAL_MEM_POLICY : @Alias:# I_MPI_INTERNAL_MEM_POLICY
20220327 015824.557 INFO             PET0 index=  95                                 MPIR_CVAR_IMPI_STATIC_BUILD : @Alias:# I_MPI_STATIC_BUILD
20220327 015824.557 INFO             PET0 index=  96                     MPIR_CVAR_IMPI_RETURN_INTERNAL_MEM_NUMA : @Alias:# I_MPI_RETURN_INTERNAL_MEM_NUMA
20220327 015824.557 INFO             PET0 index=  97                                   MPIR_CVAR_OFFLOAD_TOPOLIB : @Alias:# I_MPI_OFFLOAD_TOPOLIB
20220327 015824.557 INFO             PET0 index=  98                                        MPIR_CVAR_ENABLE_GPU : Control support of buffers offloaded to GPU/accelerator in MPI calls.$ Syntax$ I_MPI_OFFLOAD=<value>$ Arguments$ <value> - choice$ -----------------------------------------------------------------------$ <0> - Disabled$ <1> - Enabled only if Level Zero library is loaded at MPI_Init() time$ <2> - Enabled, will fail if Level Zero library is not loadable$ ----------------------------------------------------------------------- @Alias:# I_MPI_OFFLOAD @Default:# 0
20220327 015824.557 INFO             PET0 index=  99                        MPIR_CVAR_ENABLE_GPU_BUFFER_CHECKING : Turn on/off bounds checking of the offloaded buffers.$ @Alias:# I_MPI_OFFLOAD_BUFFER_CHECKING @Default:# 1
20220327 015824.557 INFO             PET0 index= 100                        MPIR_CVAR_OFFLOAD_LEVEL_ZERO_LIBRARY : Specify name or full path to Level Zero ze_loader library.$ @Alias:# I_MPI_OFFLOAD_LEVEL_ZERO_LIBRARY
20220327 015824.557 INFO             PET0 index= 101                            MPIR_CVAR_INTEL_EXTRA_FILESYSTEM : Turn on/off native parallel file systems support.$ Syntax$ I_MPI_EXTRA_FILESYSTEM=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 - Enable native support for parallel file$ systems.$ > disable | no | off | 0 - Disable native support for parallel file$ systems.$ ----------------------------------------------------------------------- @Alias:# I_MPI_EXTRA_FILESYSTEM @Default:# off
20220327 015824.557 INFO             PET0 index= 102                      MPIR_CVAR_INTEL_EXTRA_FILESYSTEM_FORCE : Force filesystem recognition logic.$ Syntax$ I_MPI_EXTRA_FILESYSTEM_FORCE=<ufs|nfs|gpfs|panfs|lustre|daos>$ @Alias:# I_MPI_EXTRA_FILESYSTEM_FORCE @Default:# not defined
20220327 015824.557 INFO             PET0 index= 103                                     MPIR_CVAR_INTEL_FABRICS : Select the particular fabrics to be used.$ Syntax$ I_MPI_FABRICS=<ofi|shm:ofi>$ Arguments$ <fabric> -  Define a network fabric.$ -----------------------------------------------------------------------$ > shm - Shared memory transport (used for intra-node$ communication only).$ > ofi - OpenFabrics Interfaces* (OFI)-capable network fabrics, such as$ Intel(R) True Scale Fabric, Intel(R) Omni-Path Architecture, InfiniBand*$ and Ethernet (through OFI$ API).$ ----------------------------------------------------------------------- @Alias:# I_MPI_FABRICS @Default:# shm:ofi
20220327 015824.557 INFO             PET0 index= 104                                       MPIR_CVAR_IMPI_MALLOC : Enable or disable the Intel MPI private memory allocator.$ ----------------------------------------------------------------------- @Alias:# I_MPI_MALLOC @Default:# 1
20220327 015824.557 INFO             PET0 index= 105                                    MPIR_CVAR_INTEL_SHM_HEAP : Enable or disable the Intel MPI shared memory allocator.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP @Default:# -1
20220327 015824.557 INFO             PET0 index= 106                                MPIR_CVAR_INTEL_SHM_HEAP_OPT : Shared memory heap optimization: "rank", "numa".$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_OPT @Default:# -1
20220327 015824.557 INFO             PET0 index= 107                              MPIR_CVAR_INTEL_SHM_HEAP_VSIZE : Set shared memory heap virtual size (in MB).$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_VSIZE @Default:# -1
20220327 015824.557 INFO             PET0 index= 108                              MPIR_CVAR_INTEL_SHM_HEAP_CSIZE : Set shared memory heap cache size (in MB).$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_CSIZE @Default:# -1
20220327 015824.557 INFO             PET0 index= 109                  MPIR_CVAR_INTEL_SHM_HEAP_NCONTIG_THRESHOLD : Set non-contig object size threshold for use shared memory allocator.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_NCONTIG_THRESHOLD @Default:# -1
20220327 015824.557 INFO             PET0 index= 110                          MPIR_CVAR_INTEL_SHM_HEAP_THRESHOLD : Set object size threshold for use shared memory allocator.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_THRESHOLD @Default:# -1
20220327 015824.557 INFO             PET0 index= 111        MPIR_CVAR_INTEL_SHM_RECV_HEAP_SHORT_MEMCPY_THRESHOLD : Threshold for short size messages receive via SHM HEAP transport.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_HEAP_SHORT_MEMCPY_THRESHOLD @Default:# -1
20220327 015824.557 INFO             PET0 index= 112      MPIR_CVAR_INTEL_SHM_RECV_HEAP_REGULAR_MEMCPY_THRESHOLD : Threshold for regular size message receive via SHM HEAP transport.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_HEAP_REGULAR_MEMCPY_THRESHOLD @Default:# -1
20220327 015824.557 INFO             PET0 index= 113            MPIR_CVAR_INTEL_SHM_RECV_INTER_HEAP_SHORT_MEMCPY : Name of memory copy function for short message receive via SHM HEAP transport, inter socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTER_HEAP_SHORT_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET0 index= 114            MPIR_CVAR_INTEL_SHM_RECV_INTRA_HEAP_SHORT_MEMCPY : Name of memory copy function for short message receive via SHM HEAP transport, intra socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTRA_HEAP_SHORT_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET0 index= 115          MPIR_CVAR_INTEL_SHM_RECV_INTER_HEAP_REGULAR_MEMCPY : Name of memory copy function for regular receive messages via SHM HEAP transport, inter socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTER_HEAP_REGULAR_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET0 index= 116          MPIR_CVAR_INTEL_SHM_RECV_INTRA_HEAP_REGULAR_MEMCPY : Name of memory copy function for regular receive messages via SHM HEAP transport, intra socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTRA_HEAP_REGULAR_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET0 index= 117                  MPIR_CVAR_INTEL_SHM_RECV_INTER_HEAP_MEMCPY : Name of memory copy function for receive messages via SHM HEAP transport, inter socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTER_HEAP_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET0 index= 118                  MPIR_CVAR_INTEL_SHM_RECV_INTRA_HEAP_MEMCPY : Name of memory copy function for receive messages via SHM HEAP transport, intra socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTRA_HEAP_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET0 index= 119                               MPIR_CVAR_CH4_SHM_POSIX_EAGER : Select a shared memory transport to be used.$ Syntax$ I_MPI_SHM=<transport>$ Arguments$ <transport> - Define a shared memory transport solution.$ -----------------------------------------------------------------------$ > disable | no | off | 0 - Do not use shared memory transport.$ > auto - Select a shared memory transport solution automatically.$ > bdw_sse - The shared memory transport solution tuned for Intel(R)$ microarchitecture code name Broadwell. The SSE/SSE2/SSE3 instruction$ set is used.$ > bdw_avx2 - The shared memory transport solution tuned for Intel(R)$ microarchitecture code name Broadwell. The AVX2 instruction set is used.$ > skx_sse - The shared memory transport solution tuned for Intel(R)$ Xeon(R) processors based on Intel(R) microarchitecture code name Skylake.$ The SSE/SSE2/SSE3 instruction set is used.$ > skx_avx2 - The shared memory transport solution tuned for Intel(R)$ Xeon(R) processors based on Intel(R) microarchitecture code name Skylake.$ The AVX2 instruction set is used.$ > skx_av
20220327 015824.557 INFO             PET0 index= 120                                     MPIR_CVAR_INTEL_SHM_OPT : Select a shared memory transport optimization strategy to be used.$ Syntax$ I_MPI_SHM_OPT=<optimization_strategy>$ Arguments$ <optimization_strategy> - Define a shared memory transport optimization strategy.$ -----------------------------------------------------------------------$ > dynamic - Let shared memory transport make decision in runtime.$ > intra - Optimize intra socket message passing.$ > inter - Optimize inter socket message passing.$ -----------------------------------------------------------------------$ @Alias:# I_MPI_SHM_OPT @Default:# dynamic
20220327 015824.557 INFO             PET0 index= 121                           MPIR_CVAR_INTEL_SHM_CELL_FWD_SIZE : Change the size of a shared memory forward cell. @Alias:# I_MPI_SHM_CELL_FWD_SIZE @Default:# -1
20220327 015824.557 INFO             PET0 index= 122                           MPIR_CVAR_INTEL_SHM_CELL_BWD_SIZE : Change the size of a shared memory backward cell. @Alias:# I_MPI_SHM_CELL_BWD_SIZE @Default:# -1
20220327 015824.557 INFO             PET0 index= 123                           MPIR_CVAR_INTEL_SHM_CELL_EXT_SIZE : Change the size of a shared memory extended cell. @Alias:# I_MPI_SHM_CELL_EXT_SIZE @Default:# -1
20220327 015824.557 INFO             PET0 index= 124                            MPIR_CVAR_INTEL_SHM_CELL_FWD_NUM : Change the number of forward cells in the shared memory transport (per rank). @Alias:# I_MPI_SHM_CELL_FWD_NUM @Default:# -1
20220327 015824.557 INFO             PET0 index= 125                       MPIR_CVAR_INTEL_SHM_CELL_FWD_HOLD_NUM : Change the number of hold forward cells in the shared memory transport (per rank). @Alias:# I_MPI_SHM_CELL_FWD_HOLD_NUM @Default:# -1
20220327 015824.557 INFO             PET0 index= 126                            MPIR_CVAR_INTEL_SHM_CELL_BWD_NUM : Change the number of backward cells in the shared memory transport (per rank). @Alias:# I_MPI_SHM_CELL_BWD_NUM @Default:# -1
20220327 015824.557 INFO             PET0 index= 127                     MPIR_CVAR_INTEL_SHM_CELL_BWD_NUMA_AWARE : Use NUMA aware backward cells (1 : true, 0 : false, -1 : auto) (per rank). @Alias:# I_MPI_SHM_CELL_BWD_NUMA_AWARE @Default:# -1
20220327 015824.557 INFO             PET0 index= 128                      MPIR_CVAR_INTEL_SHM_CELL_EXT_NUM_TOTAL : Change the total number of extended cells in the shared memory$ transport. @Alias:# I_MPI_SHM_CELL_EXT_NUM_TOTAL @Default:# -1
20220327 015824.557 INFO             PET0 index= 129                            MPIR_CVAR_INTEL_SHM_MCDRAM_LIMIT : Change the size of the shared memory bound to the multi-channel DRAM (MCDRAM) (size per rank). @Alias:# I_MPI_SHM_MCDRAM_LIMIT @Default:# -1
20220327 015824.557 INFO             PET0 index= 130                         MPIR_CVAR_INTEL_SHM_SEND_SPIN_COUNT : Control the spin count value for the shared memory transport for sending messages. @Alias:# I_MPI_SHM_SEND_SPIN_COUNT @Default:# -1
20220327 015824.557 INFO             PET0 index= 131                         MPIR_CVAR_INTEL_SHM_RECV_SPIN_COUNT : Control the spin count value for the shared memory transport for receiving messages. @Alias:# I_MPI_SHM_RECV_SPIN_COUNT @Default:# -1
20220327 015824.557 INFO             PET0 index= 132                          MPIR_CVAR_INTEL_SHM_FILE_PREFIX_4K : Mount point of 4K page size shm files. @Alias:# I_MPI_SHM_FILE_PREFIX_4K @Default:# ""
20220327 015824.557 INFO             PET0 index= 133                          MPIR_CVAR_INTEL_SHM_FILE_PREFIX_2M : Mount point of 2M huge page size shm files. @Alias:# I_MPI_SHM_FILE_PREFIX_2M @Default:# ""
20220327 015824.557 INFO             PET0 index= 134                          MPIR_CVAR_INTEL_SHM_FILE_PREFIX_1G : Mount point of 1G huge page size shm files. @Alias:# I_MPI_SHM_FILE_PREFIX_1G @Default:# ""
20220327 015824.557 INFO             PET0 index= 135                                   MPIR_CVAR_INTEL_SHM_PAUSE : The number of pauses repeated. @Alias:# I_MPI_SHM_PAUSE
20220327 015824.557 INFO             PET0 index= 136                         MPIR_CVAR_INTEL_SHM_EAGER_THRESHOLD : Eager threshold. @Alias:# I_MPI_SHM_EAGER_THRESHOLD
20220327 015824.557 INFO             PET0 index= 137                               MPIR_CVAR_INTEL_SHM_RING_SIZE : @Alias:# I_MPI_SHM_RING_SIZE
20220327 015824.557 INFO             PET0 index= 138                      MPIR_CVAR_INTEL_SHM_RING_ACK_THRESHOLD : @Alias:# I_MPI_SHM_RING_ACK_THRESHOLD
20220327 015824.557 INFO             PET0 index= 139                          MPIR_CVAR_INTEL_SHM_CELL_FWD_FIRST : @Alias:# I_MPI_SHM_CELL_FWD_FIRST
20220327 015824.557 INFO             PET0 index= 140                      MPIR_CVAR_INTEL_SHM_PROFILER_DIRECTORY : @Alias:# I_MPI_SHM_PROFILER_DIRECTORY
20220327 015824.557 INFO             PET0 index= 141                         MPIR_CVAR_INTEL_SHM_TRACE_DIRECTORY : @Alias:# I_MPI_SHM_TRACE_DIRECTORY
20220327 015824.557 INFO             PET0 index= 142                              MPIR_CVAR_INTEL_SHM_FRAME_SIZE : @Alias:# I_MPI_SHM_FRAME_SIZE
20220327 015824.557 INFO             PET0 index= 143                         MPIR_CVAR_INTEL_SHM_FRAME_THRESHOLD : @Alias:# I_MPI_SHM_FRAME_THRESHOLD
20220327 015824.557 INFO             PET0 index= 144                        MPIR_CVAR_INTEL_SHM_SEND_TINY_MEMCPY : @Alias:# I_MPI_SHM_SEND_TINY_MEMCPY
20220327 015824.557 INFO             PET0 index= 145                  MPIR_CVAR_INTEL_SHM_SEND_INTRA_RING_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_RING_MEMCPY
20220327 015824.557 INFO             PET0 index= 146                  MPIR_CVAR_INTEL_SHM_SEND_INTER_RING_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_RING_MEMCPY
20220327 015824.557 INFO             PET0 index= 147                  MPIR_CVAR_INTEL_SHM_RECV_INTRA_RING_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_RING_MEMCPY
20220327 015824.557 INFO             PET0 index= 148                  MPIR_CVAR_INTEL_SHM_RECV_INTER_RING_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_RING_MEMCPY
20220327 015824.557 INFO             PET0 index= 149              MPIR_CVAR_INTEL_SHM_SEND_INTRA_CELL_FWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_CELL_FWD_MEMCPY
20220327 015824.557 INFO             PET0 index= 150              MPIR_CVAR_INTEL_SHM_SEND_INTER_CELL_FWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_CELL_FWD_MEMCPY
20220327 015824.557 INFO             PET0 index= 151              MPIR_CVAR_INTEL_SHM_SEND_INTRA_CELL_BWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_CELL_BWD_MEMCPY
20220327 015824.557 INFO             PET0 index= 152              MPIR_CVAR_INTEL_SHM_SEND_INTER_CELL_BWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_CELL_BWD_MEMCPY
20220327 015824.557 INFO             PET0 index= 153                  MPIR_CVAR_INTEL_SHM_RECV_INTRA_CELL_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_CELL_MEMCPY
20220327 015824.557 INFO             PET0 index= 154                  MPIR_CVAR_INTEL_SHM_RECV_INTER_CELL_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_CELL_MEMCPY
20220327 015824.557 INFO             PET0 index= 155          MPIR_CVAR_INTEL_SHM_RECV_INTRA_CELL_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_CELL_REGULAR_MEMCPY
20220327 015824.557 INFO             PET0 index= 156          MPIR_CVAR_INTEL_SHM_RECV_INTER_CELL_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_CELL_REGULAR_MEMCPY
20220327 015824.557 INFO             PET0 index= 157             MPIR_CVAR_INTEL_SHM_SEND_INTRA_FRAME_FWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_FRAME_FWD_MEMCPY
20220327 015824.557 INFO             PET0 index= 158             MPIR_CVAR_INTEL_SHM_SEND_INTER_FRAME_FWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_FRAME_FWD_MEMCPY
20220327 015824.557 INFO             PET0 index= 159             MPIR_CVAR_INTEL_SHM_SEND_INTRA_FRAME_BWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_FRAME_BWD_MEMCPY
20220327 015824.557 INFO             PET0 index= 160             MPIR_CVAR_INTEL_SHM_SEND_INTER_FRAME_BWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_FRAME_BWD_MEMCPY
20220327 015824.557 INFO             PET0 index= 161                 MPIR_CVAR_INTEL_SHM_RECV_INTRA_FRAME_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_FRAME_MEMCPY
20220327 015824.557 INFO             PET0 index= 162                 MPIR_CVAR_INTEL_SHM_RECV_INTER_FRAME_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_FRAME_MEMCPY
20220327 015824.557 INFO             PET0 index= 163     MPIR_CVAR_INTEL_SHM_SEND_INTRA_FRAME_FWD_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_FRAME_FWD_REGULAR_MEMCPY
20220327 015824.557 INFO             PET0 index= 164     MPIR_CVAR_INTEL_SHM_SEND_INTER_FRAME_FWD_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_FRAME_FWD_REGULAR_MEMCPY
20220327 015824.557 INFO             PET0 index= 165     MPIR_CVAR_INTEL_SHM_SEND_INTRA_FRAME_BWD_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_FRAME_BWD_REGULAR_MEMCPY
20220327 015824.557 INFO             PET0 index= 166     MPIR_CVAR_INTEL_SHM_SEND_INTER_FRAME_BWD_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_FRAME_BWD_REGULAR_MEMCPY
20220327 015824.557 INFO             PET0 index= 167         MPIR_CVAR_INTEL_SHM_RECV_INTRA_FRAME_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_FRAME_REGULAR_MEMCPY
20220327 015824.557 INFO             PET0 index= 168         MPIR_CVAR_INTEL_SHM_RECV_INTER_FRAME_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_FRAME_REGULAR_MEMCPY
20220327 015824.557 INFO             PET0 index= 169                       MPIR_CVAR_INTEL_SHM_INPLACE_THRESHOLD : @Alias:# I_MPI_SHM_INPLACE_THRESHOLD
20220327 015824.557 INFO             PET0 index= 170              MPIR_CVAR_INTEL_SHM_SEND_TINY_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_TINY_MEMCPY_THRESHOLD
20220327 015824.557 INFO             PET0 index= 171      MPIR_CVAR_INTEL_SHM_RECV_CELL_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_RECV_CELL_REGULAR_MEMCPY_THRESHOLD
20220327 015824.557 INFO             PET0 index= 172MPIR_CVAR_INTEL_SHM_SEND_INTER_UNIDIR_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_INTER_UNIDIR_REGULAR_MEMCPY_THRESHOLD
20220327 015824.557 INFO             PET0 index= 173MPIR_CVAR_INTEL_SHM_SEND_INTER_BIDIR_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_INTER_BIDIR_REGULAR_MEMCPY_THRESHOLD
20220327 015824.557 INFO             PET0 index= 174MPIR_CVAR_INTEL_SHM_SEND_INTRA_UNIDIR_REGULAR_MEMCPY_THRESHOLD_MIN : @Alias:# I_MPI_SHM_SEND_INTRA_UNIDIR_REGULAR_MEMCPY_THRESHOLD_MIN
20220327 015824.557 INFO             PET0 index= 175MPIR_CVAR_INTEL_SHM_SEND_INTRA_UNIDIR_REGULAR_MEMCPY_THRESHOLD_MAX : @Alias:# I_MPI_SHM_SEND_INTRA_UNIDIR_REGULAR_MEMCPY_THRESHOLD_MAX
20220327 015824.557 INFO             PET0 index= 176MPIR_CVAR_INTEL_SHM_SEND_INTRA_BIDIR_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_INTRA_BIDIR_REGULAR_MEMCPY_THRESHOLD
20220327 015824.558 INFO             PET0 index= 177     MPIR_CVAR_INTEL_SHM_RECV_FRAME_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_RECV_FRAME_REGULAR_MEMCPY_THRESHOLD
20220327 015824.558 INFO             PET0 index= 178          MPIR_CVAR_INTEL_SHM_CELL_FWD_UNIDIR_INTRA_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_UNIDIR_INTRA_CAPACITY
20220327 015824.558 INFO             PET0 index= 179  MPIR_CVAR_INTEL_SHM_CELL_FWD_UNIDIR_INTER_REGULAR_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_UNIDIR_INTER_REGULAR_CAPACITY
20220327 015824.558 INFO             PET0 index= 180MPIR_CVAR_INTEL_SHM_CELL_FWD_UNIDIR_INTER_NONTEMPORAL_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_UNIDIR_INTER_NONTEMPORAL_CAPACITY
20220327 015824.558 INFO             PET0 index= 181           MPIR_CVAR_INTEL_SHM_CELL_FWD_BIDIR_INTRA_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_BIDIR_INTRA_CAPACITY
20220327 015824.558 INFO             PET0 index= 182   MPIR_CVAR_INTEL_SHM_CELL_FWD_BIDIR_INTER_REGULAR_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_BIDIR_INTER_REGULAR_CAPACITY
20220327 015824.558 INFO             PET0 index= 183MPIR_CVAR_INTEL_SHM_CELL_FWD_BIDIR_INTER_NONTEMPORAL_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_BIDIR_INTER_NONTEMPORAL_CAPACITY
20220327 015824.558 INFO             PET0 index= 184MPIR_CVAR_INTEL_SHM_SEND_FWD_FRAME_NONTEMPORAL_MEMCPY_THRESHOLD_MIN : @Alias:# I_MPI_SHM_SEND_FWD_FRAME_NONTEMPORAL_MEMCPY_THRESHOLD_MIN
20220327 015824.558 INFO             PET0 index= 185MPIR_CVAR_INTEL_SHM_SEND_FWD_FRAME_NONTEMPORAL_MEMCPY_THRESHOLD_MAX : @Alias:# I_MPI_SHM_SEND_FWD_FRAME_NONTEMPORAL_MEMCPY_THRESHOLD_MAX
20220327 015824.558 INFO             PET0 index= 186MPIR_CVAR_INTEL_SHM_SEND_FWD_CELL_NONTEMPORAL_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_FWD_CELL_NONTEMPORAL_MEMCPY_THRESHOLD
20220327 015824.558 INFO             PET0 index= 187                                MPIR_CVAR_INTEL_SHM_HEAP_THP : @Alias:# I_MPI_SHM_HEAP_THP
20220327 015824.558 INFO             PET0 index= 188                                     MPIR_CVAR_INTEL_SHM_THP : @Alias:# I_MPI_SHM_THP
20220327 015824.558 INFO             PET0 index= 189                                  MPIR_CVAR_OFI_USE_PROVIDER : Define the name of the OFI provider to load.$ Syntax$ I_MPI_OFI_PROVIDER=<name>$ Arguments$ <name> - The name of the OFI provider to load @Alias:# I_MPI_OFI_PROVIDER @Default:# not defined
20220327 015824.558 INFO             PET0 index= 190                                MPIR_CVAR_OFI_DUMP_PROVIDERS : Control the capability of printing information about all OFI providers$ and their attributes from an OFI library.$ Syntax$ I_MPI_OFI_PROVIDER_DUMP=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > yes | on | 1 - Print the list of all OFI providers and their$ attributes from an OFI library$ > no | off | 0 - No action$ ----------------------------------------------------------------------- @Alias:# I_MPI_OFI_PROVIDER_DUMP @Default:# off
20220327 015824.558 INFO             PET0 index= 191                        MPIR_CVAR_CH4_OFI_ENABLE_DIRECT_RECV : Control the capability of the direct receive in the OFI fabric.$ Syntax$ I_MPI_OFI_DRECV=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > 1 - Enable direct receive$ > 0 - Disable direct receive$ ----------------------------------------------------------------------- @Alias:# I_MPI_OFI_DRECV @Default:# not defined
20220327 015824.558 INFO             PET0 index= 192                                  MPIR_CVAR_OFI_MAX_MSG_SIZE : @Alias:# I_MPI_OFI_MAX_MSG_SIZE
20220327 015824.558 INFO             PET0 index= 193                                  MPIR_CVAR_OFI_LMT_WIN_SIZE : @Alias:# I_MPI_OFI_LMT_WIN_SIZE
20220327 015824.558 INFO             PET0 index= 194                    MPIR_CVAR_CH4_OFI_ISEND_INJECT_THRESHOLD : @Alias:# I_MPI_OFI_ISEND_INJECT_THRESHOLD
20220327 015824.558 INFO             PET0 index= 195                     MPIR_CVAR_CH4_OFI_ADDRESS_EXCHANGE_MODE : @Alias:# I_MPI_STARTUP_MODE
20220327 015824.558 INFO             PET0 index= 196                     MPIR_CVAR_CH4_OFI_LARGE_SCALE_THRESHOLD : @Alias:# I_MPI_LARGE_SCALE_THRESHOLD
20220327 015824.558 INFO             PET0 index= 197                   MPIR_CVAR_CH4_OFI_EXTREME_SCALE_THRESHOLD : @Alias:# I_MPI_EXTREME_SCALE_THRESHOLD
20220327 015824.558 INFO             PET0 index= 198                        MPIR_CVAR_CH4_OFI_DYNAMIC_CONNECTION : @Alias:# I_MPI_DYNAMIC_CONNECTION
20220327 015824.558 INFO             PET0 index= 199                              MPIR_CVAR_CH4_OFI_EXPERIMENTAL : @Alias:# I_MPI_OFI_EXPERIMENTAL @Default:# 0
20220327 015824.558 INFO             PET0 index= 200                     MPIR_CVAR_CH4_OFI_CAPABILITY_SETS_DEBUG : Prints out the configuration of each capability selected via the capability sets interface.
20220327 015824.558 INFO             PET0 index= 201                               MPIR_CVAR_CH4_OFI_ENABLE_DATA : Enable immediate data fields in OFI to transmit source rank outside of the match bits
20220327 015824.558 INFO             PET0 index= 202                           MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE : If true, the OFI addressing information will be stored with an FI_AV_TABLE. If false, an FI_AV_MAP will be used.
20220327 015824.558 INFO             PET0 index= 203                 MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS : If true, use OFI scalable endpoints.
20220327 015824.558 INFO             PET0 index= 204                             MPIR_CVAR_CH4_OFI_MAX_ENDPOINTS : Specifies the maximum number of OFI endpoints that can be used by the OFI provider. The default value is -1, indicating that no value is set.
20220327 015824.558 INFO             PET0 index= 205                        MPIR_CVAR_CH4_OFI_ENABLE_MR_SCALABLE : If true, MR_SCALABLE for OFI memory regions. If false, MR_BASIC for OFI memory regions.
20220327 015824.558 INFO             PET0 index= 206                             MPIR_CVAR_CH4_OFI_ENABLE_TAGGED : If true, use tagged message transmission functions in OFI.
20220327 015824.558 INFO             PET0 index= 207                                 MPIR_CVAR_CH4_OFI_ENABLE_AM : If true, enable OFI active message support.
20220327 015824.558 INFO             PET0 index= 208                                MPIR_CVAR_CH4_OFI_ENABLE_RMA : If true, enable OFI RMA support for MPI RMA operations. OFI support for basic RMA is always required to implement large messgage transfers in the active message code path.
20220327 015824.558 INFO             PET0 index= 209                            MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS : If true, enable OFI Atomics support.
20220327 015824.558 INFO             PET0 index= 210                       MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS : Specifies the maximum number of iovecs that can be used by the OFI provider for fetch_atomic operations. The default value is -1, indicating that no value is set.
20220327 015824.558 INFO             PET0 index= 211                 MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS : If true, enable MPI data auto progress.
20220327 015824.558 INFO             PET0 index= 212              MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS : If true, enable MPI control auto progress.
20220327 015824.558 INFO             PET0 index= 213                       MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK : If true, enable iovec for pt2pt.
20220327 015824.558 INFO             PET0 index= 214                                 MPIR_CVAR_CH4_OFI_RANK_BITS : Specifies the number of bits that will be used for matching the MPI rank. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20220327 015824.558 INFO             PET0 index= 215                                  MPIR_CVAR_CH4_OFI_TAG_BITS : Specifies the number of bits that will be used for matching the user tag. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20220327 015824.558 INFO             PET0 index= 216                             MPIR_CVAR_CH4_OFI_MAJOR_VERSION : Specifies the major version of the OFI library. The default is the major version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20220327 015824.558 INFO             PET0 index= 217                             MPIR_CVAR_CH4_OFI_MINOR_VERSION : Specifies the major version of the OFI library. The default is the minor version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20220327 015824.558 INFO             PET0 index= 218                             MPIR_CVAR_CH4_OFI_ZERO_OP_FLAGS : Zeroes rx_attr.op_flags and tx_attr.op_flags, disables use of FI_SELECTIVE_COMPLETION. Can give more optimized behavior of underlying provider.
20220327 015824.558 INFO             PET0 index= 219                            MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS : Specifies the number of buffers for receiving active messages.
20220327 015824.558 INFO             PET0 index= 220                        MPIR_CVAR_INTEL_COLL_DIRECT_PROGRESS : @Alias:# I_MPI_COLL_DIRECT_PROGRESS
20220327 015824.558 INFO             PET0 index= 221                                      MPIR_CVAR_ENABLE_HCOLL : @Alias:# I_MPI_COLL_EXTERNAL
20220327 015824.558 INFO             PET0 index= 222                                     MPIR_CVAR_USE_ALLGATHER : Control selection of MPI_Allgather algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-6 @Alias:# I_MPI_ADJUST_ALLGATHER @Default:# -1
20220327 015824.558 INFO             PET0 index= 223                                MPIR_CVAR_USE_ALLGATHER_LIST : @Alias:# I_MPI_ADJUST_ALLGATHER_LIST @Default:# -1
20220327 015824.558 INFO             PET0 index= 224                         MPIR_CVAR_USE_ALLGATHER_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLGATHER_COMPOSITION @Default:# -1
20220327 015824.558 INFO             PET0 index= 225               MPIR_CVAR_ALLGATHER_COMPOSITION_DELTA_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLGATHER_COMPOSITION_DELTA_SEGSIZE @Default:# -1
20220327 015824.558 INFO             PET0 index= 226                             MPIR_CVAR_USE_ALLGATHER_NETWORK : range: 0-5 @Alias:# I_MPI_ADJUST_ALLGATHER_NETWORK @Default:# -1
20220327 015824.558 INFO             PET0 index= 227                                MPIR_CVAR_USE_ALLGATHER_NODE : range: 0-4 @Alias:# I_MPI_ADJUST_ALLGATHER_NODE @Default:# -1
20220327 015824.558 INFO             PET0 index= 228                                    MPIR_CVAR_USE_ALLGATHERV : Control selection of MPI_Allgatherv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_ALLGATHERV @Default:# -1
20220327 015824.558 INFO             PET0 index= 229                               MPIR_CVAR_USE_ALLGATHERV_LIST : @Alias:# I_MPI_ADJUST_ALLGATHERV_LIST @Default:# -1
20220327 015824.558 INFO             PET0 index= 230                        MPIR_CVAR_USE_ALLGATHERV_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLGATHERV_COMPOSITION @Default:# -1
20220327 015824.558 INFO             PET0 index= 231                            MPIR_CVAR_USE_ALLGATHERV_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_ALLGATHERV_NETWORK @Default:# -1
20220327 015824.558 INFO             PET0 index= 232                               MPIR_CVAR_USE_ALLGATHERV_NODE : range: 0-3 @Alias:# I_MPI_ADJUST_ALLGATHERV_NODE @Default:# -1
20220327 015824.558 INFO             PET0 index= 233                   MPIR_CVAR_ALLGATHER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ALLGATHER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET0 index= 234                      MPIR_CVAR_ALLGATHER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ALLGATHER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET0 index= 235                    MPIR_CVAR_SCATTERV_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_SCATTERV_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET0 index= 236                       MPIR_CVAR_SCATTERV_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_SCATTERV_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET0 index= 237                     MPIR_CVAR_SCATTER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_SCATTER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET0 index= 238                        MPIR_CVAR_SCATTER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_SCATTER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET0 index= 239                                     MPIR_CVAR_USE_ALLREDUCE : Control selection of MPI_Allreduce algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-25 @Alias:# I_MPI_ADJUST_ALLREDUCE @Default:# -1
20220327 015824.558 INFO             PET0 index= 240                                MPIR_CVAR_USE_ALLREDUCE_LIST : @Alias:# I_MPI_ADJUST_ALLREDUCE_LIST @Default:# -1
20220327 015824.558 INFO             PET0 index= 241                         MPIR_CVAR_USE_ALLREDUCE_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLREDUCE_COMPOSITION @Default:# -1
20220327 015824.558 INFO             PET0 index= 242                             MPIR_CVAR_USE_ALLREDUCE_NETWORK : range: 0-16 @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK @Default:# -1
20220327 015824.558 INFO             PET0 index= 243                                MPIR_CVAR_USE_ALLREDUCE_NODE : range: 0-9 @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE @Default:# -1
20220327 015824.558 INFO             PET0 index= 244               MPIR_CVAR_ALLREDUCE_NETWORK_MULTIPLYING_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_MULTIPLYING_RADIX @Default:# -1
20220327 015824.558 INFO             PET0 index= 245                  MPIR_CVAR_ALLREDUCE_NODE_MULTIPLYING_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_MULTIPLYING_RADIX @Default:# -1
20220327 015824.558 INFO             PET0 index= 246                   MPIR_CVAR_ALLREDUCE_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET0 index= 247                      MPIR_CVAR_ALLREDUCE_NETWORK_KARY_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_KARY_RADIX @Default:# -1
20220327 015824.558 INFO             PET0 index= 248                      MPIR_CVAR_ALLREDUCE_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET0 index= 249                         MPIR_CVAR_ALLREDUCE_NODE_KARY_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_KARY_RADIX @Default:# -1
20220327 015824.558 INFO             PET0 index= 250                    MPIR_CVAR_ALLREDUCE_NODE_SHM_GATHER_TYPE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHM_GATHER_TYPE @Default:# -1
20220327 015824.558 INFO             PET0 index= 251                   MPIR_CVAR_ALLREDUCE_NODE_SHM_GATHER_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHM_GATHER_RADIX @Default:# -1
20220327 015824.558 INFO             PET0 index= 252                   MPIR_CVAR_ALLREDUCE_NODE_SHM_RELEASE_TYPE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHM_RELEASE_TYPE @Default:# -1
20220327 015824.558 INFO             PET0 index= 253                  MPIR_CVAR_ALLREDUCE_NODE_SHM_RELEASE_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHM_RELEASE_RADIX @Default:# -1
20220327 015824.558 INFO             PET0 index= 254                MPIR_CVAR_ALLREDUCE_NETWORK_SHUMILIN_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_SHUMILIN_SEGSIZE @Default:# -1
20220327 015824.558 INFO             PET0 index= 255                   MPIR_CVAR_ALLREDUCE_NODE_SHUMILIN_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHUMILIN_SEGSIZE @Default:# -1
20220327 015824.558 INFO             PET0 index= 256                              MPIR_CVAR_ALLREDUCE_ZETA_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_ZETA_RADIX @Default:# -1
20220327 015824.558 INFO             PET0 index= 257                           MPIR_CVAR_ALLREDUCE_ZETA_SHM_TYPE : @Alias:# I_MPI_ADJUST_ALLREDUCE_ZETA_SHM_TYPE @Default:# -1
20220327 015824.558 INFO             PET0 index= 258                              MPIR_CVAR_ALLREDUCE_IOTA_NLEAD : @Alias:# I_MPI_ADJUST_ALLREDUCE_IOTA_NLEAD @Default:# -1
20220327 015824.558 INFO             PET0 index= 259               MPIR_CVAR_ALLREDUCE_NODE_NREDUCE_NB_ALLGATHER : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_NREDUCE_NB_ALLGATHER @Default:# -1
20220327 015824.558 INFO             PET0 index= 260            MPIR_CVAR_ALLREDUCE_NETWORK_NREDUCE_NB_ALLGATHER : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_NREDUCE_NB_ALLGATHER @Default:# -1
20220327 015824.558 INFO             PET0 index= 261                MPIR_CVAR_ALLREDUCE_NODE_NREDUCE_NB_ALLTOALL : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_NREDUCE_NB_ALLTOALL @Default:# -1
20220327 015824.558 INFO             PET0 index= 262             MPIR_CVAR_ALLREDUCE_NETWORK_NREDUCE_NB_ALLTOALL : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_NREDUCE_NB_ALLTOALL @Default:# -1
20220327 015824.558 INFO             PET0 index= 263                    MPIR_CVAR_ALLREDUCE_NODE_NREDUCE_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_NREDUCE_SEGSIZE @Default:# -1
20220327 015824.558 INFO             PET0 index= 264                 MPIR_CVAR_ALLREDUCE_NETWORK_NREDUCE_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_NREDUCE_SEGSIZE @Default:# -1
20220327 015824.558 INFO             PET0 index= 265                                      MPIR_CVAR_USE_ALLTOALL : Control selection of MPI_Alltoall algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-13 @Alias:# I_MPI_ADJUST_ALLTOALL @Default:# -1
20220327 015824.558 INFO             PET0 index= 266                                 MPIR_CVAR_USE_ALLTOALL_LIST : @Alias:# I_MPI_ADJUST_ALLTOALL_LIST @Default:# -1
20220327 015824.558 INFO             PET0 index= 267                          MPIR_CVAR_USE_ALLTOALL_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLTOALL_COMPOSITION @Default:# -1
20220327 015824.558 INFO             PET0 index= 268                              MPIR_CVAR_USE_ALLTOALL_NETWORK : range: 0-7 @Alias:# I_MPI_ADJUST_ALLTOALL_NETWORK @Default:# -1
20220327 015824.558 INFO             PET0 index= 269                                 MPIR_CVAR_USE_ALLTOALL_NODE : range: 0-6 @Alias:# I_MPI_ADJUST_ALLTOALL_NODE @Default:# -1
20220327 015824.558 INFO             PET0 index= 270                MPIR_CVAR_ALLTOALL_COMPOSITION_GAMMA_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLTOALL_COMPOSITION_GAMMA_SEGSIZE @Default:# -1
20220327 015824.558 INFO             PET0 index= 271                                 MPIR_CVAR_ALLTOALL_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALL_SCATTERED_THROTTLE @Default:# -1
20220327 015824.558 INFO             PET0 index= 272               MPIR_CVAR_ALLTOALL_NETWORK_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALL_NETWORK_SCATTERED_THROTTLE @Default:# -1
20220327 015824.558 INFO             PET0 index= 273                  MPIR_CVAR_ALLTOALL_NODE_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALL_NODE_SCATTERED_THROTTLE @Default:# -1
20220327 015824.558 INFO             PET0 index= 274                             MPIR_CVAR_ALLTOALL_BRUCKS_RADIX : @Alias:# I_MPI_ADJUST_ALLTOALL_BRUCKS_RADIX @Default:# -1
20220327 015824.558 INFO             PET0 index= 275                     MPIR_CVAR_ALLTOALL_NETWORK_BRUCKS_RADIX : @Alias:# I_MPI_ADJUST_ALLTOALL_NETWORK_BRUCKS_RADIX @Default:# -1
20220327 015824.558 INFO             PET0 index= 276                        MPIR_CVAR_ALLTOALL_NODE_BRUCKS_RADIX : @Alias:# I_MPI_ADJUST_ALLTOALL_NODE_BRUCKS_RADIX @Default:# -1
20220327 015824.558 INFO             PET0 index= 277                                     MPIR_CVAR_USE_ALLTOALLV : Control selection of MPI_Alltoallv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-9 @Alias:# I_MPI_ADJUST_ALLTOALLV @Default:# -1
20220327 015824.558 INFO             PET0 index= 278                                MPIR_CVAR_USE_ALLTOALLV_LIST : @Alias:# I_MPI_ADJUST_ALLTOALLV_LIST @Default:# -1
20220327 015824.558 INFO             PET0 index= 279                         MPIR_CVAR_USE_ALLTOALLV_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLTOALLV_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET0 index= 280                             MPIR_CVAR_USE_ALLTOALLV_NETWORK : range: 0-6 @Alias:# I_MPI_ADJUST_ALLTOALLV_NETWORK @Default:# -1
20220327 015824.559 INFO             PET0 index= 281                                MPIR_CVAR_USE_ALLTOALLV_NODE : range: 0-5 @Alias:# I_MPI_ADJUST_ALLTOALLV_NODE @Default:# -1
20220327 015824.559 INFO             PET0 index= 282                                MPIR_CVAR_ALLTOALLV_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLV_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET0 index= 283              MPIR_CVAR_ALLTOALLV_NETWORK_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLV_NETWORK_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET0 index= 284                 MPIR_CVAR_ALLTOALLV_NODE_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLV_NODE_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET0 index= 285                                     MPIR_CVAR_USE_ALLTOALLW : Control selection of MPI_Alltoallw algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-6 @Alias:# I_MPI_ADJUST_ALLTOALLW @Default:# -1
20220327 015824.559 INFO             PET0 index= 286                                MPIR_CVAR_USE_ALLTOALLW_LIST : @Alias:# I_MPI_ADJUST_ALLTOALLW_LIST @Default:# -1
20220327 015824.559 INFO             PET0 index= 287                         MPIR_CVAR_USE_ALLTOALLW_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLTOALLW_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET0 index= 288                             MPIR_CVAR_USE_ALLTOALLW_NETWORK : @Alias:# I_MPI_ADJUST_ALLTOALLW_NETWORK @Default:# -1
20220327 015824.559 INFO             PET0 index= 289                                MPIR_CVAR_USE_ALLTOALLW_NODE : @Alias:# I_MPI_ADJUST_ALLTOALLW_NODE @Default:# -1
20220327 015824.559 INFO             PET0 index= 290                                MPIR_CVAR_ALLTOALLW_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLW_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET0 index= 291              MPIR_CVAR_ALLTOALLW_NETWORK_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLW_NETWORK_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET0 index= 292                 MPIR_CVAR_ALLTOALLW_NODE_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLW_NODE_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET0 index= 293                                       MPIR_CVAR_USE_BARRIER : Control selection of MPI_Barrier algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-11 @Alias:# I_MPI_ADJUST_BARRIER @Default:# -1
20220327 015824.559 INFO             PET0 index= 294                                  MPIR_CVAR_USE_BARRIER_LIST : @Alias:# I_MPI_ADJUST_BARRIER_LIST @Default:# -1
20220327 015824.559 INFO             PET0 index= 295                           MPIR_CVAR_USE_BARRIER_COMPOSITION : @Alias:# I_MPI_ADJUST_BARRIER_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET0 index= 296                               MPIR_CVAR_USE_BARRIER_NETWORK : range: 0-6 @Alias:# I_MPI_ADJUST_BARRIER_NETWORK @Default:# -1
20220327 015824.559 INFO             PET0 index= 297                                  MPIR_CVAR_USE_BARRIER_NODE : range: 0-4 @Alias:# I_MPI_ADJUST_BARRIER_NODE @Default:# -1
20220327 015824.559 INFO             PET0 index= 298                 MPIR_CVAR_BARRIER_NETWORK_MULTIPLYING_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NETWORK_MULTIPLYING_RADIX @Default:# -1
20220327 015824.559 INFO             PET0 index= 299                     MPIR_CVAR_BARRIER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET0 index= 300                        MPIR_CVAR_BARRIER_NETWORK_KARY_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NETWORK_KARY_RADIX @Default:# -1
20220327 015824.559 INFO             PET0 index= 301          MPIR_CVAR_BARRIER_NETWORK_RECURSIVE_EXCHANGE_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NETWORK_RECURSIVE_EXCHANGE_RADIX @Default:# -1
20220327 015824.559 INFO             PET0 index= 302                        MPIR_CVAR_BARRIER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET0 index= 303                           MPIR_CVAR_BARRIER_NODE_KARY_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_KARY_RADIX @Default:# -1
20220327 015824.559 INFO             PET0 index= 304             MPIR_CVAR_BARRIER_NODE_RECURSIVE_EXCHANGE_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_RECURSIVE_EXCHANGE_RADIX @Default:# -1
20220327 015824.559 INFO             PET0 index= 305                      MPIR_CVAR_BARRIER_NODE_SHM_GATHER_TYPE : @Alias:# I_MPI_ADJUST_BARRIER_NODE_SHM_GATHER_TYPE @Default:# -1
20220327 015824.559 INFO             PET0 index= 306                     MPIR_CVAR_BARRIER_NODE_SHM_GATHER_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_SHM_GATHER_RADIX @Default:# -1
20220327 015824.559 INFO             PET0 index= 307                     MPIR_CVAR_BARRIER_NODE_SHM_RELEASE_TYPE : @Alias:# I_MPI_ADJUST_BARRIER_NODE_SHM_RELEASE_TYPE @Default:# -1
20220327 015824.559 INFO             PET0 index= 308                    MPIR_CVAR_BARRIER_NODE_SHM_RELEASE_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_SHM_RELEASE_RADIX @Default:# -1
20220327 015824.559 INFO             PET0 index= 309                                MPIR_CVAR_BARRIER_ZETA_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_ZETA_RADIX @Default:# -1
20220327 015824.559 INFO             PET0 index= 310                             MPIR_CVAR_BARRIER_ZETA_SHM_TYPE : @Alias:# I_MPI_ADJUST_BARRIER_ZETA_SHM_TYPE @Default:# -1
20220327 015824.559 INFO             PET0 index= 311                                         MPIR_CVAR_USE_BCAST : Control selection of MPI_Bcast algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-18 @Alias:# I_MPI_ADJUST_BCAST @Default:# -1
20220327 015824.559 INFO             PET0 index= 312                                    MPIR_CVAR_USE_BCAST_LIST : @Alias:# I_MPI_ADJUST_BCAST_LIST @Default:# -1
20220327 015824.559 INFO             PET0 index= 313                             MPIR_CVAR_USE_BCAST_COMPOSITION : @Alias:# I_MPI_ADJUST_BCAST_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET0 index= 314                                 MPIR_CVAR_USE_BCAST_NETWORK : range: 0-13 @Alias:# I_MPI_ADJUST_BCAST_NETWORK @Default:# -1
20220327 015824.559 INFO             PET0 index= 315                                    MPIR_CVAR_USE_BCAST_NODE : range: 0-9 @Alias:# I_MPI_ADJUST_BCAST_NODE @Default:# -1
20220327 015824.559 INFO             PET0 index= 316                               MPIR_CVAR_BCAST_EPSILON_NRAIL : @Alias:# I_MPI_ADJUST_BCAST_EPSILON_NRAIL @Default:# -1
20220327 015824.559 INFO             PET0 index= 317                          MPIR_CVAR_BCAST_NETWORK_KARY_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_KARY_RADIX @Default:# -1
20220327 015824.559 INFO             PET0 index= 318                       MPIR_CVAR_BCAST_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET0 index= 319                        MPIR_CVAR_BCAST_NETWORK_KARY_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_KARY_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET0 index= 320                     MPIR_CVAR_BCAST_NETWORK_KNOMIAL_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_KNOMIAL_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET0 index= 321                             MPIR_CVAR_BCAST_NODE_KARY_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NODE_KARY_RADIX @Default:# -1
20220327 015824.559 INFO             PET0 index= 322                          MPIR_CVAR_BCAST_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET0 index= 323                           MPIR_CVAR_BCAST_NODE_KARY_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_KARY_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET0 index= 324                        MPIR_CVAR_BCAST_NODE_KNOMIAL_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_KNOMIAL_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET0 index= 325                          MPIR_CVAR_BCAST_NETWORK_TREE_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_TREE_RADIX @Default:# -1
20220327 015824.559 INFO             PET0 index= 326                        MPIR_CVAR_BCAST_NETWORK_TREE_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_TREE_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET0 index= 327                           MPIR_CVAR_BCAST_NETWORK_TREE_TYPE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_TREE_TYPE @Default:# -1
20220327 015824.559 INFO             PET0 index= 328                       MPIR_CVAR_BCAST_NETWORK_TREE_THROTTLE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_TREE_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET0 index= 329                       MPIR_CVAR_BCAST_NODE_SHUMILIN_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHUMILIN_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET0 index= 330                        MPIR_CVAR_BCAST_NODE_SHM_GATHER_TYPE : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHM_GATHER_TYPE @Default:# -1
20220327 015824.559 INFO             PET0 index= 331                       MPIR_CVAR_BCAST_NODE_SHM_GATHER_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHM_GATHER_RADIX @Default:# -1
20220327 015824.559 INFO             PET0 index= 332                       MPIR_CVAR_BCAST_NODE_SHM_RELEASE_TYPE : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHM_RELEASE_TYPE @Default:# -1
20220327 015824.559 INFO             PET0 index= 333                      MPIR_CVAR_BCAST_NODE_SHM_RELEASE_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHM_RELEASE_RADIX @Default:# -1
20220327 015824.559 INFO             PET0 index= 334                    MPIR_CVAR_BCAST_NETWORK_SHUMILIN_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_SHUMILIN_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET0 index= 335                 MPIR_CVAR_BCAST_NODE_NUMA_AWARE_MEMCPY_ARCH : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_MEMCPY_ARCH @Default:# -1
20220327 015824.559 INFO             PET0 index= 336                   MPIR_CVAR_BCAST_NODE_NUMA_AWARE_FRAME_NUM : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_FRAME_NUM
20220327 015824.559 INFO             PET0 index= 337                  MPIR_CVAR_BCAST_NODE_NUMA_AWARE_FRAME_SIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_FRAME_SIZE
20220327 015824.559 INFO             PET0 index= 338  MPIR_CVAR_BCAST_NODE_NUMA_AWARE_RECV_NONTEMPORAL_THRESHOLD : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_NONTEMPORAL_THRESHOLD
20220327 015824.559 INFO             PET0 index= 339      MPIR_CVAR_BCAST_NODE_NUMA_AWARE_FRAME_NONTEMPORAL_SIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_FRAME_NONTEMPORAL_SIZE
20220327 015824.559 INFO             PET0 index= 340 MPIR_CVAR_BCAST_NODE_NUMA_AWARE_TINY_MESSAGE_SIZE_THRESHOLD : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_TINY_MESSAGE_SIZE_THRESHOLD
20220327 015824.559 INFO             PET0 index= 341MPIR_CVAR_BCAST_NODE_NUMA_AWARE_SHM_HEAP_MESSAGE_SIZE_THRESHOLD : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_SHM_HEAP_MESSAGE_SIZE_THRESHOLD
20220327 015824.559 INFO             PET0 index= 342                                        MPIR_CVAR_USE_EXSCAN : Control selection of MPI_Exscan algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_EXSCAN @Default:# -1
20220327 015824.559 INFO             PET0 index= 343                                   MPIR_CVAR_USE_EXSCAN_LIST : @Alias:# I_MPI_ADJUST_EXSCAN_LIST @Default:# -1
20220327 015824.559 INFO             PET0 index= 344                            MPIR_CVAR_USE_EXSCAN_COMPOSITION : @Alias:# I_MPI_ADJUST_EXSCAN_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET0 index= 345                                MPIR_CVAR_USE_EXSCAN_NETWORK : @Alias:# I_MPI_ADJUST_EXSCAN_NETWORK @Default:# -1
20220327 015824.559 INFO             PET0 index= 346                                   MPIR_CVAR_USE_EXSCAN_NODE : @Alias:# I_MPI_ADJUST_EXSCAN_NODE @Default:# -1
20220327 015824.559 INFO             PET0 index= 347                                        MPIR_CVAR_USE_GATHER : Control selection of MPI_Gather algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_GATHER @Default:# -1
20220327 015824.559 INFO             PET0 index= 348                                   MPIR_CVAR_USE_GATHER_LIST : @Alias:# I_MPI_ADJUST_GATHER_LIST @Default:# -1
20220327 015824.559 INFO             PET0 index= 349                            MPIR_CVAR_USE_GATHER_COMPOSITION : @Alias:# I_MPI_ADJUST_GATHER_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET0 index= 350                                MPIR_CVAR_USE_GATHER_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_GATHER_NETWORK @Default:# -1
20220327 015824.559 INFO             PET0 index= 351                                   MPIR_CVAR_USE_GATHER_NODE : range: 0-3 @Alias:# I_MPI_ADJUST_GATHER_NODE @Default:# -1
20220327 015824.559 INFO             PET0 index= 352            MPIR_CVAR_GATHER_NODE_BINOMIAL_SEGMENTED_SEGSIZE : @Alias:# I_MPI_ADJUST_GATHER_NODE_BINOMIAL_SEGMENTED_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET0 index= 353         MPIR_CVAR_GATHER_NETWORK_BINOMIAL_SEGMENTED_SEGSIZE : @Alias:# I_MPI_ADJUST_GATHER_NETWORK_BINOMIAL_SEGMENTED_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET0 index= 354                                       MPIR_CVAR_USE_GATHERV : Control selection of MPI_Gatherv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-4 @Alias:# I_MPI_ADJUST_GATHERV @Default:# -1
20220327 015824.559 INFO             PET0 index= 355                                  MPIR_CVAR_USE_GATHERV_LIST : @Alias:# I_MPI_ADJUST_GATHERV_LIST @Default:# -1
20220327 015824.559 INFO             PET0 index= 356                           MPIR_CVAR_USE_GATHERV_COMPOSITION : @Alias:# I_MPI_ADJUST_GATHERV_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET0 index= 357                               MPIR_CVAR_USE_GATHERV_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_GATHERV_NETWORK @Default:# -1
20220327 015824.559 INFO             PET0 index= 358                                  MPIR_CVAR_USE_GATHERV_NODE : range: 0-2 @Alias:# I_MPI_ADJUST_GATHERV_NODE @Default:# -1
20220327 015824.559 INFO             PET0 index= 359                     MPIR_CVAR_GATHERV_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_GATHERV_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET0 index= 360                        MPIR_CVAR_GATHERV_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_GATHERV_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET0 index= 361                                MPIR_CVAR_USE_REDUCE_SCATTER : Control selection of MPI_Reduce_scatter algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-6 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER @Default:# -1
20220327 015824.559 INFO             PET0 index= 362                           MPIR_CVAR_USE_REDUCE_SCATTER_LIST : @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_LIST @Default:# -1
20220327 015824.559 INFO             PET0 index= 363                    MPIR_CVAR_USE_REDUCE_SCATTER_COMPOSITION : @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET0 index= 364                        MPIR_CVAR_USE_REDUCE_SCATTER_NETWORK : range: 0-5 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_NETWORK @Default:# -1
20220327 015824.559 INFO             PET0 index= 365                           MPIR_CVAR_USE_REDUCE_SCATTER_NODE : range: 0-4 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_NODE @Default:# -1
20220327 015824.559 INFO             PET0 index= 366                          MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK : Control selection of MPI_Reduce_scatter_block algorithm presets. Arguments$ <algid> - Algorithm identifier$ range: 0-4 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK @Default:# -1
20220327 015824.559 INFO             PET0 index= 367                     MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK_LIST : @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK_LIST @Default:# -1
20220327 015824.559 INFO             PET0 index= 368              MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK_COMPOSITION : @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET0 index= 369                  MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK_NETWORK @Default:# -1
20220327 015824.559 INFO             PET0 index= 370                     MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK_NODE : range: 0-3 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK_NODE @Default:# -1
20220327 015824.559 INFO             PET0 index= 371                                        MPIR_CVAR_USE_REDUCE : Control selection of MPI_Reduce algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-13 @Alias:# I_MPI_ADJUST_REDUCE @Default:# -1
20220327 015824.559 INFO             PET0 index= 372                                   MPIR_CVAR_USE_REDUCE_LIST : @Alias:# I_MPI_ADJUST_REDUCE_LIST @Default:# -1
20220327 015824.559 INFO             PET0 index= 373                            MPIR_CVAR_USE_REDUCE_COMPOSITION : @Alias:# I_MPI_ADJUST_REDUCE_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET0 index= 374                                MPIR_CVAR_USE_REDUCE_NETWORK : range: 0-10 @Alias:# I_MPI_ADJUST_REDUCE_NETWORK @Default:# -1
20220327 015824.559 INFO             PET0 index= 375                                   MPIR_CVAR_USE_REDUCE_NODE : range: 0-7 @Alias:# I_MPI_ADJUST_REDUCE_NODE @Default:# -1
20220327 015824.559 INFO             PET0 index= 376                         MPIR_CVAR_REDUCE_NETWORK_KARY_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KARY_RADIX @Default:# -1
20220327 015824.559 INFO             PET0 index= 377                      MPIR_CVAR_REDUCE_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET0 index= 378                      MPIR_CVAR_REDUCE_NETWORK_KARY_NBUFFERS : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KARY_NBUFFERS @Default:# -1
20220327 015824.559 INFO             PET0 index= 379                   MPIR_CVAR_REDUCE_NETWORK_KNOMIAL_NBUFFERS : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KNOMIAL_NBUFFERS @Default:# -1
20220327 015824.559 INFO             PET0 index= 380                       MPIR_CVAR_REDUCE_NETWORK_KARY_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KARY_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET0 index= 381                    MPIR_CVAR_REDUCE_NETWORK_KNOMIAL_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KNOMIAL_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET0 index= 382                       MPIR_CVAR_REDUCE_NETWORK_RING_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_RING_SEGSIZE @Default:# -1
20220327 015824.560 INFO             PET0 index= 383                            MPIR_CVAR_REDUCE_NODE_KARY_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KARY_RADIX @Default:# -1
20220327 015824.560 INFO             PET0 index= 384                         MPIR_CVAR_REDUCE_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET0 index= 385                         MPIR_CVAR_REDUCE_NODE_KARY_NBUFFERS : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KARY_NBUFFERS @Default:# -1
20220327 015824.560 INFO             PET0 index= 386                      MPIR_CVAR_REDUCE_NODE_KNOMIAL_NBUFFERS : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KNOMIAL_NBUFFERS @Default:# -1
20220327 015824.560 INFO             PET0 index= 387                          MPIR_CVAR_REDUCE_NODE_KARY_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KARY_SEGSIZE @Default:# -1
20220327 015824.560 INFO             PET0 index= 388                       MPIR_CVAR_REDUCE_NODE_KNOMIAL_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KNOMIAL_SEGSIZE @Default:# -1
20220327 015824.560 INFO             PET0 index= 389                          MPIR_CVAR_REDUCE_NODE_RING_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_RING_SEGSIZE @Default:# -1
20220327 015824.560 INFO             PET0 index= 390                       MPIR_CVAR_REDUCE_NODE_SHM_GATHER_TYPE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_SHM_GATHER_TYPE @Default:# -1
20220327 015824.560 INFO             PET0 index= 391                      MPIR_CVAR_REDUCE_NODE_SHM_GATHER_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NODE_SHM_GATHER_RADIX @Default:# -1
20220327 015824.560 INFO             PET0 index= 392                      MPIR_CVAR_REDUCE_NODE_SHM_RELEASE_TYPE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_SHM_RELEASE_TYPE @Default:# -1
20220327 015824.560 INFO             PET0 index= 393                     MPIR_CVAR_REDUCE_NODE_SHM_RELEASE_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NODE_SHM_RELEASE_RADIX @Default:# -1
20220327 015824.560 INFO             PET0 index= 394                                          MPIR_CVAR_USE_SCAN : Control selection of MPI_Scan algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_SCAN @Default:# -1
20220327 015824.560 INFO             PET0 index= 395                                     MPIR_CVAR_USE_SCAN_LIST : @Alias:# I_MPI_ADJUST_SCAN_LIST @Default:# -1
20220327 015824.560 INFO             PET0 index= 396                              MPIR_CVAR_USE_SCAN_COMPOSITION : @Alias:# I_MPI_ADJUST_SCAN_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET0 index= 397                                  MPIR_CVAR_USE_SCAN_NETWORK : @Alias:# I_MPI_ADJUST_SCAN_NETWORK @Default:# -1
20220327 015824.560 INFO             PET0 index= 398                                     MPIR_CVAR_USE_SCAN_NODE : @Alias:# I_MPI_ADJUST_SCAN_NODE @Default:# -1
20220327 015824.560 INFO             PET0 index= 399                                       MPIR_CVAR_USE_SCATTER : Control selection of MPI_Scatter algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_SCATTER @Default:# -1
20220327 015824.560 INFO             PET0 index= 400                                  MPIR_CVAR_USE_SCATTER_LIST : @Alias:# I_MPI_ADJUST_SCATTER_LIST @Default:# -1
20220327 015824.560 INFO             PET0 index= 401                           MPIR_CVAR_USE_SCATTER_COMPOSITION : @Alias:# I_MPI_ADJUST_SCATTER_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET0 index= 402                               MPIR_CVAR_USE_SCATTER_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_SCATTER_NETWORK @Default:# -1
20220327 015824.560 INFO             PET0 index= 403                                  MPIR_CVAR_USE_SCATTER_NODE : range: 0-3 @Alias:# I_MPI_ADJUST_SCATTER_NODE @Default:# -1
20220327 015824.560 INFO             PET0 index= 404                                      MPIR_CVAR_USE_SCATTERV : Control selection of MPI_Scatterv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-4 @Alias:# I_MPI_ADJUST_SCATTERV @Default:# -1
20220327 015824.560 INFO             PET0 index= 405                                 MPIR_CVAR_USE_SCATTERV_LIST : @Alias:# I_MPI_ADJUST_SCATTERV_LIST @Default:# -1
20220327 015824.560 INFO             PET0 index= 406                          MPIR_CVAR_USE_SCATTERV_COMPOSITION : @Alias:# I_MPI_ADJUST_SCATTERV_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET0 index= 407                              MPIR_CVAR_USE_SCATTERV_NETWORK : range: 0-3 @Alias:# I_MPI_ADJUST_SCATTERV_NETWORK @Default:# -1
20220327 015824.560 INFO             PET0 index= 408                                 MPIR_CVAR_USE_SCATTERV_NODE : range: 0-2 @Alias:# I_MPI_ADJUST_SCATTERV_NODE @Default:# -1
20220327 015824.560 INFO             PET0 index= 409                                    MPIR_CVAR_USE_IALLREDUCE : Control selection of MPI_Iallreduce algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-9 @Alias:# I_MPI_ADJUST_IALLREDUCE @Default:# -1
20220327 015824.560 INFO             PET0 index= 410                               MPIR_CVAR_USE_IALLREDUCE_LIST : @Alias:# I_MPI_ADJUST_IALLREDUCE_LIST @Default:# -1
20220327 015824.560 INFO             PET0 index= 411                        MPIR_CVAR_USE_IALLREDUCE_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLREDUCE_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET0 index= 412                            MPIR_CVAR_USE_IALLREDUCE_NETWORK : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK @Default:# -1
20220327 015824.560 INFO             PET0 index= 413                               MPIR_CVAR_USE_IALLREDUCE_NODE : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE @Default:# -1
20220327 015824.560 INFO             PET0 index= 414                   MPIR_CVAR_IALLREDUCE_KNOMIAL_REDUCE_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_KNOMIAL_REDUCE_RADIX @Default:# -1
20220327 015824.560 INFO             PET0 index= 415                    MPIR_CVAR_IALLREDUCE_KNOMIAL_BCAST_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_KNOMIAL_BCAST_RADIX @Default:# -1
20220327 015824.560 INFO             PET0 index= 416           MPIR_CVAR_IALLREDUCE_NETWORK_KNOMIAL_REDUCE_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK_KNOMIAL_REDUCE_RADIX @Default:# -1
20220327 015824.560 INFO             PET0 index= 417              MPIR_CVAR_IALLREDUCE_NODE_KNOMIAL_REDUCE_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE_KNOMIAL_REDUCE_RADIX @Default:# -1
20220327 015824.560 INFO             PET0 index= 418            MPIR_CVAR_IALLREDUCE_NETWORK_KNOMIAL_BCAST_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK_KNOMIAL_BCAST_RADIX @Default:# -1
20220327 015824.560 INFO             PET0 index= 419               MPIR_CVAR_IALLREDUCE_NODE_KNOMIAL_BCAST_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE_KNOMIAL_BCAST_RADIX @Default:# -1
20220327 015824.560 INFO             PET0 index= 420                 MPIR_CVAR_IALLREDUCE_NODE_NREDUCE_DO_GATHER : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE_NREDUCE_DO_GATHER @Default:# -1
20220327 015824.560 INFO             PET0 index= 421              MPIR_CVAR_IALLREDUCE_NETWORK_NREDUCE_DO_GATHER : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK_NREDUCE_DO_GATHER @Default:# -1
20220327 015824.560 INFO             PET0 index= 422              MPIR_CVAR_IALLREDUCE_NODE_NREDUCE_NB_ALLGATHER : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE_NREDUCE_NB_ALLGATHER @Default:# -1
20220327 015824.560 INFO             PET0 index= 423           MPIR_CVAR_IALLREDUCE_NETWORK_NREDUCE_NB_ALLGATHER : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK_NREDUCE_NB_ALLGATHER @Default:# -1
20220327 015824.560 INFO             PET0 index= 424                                        MPIR_CVAR_USE_IBCAST : Control selection of MPI_Ibcast algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_IBCAST @Default:# -1
20220327 015824.560 INFO             PET0 index= 425                                   MPIR_CVAR_USE_IBCAST_LIST : @Alias:# I_MPI_ADJUST_IBCAST_LIST @Default:# -1
20220327 015824.560 INFO             PET0 index= 426                            MPIR_CVAR_USE_IBCAST_COMPOSITION : @Alias:# I_MPI_ADJUST_IBCAST_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET0 index= 427                                MPIR_CVAR_USE_IBCAST_NETWORK : @Alias:# I_MPI_ADJUST_IBCAST_NETWORK @Default:# -1
20220327 015824.560 INFO             PET0 index= 428                                   MPIR_CVAR_USE_IBCAST_NODE : @Alias:# I_MPI_ADJUST_IBCAST_NODE @Default:# -1
20220327 015824.560 INFO             PET0 index= 429                              MPIR_CVAR_IBCAST_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IBCAST_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET0 index= 430                      MPIR_CVAR_IBCAST_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IBCAST_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET0 index= 431                         MPIR_CVAR_IBCAST_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IBCAST_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET0 index= 432                                       MPIR_CVAR_USE_IREDUCE : Control selection of MPI_Ireduce algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-4 @Alias:# I_MPI_ADJUST_IREDUCE @Default:# -1
20220327 015824.560 INFO             PET0 index= 433                                  MPIR_CVAR_USE_IREDUCE_LIST : @Alias:# I_MPI_ADJUST_IREDUCE_LIST @Default:# -1
20220327 015824.560 INFO             PET0 index= 434                           MPIR_CVAR_USE_IREDUCE_COMPOSITION : @Alias:# I_MPI_ADJUST_IREDUCE_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET0 index= 435                               MPIR_CVAR_USE_IREDUCE_NETWORK : @Alias:# I_MPI_ADJUST_IREDUCE_NETWORK @Default:# -1
20220327 015824.560 INFO             PET0 index= 436                                  MPIR_CVAR_USE_IREDUCE_NODE : @Alias:# I_MPI_ADJUST_IREDUCE_NODE @Default:# -1
20220327 015824.560 INFO             PET0 index= 437                             MPIR_CVAR_IREDUCE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IREDUCE_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET0 index= 438                     MPIR_CVAR_IREDUCE_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IREDUCE_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET0 index= 439                        MPIR_CVAR_IREDUCE_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IREDUCE_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET0 index= 440                                       MPIR_CVAR_USE_IGATHER : Control selection of MPI_Igather algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-2 @Alias:# I_MPI_ADJUST_IGATHER @Default:# -1
20220327 015824.560 INFO             PET0 index= 441                                  MPIR_CVAR_USE_IGATHER_LIST : @Alias:# I_MPI_ADJUST_IGATHER_LIST @Default:# -1
20220327 015824.560 INFO             PET0 index= 442                           MPIR_CVAR_USE_IGATHER_COMPOSITION : @Alias:# I_MPI_ADJUST_IGATHER_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET0 index= 443                               MPIR_CVAR_USE_IGATHER_NETWORK : @Alias:# I_MPI_ADJUST_IGATHER_NETWORK @Default:# -1
20220327 015824.560 INFO             PET0 index= 444                                  MPIR_CVAR_USE_IGATHER_NODE : @Alias:# I_MPI_ADJUST_IGATHER_NODE @Default:# -1
20220327 015824.560 INFO             PET0 index= 445                             MPIR_CVAR_IGATHER_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IGATHER_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET0 index= 446                     MPIR_CVAR_IGATHER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IGATHER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET0 index= 447                        MPIR_CVAR_IGATHER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IGATHER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET0 index= 448                                    MPIR_CVAR_USE_IALLGATHER : Control selection of MPI_Iallgather algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_IALLGATHER @Default:# -1
20220327 015824.560 INFO             PET0 index= 449                               MPIR_CVAR_USE_IALLGATHER_LIST : @Alias:# I_MPI_ADJUST_IALLGATHER_LIST @Default:# -1
20220327 015824.560 INFO             PET0 index= 450                        MPIR_CVAR_USE_IALLGATHER_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLGATHER_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET0 index= 451                            MPIR_CVAR_USE_IALLGATHER_NETWORK : @Alias:# I_MPI_ADJUST_IALLGATHER_NETWORK @Default:# -1
20220327 015824.560 INFO             PET0 index= 452                               MPIR_CVAR_USE_IALLGATHER_NODE : @Alias:# I_MPI_ADJUST_IALLGATHER_NODE @Default:# -1
20220327 015824.560 INFO             PET0 index= 453                  MPIR_CVAR_IALLGATHER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IALLGATHER_NETWORK_KNOMIAL_RADIX
20220327 015824.560 INFO             PET0 index= 454                     MPIR_CVAR_IALLGATHER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IALLGATHER_NODE_KNOMIAL_RADIX
20220327 015824.560 INFO             PET0 index= 455                                     MPIR_CVAR_USE_IALLTOALL : Control selection of MPI_Ialltoall algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-7 @Alias:# I_MPI_ADJUST_IALLTOALL @Default:# -1
20220327 015824.560 INFO             PET0 index= 456                                MPIR_CVAR_USE_IALLTOALL_LIST : @Alias:# I_MPI_ADJUST_IALLTOALL_LIST @Default:# -1
20220327 015824.560 INFO             PET0 index= 457                         MPIR_CVAR_USE_IALLTOALL_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLTOALL_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET0 index= 458                             MPIR_CVAR_USE_IALLTOALL_NETWORK : @Alias:# I_MPI_ADJUST_IALLTOALL_NETWORK @Default:# -1
20220327 015824.560 INFO             PET0 index= 459                                MPIR_CVAR_USE_IALLTOALL_NODE : @Alias:# I_MPI_ADJUST_IALLTOALL_NODE @Default:# -1
20220327 015824.560 INFO             PET0 index= 460              MPIR_CVAR_IALLTOALL_PERMUTED_SENDRECV_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALL_PERMUTED_SENDRECV_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET0 index= 461      MPIR_CVAR_IALLTOALL_NETWORK_PERMUTED_SENDRECV_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALL_NETWORK_PERMUTED_SENDRECV_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET0 index= 462         MPIR_CVAR_IALLTOALL_NODE_PERMUTED_SENDRECV_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALL_NODE_PERMUTED_SENDRECV_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET0 index= 463                                    MPIR_CVAR_USE_IALLTOALLV : Control selection of MPI_Ialltoallv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-6 @Alias:# I_MPI_ADJUST_IALLTOALLV @Default:# -1
20220327 015824.560 INFO             PET0 index= 464                               MPIR_CVAR_USE_IALLTOALLV_LIST : @Alias:# I_MPI_ADJUST_IALLTOALLV_LIST @Default:# -1
20220327 015824.560 INFO             PET0 index= 465                        MPIR_CVAR_USE_IALLTOALLV_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLTOALLV_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET0 index= 466                            MPIR_CVAR_USE_IALLTOALLV_NETWORK : @Alias:# I_MPI_ADJUST_IALLTOALLV_NETWORK @Default:# -1
20220327 015824.560 INFO             PET0 index= 467                               MPIR_CVAR_USE_IALLTOALLV_NODE : @Alias:# I_MPI_ADJUST_IALLTOALLV_NODE @Default:# -1
20220327 015824.560 INFO             PET0 index= 468                       MPIR_CVAR_IALLTOALLV_BLOCKED_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALLV_BLOCKED_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET0 index= 469               MPIR_CVAR_IALLTOALLV_NETWORK_BLOCKED_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALLV_NETWORK_BLOCKED_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET0 index= 470                                    MPIR_CVAR_USE_IALLTOALLW : Control selection of MPI_Ialltoallw algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-1 @Alias:# I_MPI_ADJUST_IALLTOALLW @Default:# -1
20220327 015824.560 INFO             PET0 index= 471                               MPIR_CVAR_USE_IALLTOALLW_LIST : @Alias:# I_MPI_ADJUST_IALLTOALLW_LIST @Default:# -1
20220327 015824.560 INFO             PET0 index= 472                        MPIR_CVAR_USE_IALLTOALLW_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLTOALLW_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET0 index= 473                            MPIR_CVAR_USE_IALLTOALLW_NETWORK : @Alias:# I_MPI_ADJUST_IALLTOALLW_NETWORK @Default:# -1
20220327 015824.560 INFO             PET0 index= 474                               MPIR_CVAR_USE_IALLTOALLW_NODE : @Alias:# I_MPI_ADJUST_IALLTOALLW_NODE @Default:# -1
20220327 015824.560 INFO             PET0 index= 475                       MPIR_CVAR_IALLTOALLW_BLOCKED_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALLW_BLOCKED_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET0 index= 476               MPIR_CVAR_IALLTOALLW_NETWORK_BLOCKED_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALLW_NETWORK_BLOCKED_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET0 index= 477                                      MPIR_CVAR_USE_IGATHERV : Control selection of MPI_Igatherv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_IGATHERV @Default:# -1
20220327 015824.560 INFO             PET0 index= 478                                 MPIR_CVAR_USE_IGATHERV_LIST : @Alias:# I_MPI_ADJUST_IGATHERV_LIST @Default:# -1
20220327 015824.560 INFO             PET0 index= 479                          MPIR_CVAR_USE_IGATHERV_COMPOSITION : @Alias:# I_MPI_ADJUST_IGATHERV_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET0 index= 480                              MPIR_CVAR_USE_IGATHERV_NETWORK : @Alias:# I_MPI_ADJUST_IGATHERV_NETWORK @Default:# -1
20220327 015824.561 INFO             PET0 index= 481                                 MPIR_CVAR_USE_IGATHERV_NODE : @Alias:# I_MPI_ADJUST_IGATHERV_NODE @Default:# -1
20220327 015824.561 INFO             PET0 index= 482                                     MPIR_CVAR_USE_ISCATTERV : Control selection of MPI_Iscatterv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-1 @Alias:# I_MPI_ADJUST_ISCATTERV @Default:# -1
20220327 015824.561 INFO             PET0 index= 483                                MPIR_CVAR_USE_ISCATTERV_LIST : @Alias:# I_MPI_ADJUST_ISCATTERV_LIST @Default:# -1
20220327 015824.561 INFO             PET0 index= 484                         MPIR_CVAR_USE_ISCATTERV_COMPOSITION : @Alias:# I_MPI_ADJUST_ISCATTERV_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET0 index= 485                             MPIR_CVAR_USE_ISCATTERV_NETWORK : @Alias:# I_MPI_ADJUST_ISCATTERV_NETWORK @Default:# -1
20220327 015824.561 INFO             PET0 index= 486                                MPIR_CVAR_USE_ISCATTERV_NODE : @Alias:# I_MPI_ADJUST_ISCATTERV_NODE @Default:# -1
20220327 015824.561 INFO             PET0 index= 487                                      MPIR_CVAR_USE_IBARRIER : Control selection of MPI_Ibarrier algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-2 @Alias:# I_MPI_ADJUST_IBARRIER @Default:# -1
20220327 015824.561 INFO             PET0 index= 488                                 MPIR_CVAR_USE_IBARRIER_LIST : @Alias:# I_MPI_ADJUST_IBARRIER_LIST @Default:# -1
20220327 015824.561 INFO             PET0 index= 489                          MPIR_CVAR_USE_IBARRIER_COMPOSITION : @Alias:# I_MPI_ADJUST_IBARRIER_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET0 index= 490                              MPIR_CVAR_USE_IBARRIER_NETWORK : @Alias:# I_MPI_ADJUST_IBARRIER_NETWORK @Default:# -1
20220327 015824.561 INFO             PET0 index= 491                                 MPIR_CVAR_USE_IBARRIER_NODE : @Alias:# I_MPI_ADJUST_IBARRIER_NODE @Default:# -1
20220327 015824.561 INFO             PET0 index= 492                                      MPIR_CVAR_USE_ISCATTER : Control selection of MPI_Iscatter algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-2 @Alias:# I_MPI_ADJUST_ISCATTER @Default:# -1
20220327 015824.561 INFO             PET0 index= 493                                 MPIR_CVAR_USE_ISCATTER_LIST : @Alias:# I_MPI_ADJUST_ISCATTER_LIST @Default:# -1
20220327 015824.561 INFO             PET0 index= 494                          MPIR_CVAR_USE_ISCATTER_COMPOSITION : @Alias:# I_MPI_ADJUST_ISCATTER_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET0 index= 495                              MPIR_CVAR_USE_ISCATTER_NETWORK : @Alias:# I_MPI_ADJUST_ISCATTER_NETWORK @Default:# -1
20220327 015824.561 INFO             PET0 index= 496                                 MPIR_CVAR_USE_ISCATTER_NODE : @Alias:# I_MPI_ADJUST_ISCATTER_NODE @Default:# -1
20220327 015824.561 INFO             PET0 index= 497                            MPIR_CVAR_ISCATTER_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ISCATTER_KNOMIAL_RADIX @Default:# -1
20220327 015824.561 INFO             PET0 index= 498                    MPIR_CVAR_ISCATTER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ISCATTER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.561 INFO             PET0 index= 499                       MPIR_CVAR_ISCATTER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ISCATTER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.561 INFO             PET0 index= 500                                   MPIR_CVAR_USE_IALLGATHERV : Control selection of MPI_Iallgatherv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_IALLGATHERV @Default:# -1
20220327 015824.561 INFO             PET0 index= 501                              MPIR_CVAR_USE_IALLGATHERV_LIST : @Alias:# I_MPI_ADJUST_IALLGATHERV_LIST @Default:# -1
20220327 015824.561 INFO             PET0 index= 502                       MPIR_CVAR_USE_IALLGATHERV_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLGATHERV_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET0 index= 503                           MPIR_CVAR_USE_IALLGATHERV_NETWORK : @Alias:# I_MPI_ADJUST_IALLGATHERV_NETWORK @Default:# -1
20220327 015824.561 INFO             PET0 index= 504                              MPIR_CVAR_USE_IALLGATHERV_NODE : @Alias:# I_MPI_ADJUST_IALLGATHERV_NODE @Default:# -1
20220327 015824.561 INFO             PET0 index= 505                                       MPIR_CVAR_USE_IEXSCAN : Control selection of MPI_Iexscan algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-1 @Alias:# I_MPI_ADJUST_IEXSCAN @Default:# -1
20220327 015824.561 INFO             PET0 index= 506                                  MPIR_CVAR_USE_IEXSCAN_LIST : @Alias:# I_MPI_ADJUST_IEXSCAN_LIST @Default:# -1
20220327 015824.561 INFO             PET0 index= 507                           MPIR_CVAR_USE_IEXSCAN_COMPOSITION : @Alias:# I_MPI_ADJUST_IEXSCAN_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET0 index= 508                               MPIR_CVAR_USE_IEXSCAN_NETWORK : @Alias:# I_MPI_ADJUST_IEXSCAN_NETWORK @Default:# -1
20220327 015824.561 INFO             PET0 index= 509                                  MPIR_CVAR_USE_IEXSCAN_NODE : @Alias:# I_MPI_ADJUST_IEXSCAN_NODE @Default:# -1
20220327 015824.561 INFO             PET0 index= 510                                         MPIR_CVAR_USE_ISCAN : Control selection of MPI_Iscan algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-1 @Alias:# I_MPI_ADJUST_ISCAN @Default:# -1
20220327 015824.561 INFO             PET0 index= 511                                    MPIR_CVAR_USE_ISCAN_LIST : @Alias:# I_MPI_ADJUST_ISCAN_LIST @Default:# -1
20220327 015824.561 INFO             PET0 index= 512                             MPIR_CVAR_USE_ISCAN_COMPOSITION : @Alias:# I_MPI_ADJUST_ISCAN_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET0 index= 513                                 MPIR_CVAR_USE_ISCAN_NETWORK : @Alias:# I_MPI_ADJUST_ISCAN_NETWORK @Default:# -1
20220327 015824.561 INFO             PET0 index= 514                                    MPIR_CVAR_USE_ISCAN_NODE : @Alias:# I_MPI_ADJUST_ISCAN_NODE @Default:# -1
20220327 015824.561 INFO             PET0 index= 515                               MPIR_CVAR_USE_IREDUCE_SCATTER : Control selection of MPI_Ireduce_scatter algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER @Default:# -1
20220327 015824.561 INFO             PET0 index= 516                          MPIR_CVAR_USE_IREDUCE_SCATTER_LIST : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_LIST @Default:# -1
20220327 015824.561 INFO             PET0 index= 517                   MPIR_CVAR_USE_IREDUCE_SCATTER_COMPOSITION : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET0 index= 518                       MPIR_CVAR_USE_IREDUCE_SCATTER_NETWORK : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_NETWORK @Default:# -1
20220327 015824.561 INFO             PET0 index= 519                          MPIR_CVAR_USE_IREDUCE_SCATTER_NODE : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_NODE @Default:# -1
20220327 015824.561 INFO             PET0 index= 520                         MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK : Control selection of MPI_Ireduce_scatter_block algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK @Default:# -1
20220327 015824.561 INFO             PET0 index= 521                    MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK_LIST : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK_LIST @Default:# -1
20220327 015824.561 INFO             PET0 index= 522             MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK_COMPOSITION : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET0 index= 523                 MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK_NETWORK : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK_NETWORK @Default:# -1
20220327 015824.561 INFO             PET0 index= 524                    MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK_NODE : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK_NODE @Default:# -1
20220327 015824.561 INFO             PET0 index= 525                               MPIR_CVAR_IMPI_SHMGR_DATASIZE : Define the size of shared memory area available for each rank for data placement. Messages greater than this value will not be processed by SHM-based collective operation, but will be processed by point-to-point based collective operation. The value must be a multiple of 4096. @Alias:# I_MPI_COLL_SHM_THRESHOLD @Default:# 16384
20220327 015824.561 INFO             PET0 index= 526                              MPIR_CVAR_IMPI_SHMGR_SPINCOUNT : @Alias:# I_MPI_COLL_SHM_PROGRESS_SPIN_COUNT
20220327 015824.561 INFO             PET0 index= 527                              MPIR_CVAR_INTEL_COLL_INTRANODE : @Alias:# I_MPI_COLL_INTRANODE
20220327 015824.561 INFO             PET0 index= 528                         MPIR_CVAR_ENABLE_EXPERIMENTAL_ALGOS : @Alias:# I_MPI_COLL_EXPERIMENTAL
20220327 015824.561 INFO             PET0 index= 529                                    MPIR_CVAR_IMPI_WAIT_MODE : @Alias:# I_MPI_WAIT_MODE
20220327 015824.561 INFO             PET0 index= 530                                 MPIR_CVAR_IMPI_THREAD_SLEEP : @Alias:# I_MPI_THREAD_SLEEP
20220327 015824.561 INFO             PET0 index= 531                          MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the short message algorithm will be used if the send buffer size is < this value (in bytes). (See also: MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE)
20220327 015824.561 INFO             PET0 index= 532                           MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the long message algorithm will be used if the send buffer size is >= this value (in bytes) (See also: MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE)
20220327 015824.561 INFO             PET0 index= 533                      MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE : The smallest message size that will be used for the pipelined, large-message, ring algorithm in the MPI_Allgatherv implementation.
20220327 015824.561 INFO             PET0 index= 534                          MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20220327 015824.561 INFO             PET0 index= 535                            MPIR_CVAR_ENABLE_SMP_COLLECTIVES : Enable SMP aware collective communication.
20220327 015824.561 INFO             PET0 index= 536                              MPIR_CVAR_ENABLE_SMP_ALLREDUCE : Enable SMP aware allreduce.
20220327 015824.561 INFO             PET0 index= 537                        MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE : Maximum message size for which SMP-aware allreduce is used.  A value of '0' uses SMP-aware allreduce for all message sizes.
20220327 015824.561 INFO             PET0 index= 538                           MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE : the short message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value (See also: MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE)
20220327 015824.561 INFO             PET0 index= 539                          MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE : the medium message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value and larger than MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE (See also: MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE)
20220327 015824.561 INFO             PET0 index= 540                                MPIR_CVAR_ENABLE_SMP_BARRIER : Enable SMP aware barrier.
20220327 015824.561 INFO             PET0 index= 541                                   MPIR_CVAR_BCAST_MIN_PROCS : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_SHORT_MSG_SIZE, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20220327 015824.561 INFO             PET0 index= 542                              MPIR_CVAR_BCAST_SHORT_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20220327 015824.561 INFO             PET0 index= 543                               MPIR_CVAR_BCAST_LONG_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_SHORT_MSG_SIZE)
20220327 015824.561 INFO             PET0 index= 544                                  MPIR_CVAR_ENABLE_SMP_BCAST : Enable SMP aware broadcast (See also: MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE)
20220327 015824.561 INFO             PET0 index= 545                            MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE : Maximum message size for which SMP-aware broadcast is used.  A value of '0' uses SMP-aware broadcast for all message sizes. (See also: MPIR_CVAR_ENABLE_SMP_BCAST)
20220327 015824.561 INFO             PET0 index= 546                            MPIR_CVAR_GATHER_VSMALL_MSG_SIZE : use a temporary buffer for intracommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE)
20220327 015824.561 INFO             PET0 index= 547                       MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_VSMALL_MSG_SIZE)
20220327 015824.561 INFO             PET0 index= 548                     MPIR_CVAR_GATHERV_INTER_SSEND_MIN_PROCS : Use Ssend (synchronous send) for intercommunicator MPI_Gatherv if the "group B" size is >= this value.  Specifying "-1" always avoids using Ssend.  For backwards compatibility, specifying "0" uses the default value.
20220327 015824.561 INFO             PET0 index= 549                             MPIR_CVAR_REDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20220327 015824.561 INFO             PET0 index= 550                                 MPIR_CVAR_ENABLE_SMP_REDUCE : Enable SMP aware reduce.
20220327 015824.561 INFO             PET0 index= 551                           MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE : Maximum message size for which SMP-aware reduce is used.  A value of '0' uses SMP-aware reduce for all message sizes.
20220327 015824.561 INFO             PET0 index= 552                      MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Scatter if the send buffer size is < this value (in bytes)
20220327 015824.561 INFO             PET0 index= 553                                  MPIR_CVAR_USE_CPU_PLATFORM : @Alias:# I_MPI_PLATFORM
20220327 015824.561 INFO             PET0 index= 554                          MPIR_CVAR_FAILURE_ON_COLL_FALLBACK : @Alias:# I_MPI_ADJUST_FAILURE_ON_COLL_FALLBACK
20220327 015824.561 INFO             PET0 index= 555                         MPIR_CVAR_FAILURE_ON_MATCH_FALLBACK : @Alias:# I_MPI_ADJUST_FAILURE_ON_MATCH_FALLBACK
20220327 015824.561 INFO             PET0 index= 556                           MPIR_CVAR_ADJUST_SENDRECV_REPLACE : @Alias:# I_MPI_ADJUST_SENDRECV_REPLACE
20220327 015824.561 INFO             PET0 index= 557                MPIR_CVAR_ADJUST_SENDRECV_REPLACE_FRAME_SIZE : @Alias:# I_MPI_ADJUST_SENDRECV_REPLACE_FRAME_SIZE
20220327 015824.561 INFO             PET0 index= 558                         MPIR_CVAR_NUMERICAL_REPRODUCIBILITY : @Alias:# I_MPI_CBWR
20220327 015824.561 INFO             PET0 index= 559                                    MPIR_CVAR_USE_TUNING_CH4 : @Alias:# I_MPI_TUNING
20220327 015824.561 INFO             PET0 index= 560                                    MPIR_CVAR_USE_TUNING_NET : @Alias:# I_MPI_TUNING_NETWORK
20220327 015824.561 INFO             PET0 index= 561                                    MPIR_CVAR_USE_TUNING_SHM : @Alias:# I_MPI_TUNING_NODE
20220327 015824.561 INFO             PET0 index= 562                                   MPIR_CVAR_DUMP_TUNING_CH4 : @Alias:# I_MPI_TUNING_COMPOSITION_DUMP
20220327 015824.561 INFO             PET0 index= 563                                   MPIR_CVAR_DUMP_TUNING_NET : @Alias:# I_MPI_TUNING_NETWORK_DUMP
20220327 015824.561 INFO             PET0 index= 564                                   MPIR_CVAR_DUMP_TUNING_SHM : @Alias:# I_MPI_TUNING_NODE_DUMP
20220327 015824.561 INFO             PET0 index= 565                                        MPIR_CVAR_BIN_TUNING : @Alias:# I_MPI_TUNING_BIN
20220327 015824.561 INFO             PET0 index= 566                                   MPIR_CVAR_BIN_DUMP_TUNING : @Alias:# I_MPI_TUNING_BIN_DUMP
20220327 015824.561 INFO             PET0 index= 567                                   MPIR_CVAR_TUNING_BIN_PATH : @Alias:# I_MPI_TUNING_BIN_PATH
20220327 015824.561 INFO             PET0 index= 568                            MPIR_CVAR_TUNING_COMPOSITION_PPN : @Alias:# I_MPI_TUNING_COMPOSITION_PPN
20220327 015824.561 INFO             PET0 index= 569                                MPIR_CVAR_TUNING_NETWORK_PPN : @Alias:# I_MPI_TUNING_NETWORK_PPN
20220327 015824.561 INFO             PET0 index= 570                                   MPIR_CVAR_TUNING_NODE_PPN : @Alias:# I_MPI_TUNING_NODE_PPN
20220327 015824.561 INFO             PET0 index= 571                 MPIR_CVAR_TUNING_COMPOSITION_COMM_HIERARCHY : @Alias:# I_MPI_TUNING_COMPOSITION_COMM_HIERARCHY
20220327 015824.561 INFO             PET0 index= 572                     MPIR_CVAR_TUNING_NETWORK_COMM_HIERARCHY : @Alias:# I_MPI_TUNING_NETWORK_COMM_HIERARCHY
20220327 015824.561 INFO             PET0 index= 573                        MPIR_CVAR_TUNING_NODE_COMM_HIERARCHY : @Alias:# I_MPI_TUNING_NODE_COMM_HIERARCHY
20220327 015824.561 INFO             PET0 index= 574                                       MPIR_CVAR_TUNING_MODE : @Alias:# I_MPI_TUNING_MODE
20220327 015824.561 INFO             PET0 index= 575                                MPIR_CVAR_TUNING_AUTO_POLICY : @Alias:# I_MPI_TUNING_AUTO_POLICY
20220327 015824.561 INFO             PET0 index= 576                             MPIR_CVAR_TUNING_AUTO_COMM_LIST : @Alias:# I_MPI_TUNING_AUTO_COMM_LIST
20220327 015824.561 INFO             PET0 index= 577                             MPIR_CVAR_TUNING_AUTO_COMM_USER : @Alias:# I_MPI_TUNING_AUTO_COMM_USER
20220327 015824.561 INFO             PET0 index= 578                          MPIR_CVAR_TUNING_AUTO_COMM_DEFAULT : @Alias:# I_MPI_TUNING_AUTO_COMM_DEFAULT
20220327 015824.561 INFO             PET0 index= 579                              MPIR_CVAR_TUNING_AUTO_ITER_NUM : @Alias:# I_MPI_TUNING_AUTO_ITER_NUM
20220327 015824.561 INFO             PET0 index= 580                           MPIR_CVAR_TUNING_AUTO_ITER_POLICY : @Alias:# I_MPI_TUNING_AUTO_ITER_POLICY
20220327 015824.561 INFO             PET0 index= 581                 MPIR_CVAR_TUNING_AUTO_ITER_POLICY_THRESHOLD : @Alias:# I_MPI_TUNING_AUTO_ITER_POLICY_THRESHOLD
20220327 015824.561 INFO             PET0 index= 582                                  MPIR_CVAR_TUNING_AUTO_SYNC : @Alias:# I_MPI_TUNING_AUTO_SYNC
20220327 015824.562 INFO             PET0 index= 583                          MPIR_CVAR_TUNING_AUTO_STORAGE_SIZE : @Alias:# I_MPI_TUNING_AUTO_STORAGE_SIZE
20220327 015824.562 INFO             PET0 index= 584                       MPIR_CVAR_TUNING_AUTO_WARMUP_ITER_NUM : @Alias:# I_MPI_TUNING_AUTO_WARMUP_ITER_NUM
20220327 015824.562 INFO             PET0 index= 585                                 MPIR_CVAR_TUNING_AUTO_SMART : @Alias:# I_MPI_TUNING_AUTO_SMART
20220327 015824.562 INFO             PET0 index= 586                                  MPIR_CVAR_TUNING_COLL_LIST : @Alias:# I_MPI_TUNING_COLL_LIST
20220327 015824.562 INFO             PET0 index= 587                               MPIR_CVAR_TUNING_COLL_VEC_OPS : @Alias:# I_MPI_TUNING_COLL_VEC_OPS
20220327 015824.562 INFO             PET0 index= 588                              MPIR_CVAR_DEFAULT_THREAD_LEVEL : @Alias:# I_MPI_THREAD_LEVEL
20220327 015824.562 INFO             PET0 index= 589                                      MPIR_CVAR_THREAD_SPLIT : Control the MPI_THREAD_SPLIT model support. @Alias:# I_MPI_THREAD_SPLIT @Default:# false
20220327 015824.562 INFO             PET0 index= 590                                    MPIR_CVAR_THREAD_RUNTIME : Control threading runtimes support. @Alias:# I_MPI_THREAD_RUNTIME @Default:# generic
20220327 015824.562 INFO             PET0 index= 591                                     MPIR_CVAR_THREAD_ID_KEY : Set the MPI info object key that is used to explicitly define the application $thread_id for a communicator. @Alias:# I_MPI_THREAD_ID_KEY @Default:# -1
20220327 015824.562 INFO             PET0 index= 592                                        MPIR_CVAR_THREAD_MAX : Set the maximum number of application threads per rank. @Alias:# I_MPI_THREAD_MAX @Default:# -1
20220327 015824.562 INFO             PET0 index= 593                                    MPIR_CVAR_ASYNC_PROGRESS : Enables asynchronous progress threads. @Alias:# I_MPI_ASYNC_PROGRESS @Default:# false
20220327 015824.562 INFO             PET0 index= 594                          MPIR_CVAR_CH4_MAX_PROGRESS_THREADS : Specifies the maximum number of progress threads. @Alias:# I_MPI_ASYNC_PROGRESS_THREADS @Default:# 1
20220327 015824.562 INFO             PET0 index= 595                      MPIR_CVAR_CH4_PROGRESS_THREAD_AFFINITY : Specifies affinity for all progress threads of local processes. @Alias:# I_MPI_ASYNC_PROGRESS_PIN @Default:# not defined
20220327 015824.562 INFO             PET0 index= 596                                         MPIR_CVAR_EP_ID_KEY : Set the MPI info object key that is used to explicitly define the progress $thread_id for a communicator. @Alias:# I_MPI_ASYNC_PROGRESS_ID_KEY @Default:# thread_id
20220327 015824.562 INFO             PET0 index= 597                                       MPIR_CVAR_THREAD_MODE : @Alias:# I_MPI_THREAD_MODE
20220327 015824.562 INFO             PET0 index= 598                                 MPIR_CVAR_THREAD_LOCK_LEVEL : @Alias:# I_MPI_THREAD_LOCK_LEVEL
20220327 015824.562 INFO             PET0 index= 599                                  MPIR_CVAR_CH4_OFI_MAX_VCIS : @Alias:# I_MPI_THREAD_EP_MAX
20220327 015824.562 INFO             PET0 index= 600                                       MPIR_CVAR_INTEL_DEBUG : Print out debugging information when an MPI program starts running.$ Syntax$ I_MPI_DEBUG=<level>$ Arguments$ <level> - indicate level of debug information provided @Alias:# I_MPI_DEBUG @Default:# 0
20220327 015824.562 INFO             PET0 index= 601                                     MPIR_CVAR_DEBUG_VERSION : Print Intel MPI version. @Alias:# I_MPI_PRINT_VERSION @Default:# 0
20220327 015824.562 INFO             PET0 index= 602                                    MPIR_CVAR_ERROR_CHECKING : @Alias:# I_MPI_ERROR_CHECKING
20220327 015824.562 INFO             PET0 index= 603                                        MPIR_CVAR_MULTI_INIT : @Alias:# I_MPI_MULTI_INIT
20220327 015824.562 INFO             PET0 index= 604                               MPIR_CVAR_REMOVED_VAR_WARNING : Print out a warning if a removed environment variable is set. @Alias:# I_MPI_REMOVED_VAR_WARNING @Default:# 1
20220327 015824.562 INFO             PET0 index= 605                                MPIR_CVAR_VAR_CHECK_SPELLING : Print out a warning if an unknown environment variable is set. @Alias:# I_MPI_VAR_CHECK_SPELLING @Default:# 1
20220327 015824.562 INFO             PET0 index= 606                           MPIR_CVAR_INTEL_MPI_COMPATIBILITY : Select the runtime compatibility mode.$ Syntax$ I_MPI_COMPATIBILITY=<value>$ Arguments$ <value> - Define compatibility mode$ -----------------------------------------------------------------------$ <not defined> - The MPI-3.1 standard compatibility$ <3> - The Intel® MPI Library 3.x compatible mode$ <4> - The Intel® MPI Library 4.x compatible mode$ <5> - The Intel® MPI Library 5.x compatible mode$ ----------------------------------------------------------------------- @Alias:# I_MPI_COMPATIBILITY @Default:# 5
20220327 015824.562 INFO             PET0 index= 607                          MPIR_CVAR_IMPI_PROGRESS_SPIN_COUNT : @Alias:# I_MPI_SPIN_COUNT
20220327 015824.562 INFO             PET0 index= 608                         MPIR_CVAR_IMPI_PROGRESS_PAUSE_COUNT : @Alias:# I_MPI_PAUSE_COUNT
20220327 015824.562 INFO             PET0 index= 609                                 MPIR_CVAR_IMPI_THREAD_YIELD : @Alias:# I_MPI_THREAD_YIELD
20220327 015824.562 INFO             PET0 index= 610                                      MPIR_CVAR_SILENT_ABORT : Do not print abort warning message @Alias:# I_MPI_SILENT_ABORT
20220327 015824.562 INFO             PET0 index= 611                                  MPIR_CVAR_JOB_IDLE_TIMEOUT : Abort job if idle time is larger than the threshold in seconds. @Alias:# I_MPI_JOB_IDLE_TIMEOUT
20220327 015824.562 INFO             PET0 index= 612                              MPIR_CVAR_PMI_VALUE_LENGTH_MAX : Set PMI buffer length as minimum of variable value and PMI_KVS_Get_value_length_max(). @Alias:# I_MPI_PMI_VALUE_LENGTH_MAX
20220327 015824.562 INFO             PET0 index= 613                                       MPIR_CVAR_PMI_LIBRARY : Specify the name to third party implementation of the PMI library. @Alias:# I_MPI_PMI_LIBRARY
20220327 015824.562 INFO             PET0 index= 614                                               MPIR_CVAR_PMI : Select PMI version. Choices: auto, pmi1, pmi2, pmix.$ By default pmi version will be chosen automatically. @Alias:# I_MPI_PMI
20220327 015824.562 INFO             PET0 index= 615                                 MPIR_CVAR_NODEMAP_ALGORITHM : Select algorithm for nodemap creation. Choices: pmi_process_mapping, slurm, pmi_alltoall, auto. @Alias:# I_MPI_NODEMAP_ALGORITHM
20220327 015824.562 INFO             PET0 index= 616                                      MPIR_CVAR_ASYNC_REDUCE : @Alias:# I_MPI_ASYNC_REDUCE @Verbosity:# hidden
20220327 015824.562 INFO             PET0 index= 617                            MPIR_CVAR_CH4_MAX_REDUCE_THREADS : @Alias:# I_MPI_ASYNC_REDUCE_THREADS @Verbosity:# hidden
20220327 015824.562 INFO             PET0 index= 618                      MPIR_CVAR_ASYNC_REDUCE_COUNT_THRESHOLD : @Alias:# I_MPI_ASYNC_REDUCE_COUNT_THRESHOLD @Verbosity:# hidden
20220327 015824.562 INFO             PET0 index= 619                        MPIR_CVAR_CH4_REDUCE_THREAD_AFFINITY : @Alias:# I_MPI_ASYNC_REDUCE_PIN @Verbosity:# hidden
20220327 015824.562 INFO             PET0 --- VMK::logSystem() end ---------------------------------
20220327 015824.562 INFO             PET0 main: --- VMK::log() start -------------------------------------
20220327 015824.562 INFO             PET0 main: vm located at: 0x87cf80
20220327 015824.562 INFO             PET0 main: petCount=6 localPet=0 mypthid=140737352203136 currentSsiPe=0
20220327 015824.562 INFO             PET0 main: Current system level affinity pinning for local PET:
20220327 015824.562 INFO             PET0 main:  SSIPE=0
20220327 015824.562 INFO             PET0 main:  SSIPE=1
20220327 015824.562 INFO             PET0 main:  SSIPE=2
20220327 015824.562 INFO             PET0 main:  SSIPE=3
20220327 015824.562 INFO             PET0 main: Current system level OMP_NUM_THREADS setting for local PET: 1
20220327 015824.562 INFO             PET0 main: ssiCount=1 localSsi=0
20220327 015824.562 INFO             PET0 main: mpionly=1 threadsflag=0
20220327 015824.562 INFO             PET0 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220327 015824.562 INFO             PET0 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220327 015824.562 INFO             PET0 main:  PE=0 SSI=0 SSIPE=0
20220327 015824.562 INFO             PET0 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220327 015824.562 INFO             PET0 main:  PE=1 SSI=0 SSIPE=1
20220327 015824.562 INFO             PET0 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220327 015824.562 INFO             PET0 main:  PE=2 SSI=0 SSIPE=2
20220327 015824.562 INFO             PET0 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220327 015824.562 INFO             PET0 main:  PE=3 SSI=0 SSIPE=3
20220327 015824.562 INFO             PET0 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220327 015824.562 INFO             PET0 main:  PE=4 SSI=0 SSIPE=4
20220327 015824.562 INFO             PET0 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220327 015824.562 INFO             PET0 main:  PE=5 SSI=0 SSIPE=5
20220327 015824.562 INFO             PET0 main: --- VMK::log() end ---------------------------------------
20220327 015824.565 INFO             PET0 Executing 'userm1_setvm'
20220327 015824.566 INFO             PET0 Executing 'userm1_register'
20220327 015824.566 INFO             PET0 Executing 'userm2_setvm'
20220327 015824.567 INFO             PET0 Executing 'userm2_register'
20220327 015824.649 INFO             PET0 Entering 'user1_run'
20220327 015824.649 INFO             PET0 model1: --- VMK::log() start -------------------------------------
20220327 015824.649 INFO             PET0 model1: vm located at: 0xa5d3a0
20220327 015824.649 INFO             PET0 model1: petCount=6 localPet=0 mypthid=140737352203136 currentSsiPe=0
20220327 015824.649 INFO             PET0 model1: Current system level affinity pinning for local PET:
20220327 015824.649 INFO             PET0 model1:  SSIPE=0
20220327 015824.649 INFO             PET0 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220327 015824.649 INFO             PET0 model1: ssiCount=1 localSsi=0
20220327 015824.649 INFO             PET0 model1: mpionly=1 threadsflag=0
20220327 015824.649 INFO             PET0 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220327 015824.649 INFO             PET0 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220327 015824.649 INFO             PET0 model1:  PE=0 SSI=0 SSIPE=0
20220327 015824.649 INFO             PET0 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220327 015824.649 INFO             PET0 model1:  PE=1 SSI=0 SSIPE=1
20220327 015824.649 INFO             PET0 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220327 015824.649 INFO             PET0 model1:  PE=2 SSI=0 SSIPE=2
20220327 015824.649 INFO             PET0 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220327 015824.649 INFO             PET0 model1:  PE=3 SSI=0 SSIPE=3
20220327 015824.649 INFO             PET0 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220327 015824.649 INFO             PET0 model1:  PE=4 SSI=0 SSIPE=4
20220327 015824.649 INFO             PET0 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220327 015824.649 INFO             PET0 model1:  PE=5 SSI=0 SSIPE=5
20220327 015824.649 INFO             PET0 model1: --- VMK::log() end ---------------------------------------
20220327 015824.649 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220327 015824.775 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220327 015824.900 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220327 015825.025 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220327 015825.150 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220327 015825.276 INFO             PET0 Exiting 'user1_run'
20220327 015825.278 INFO             PET0 Entering 'user2_run'
20220327 015825.278 INFO             PET0 model2: --- VMK::log() start -------------------------------------
20220327 015825.278 INFO             PET0 model2: vm located at: 0xa5d5a0
20220327 015825.278 INFO             PET0 model2: petCount=2 localPet=0 mypthid=140737352203136 currentSsiPe=0
20220327 015825.278 INFO             PET0 model2: Current system level affinity pinning for local PET:
20220327 015825.278 INFO             PET0 model2:  SSIPE=0
20220327 015825.278 INFO             PET0 model2: Current system level OMP_NUM_THREADS setting for local PET: 3
20220327 015825.278 INFO             PET0 model2: ssiCount=1 localSsi=0
20220327 015825.278 INFO             PET0 model2: mpionly=1 threadsflag=0
20220327 015825.278 INFO             PET0 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220327 015825.278 INFO             PET0 model2: PET=0 lpid=0 tid=0 pid=0 peCount=3 accCount=0
20220327 015825.278 INFO             PET0 model2:  PE=0 SSI=0 SSIPE=0
20220327 015825.278 INFO             PET0 model2:  PE=1 SSI=0 SSIPE=1
20220327 015825.278 INFO             PET0 model2:  PE=2 SSI=0 SSIPE=2
20220327 015825.278 INFO             PET0 model2: PET=1 lpid=1 tid=0 pid=3 peCount=3 accCount=0
20220327 015825.278 INFO             PET0 model2:  PE=3 SSI=0 SSIPE=3
20220327 015825.278 INFO             PET0 model2:  PE=4 SSI=0 SSIPE=4
20220327 015825.278 INFO             PET0 model2:  PE=5 SSI=0 SSIPE=5
20220327 015825.278 INFO             PET0 model2: --- VMK::log() end ---------------------------------------
20220327 015825.279 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220327 015825.279 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220327 015825.279 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220327 015825.580 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220327 015825.580 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220327 015825.580 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220327 015825.877 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220327 015825.877 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220327 015825.878 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220327 015826.174 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220327 015826.175 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220327 015826.175 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220327 015826.472 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220327 015826.472 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220327 015826.472 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220327 015826.769 INFO             PET0  user2_run: All data correct.
20220327 015826.769 INFO             PET0 Exiting 'user2_run'
20220327 015826.769 INFO             PET0 Entering 'user1_run'
20220327 015826.769 INFO             PET0 model1: --- VMK::log() start -------------------------------------
20220327 015826.769 INFO             PET0 model1: vm located at: 0xa5d3a0
20220327 015826.769 INFO             PET0 model1: petCount=6 localPet=0 mypthid=140737352203136 currentSsiPe=0
20220327 015826.769 INFO             PET0 model1: Current system level affinity pinning for local PET:
20220327 015826.769 INFO             PET0 model1:  SSIPE=0
20220327 015826.769 INFO             PET0 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220327 015826.769 INFO             PET0 model1: ssiCount=1 localSsi=0
20220327 015826.769 INFO             PET0 model1: mpionly=1 threadsflag=0
20220327 015826.769 INFO             PET0 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220327 015826.769 INFO             PET0 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220327 015826.769 INFO             PET0 model1:  PE=0 SSI=0 SSIPE=0
20220327 015826.769 INFO             PET0 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220327 015826.769 INFO             PET0 model1:  PE=1 SSI=0 SSIPE=1
20220327 015826.769 INFO             PET0 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220327 015826.769 INFO             PET0 model1:  PE=2 SSI=0 SSIPE=2
20220327 015826.769 INFO             PET0 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220327 015826.769 INFO             PET0 model1:  PE=3 SSI=0 SSIPE=3
20220327 015826.769 INFO             PET0 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220327 015826.769 INFO             PET0 model1:  PE=4 SSI=0 SSIPE=4
20220327 015826.769 INFO             PET0 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220327 015826.769 INFO             PET0 model1:  PE=5 SSI=0 SSIPE=5
20220327 015826.769 INFO             PET0 model1: --- VMK::log() end ---------------------------------------
20220327 015826.769 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220327 015826.894 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220327 015827.020 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220327 015827.145 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220327 015827.270 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220327 015827.395 INFO             PET0 Exiting 'user1_run'
20220327 015827.412 INFO             PET0 Entering 'user2_run'
20220327 015827.412 INFO             PET0 model2: --- VMK::log() start -------------------------------------
20220327 015827.412 INFO             PET0 model2: vm located at: 0xa5d5a0
20220327 015827.412 INFO             PET0 model2: petCount=2 localPet=0 mypthid=140737352203136 currentSsiPe=0
20220327 015827.412 INFO             PET0 model2: Current system level affinity pinning for local PET:
20220327 015827.412 INFO             PET0 model2:  SSIPE=0
20220327 015827.412 INFO             PET0 model2: Current system level OMP_NUM_THREADS setting for local PET: 3
20220327 015827.412 INFO             PET0 model2: ssiCount=1 localSsi=0
20220327 015827.412 INFO             PET0 model2: mpionly=1 threadsflag=0
20220327 015827.412 INFO             PET0 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220327 015827.412 INFO             PET0 model2: PET=0 lpid=0 tid=0 pid=0 peCount=3 accCount=0
20220327 015827.412 INFO             PET0 model2:  PE=0 SSI=0 SSIPE=0
20220327 015827.412 INFO             PET0 model2:  PE=1 SSI=0 SSIPE=1
20220327 015827.412 INFO             PET0 model2:  PE=2 SSI=0 SSIPE=2
20220327 015827.412 INFO             PET0 model2: PET=1 lpid=1 tid=0 pid=3 peCount=3 accCount=0
20220327 015827.412 INFO             PET0 model2:  PE=3 SSI=0 SSIPE=3
20220327 015827.412 INFO             PET0 model2:  PE=4 SSI=0 SSIPE=4
20220327 015827.412 INFO             PET0 model2:  PE=5 SSI=0 SSIPE=5
20220327 015827.412 INFO             PET0 model2: --- VMK::log() end ---------------------------------------
20220327 015827.412 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220327 015827.412 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220327 015827.412 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220327 015827.709 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220327 015827.709 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220327 015827.709 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220327 015828.006 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220327 015828.006 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220327 015828.006 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220327 015828.304 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220327 015828.304 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220327 015828.304 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220327 015828.601 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220327 015828.601 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220327 015828.601 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220327 015828.897 INFO             PET0  user2_run: All data correct.
20220327 015828.897 INFO             PET0 Exiting 'user2_run'
20220327 015828.897 INFO             PET0  NUMBER_OF_PROCESSORS           6
20220327 015828.897 INFO             PET0  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220327 015828.897 INFO             PET0 Finalizing ESMF
20220327 015824.554 INFO             PET1 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220327 015824.554 INFO             PET1 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220327 015824.554 INFO             PET1 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220327 015824.554 INFO             PET1 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220327 015824.554 INFO             PET1 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220327 015824.554 INFO             PET1 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220327 015824.554 INFO             PET1 Running with ESMF Version   : v8.3.0b10-105-ga5295e34ae
20220327 015824.554 INFO             PET1 ESMF library build date/time: "Mar 27 2022" "01:23:54"
20220327 015824.554 INFO             PET1 ESMF library build location : /gpfsm/dnb04/projects/p98/mpotts/esmf/intel_19.1.3_intelmpi_O_develop
20220327 015824.554 INFO             PET1 ESMF_COMM                   : intelmpi
20220327 015824.554 INFO             PET1 ESMF_MOAB                   : enabled
20220327 015824.554 INFO             PET1 ESMF_LAPACK                 : enabled
20220327 015824.554 INFO             PET1 ESMF_NETCDF                 : enabled
20220327 015824.554 INFO             PET1 ESMF_PNETCDF                : disabled
20220327 015824.554 INFO             PET1 ESMF_PIO                    : enabled
20220327 015824.554 INFO             PET1 ESMF_YAMLCPP                : enabled
20220327 015824.555 INFO             PET1 --- VMK::logSystem() start -------------------------------
20220327 015824.555 INFO             PET1 esmfComm=intelmpi
20220327 015824.555 INFO             PET1 isPthreadsEnabled=1
20220327 015824.555 INFO             PET1 isOpenMPEnabled=1
20220327 015824.555 INFO             PET1 isOpenACCEnabled=0
20220327 015824.556 INFO             PET1 isSsiSharedMemoryEnabled=1
20220327 015824.556 INFO             PET1 ssiCount=1 peCount=6
20220327 015824.556 INFO             PET1 PE=0 SSI=0 SSIPE=0
20220327 015824.556 INFO             PET1 PE=1 SSI=0 SSIPE=1
20220327 015824.556 INFO             PET1 PE=2 SSI=0 SSIPE=2
20220327 015824.556 INFO             PET1 PE=3 SSI=0 SSIPE=3
20220327 015824.556 INFO             PET1 PE=4 SSI=0 SSIPE=4
20220327 015824.556 INFO             PET1 PE=5 SSI=0 SSIPE=5
20220327 015824.556 INFO             PET1 --- VMK::logSystem() MPI Control Variables ---------------
20220327 015824.556 INFO             PET1 index=   0                                          I_MPI_DEBUG_OUTPUT : @Default:# not defined
20220327 015824.556 INFO             PET1 index=   1                                        I_MPI_DEBUG_COREDUMP : @Default:# 1
20220327 015824.556 INFO             PET1 index=   2                                          I_MPI_LIBRARY_KIND : @Default:# not defined
20220327 015824.556 INFO             PET1 index=   3                                  I_MPI_OFI_LIBRARY_INTERNAL : @Default:# not defined
20220327 015824.556 INFO             PET1 index=   4                                            I_MPI_CC_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET1 index=   5                                           I_MPI_CXX_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET1 index=   6                                            I_MPI_FC_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET1 index=   7                                           I_MPI_F77_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET1 index=   8                                           I_MPI_F90_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET1 index=   9                                         I_MPI_TRACE_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  10                                         I_MPI_CHECK_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  11                                        I_MPI_CHECK_COMPILER : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  12                                                    I_MPI_CC : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  13                                                   I_MPI_CXX : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  14                                                    I_MPI_FC : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  15                                                   I_MPI_F90 : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  16                                                   I_MPI_F77 : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  17                                                  I_MPI_ROOT : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  18                                           I_MPI_ONEAPI_ROOT : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  19                                           MPIR_CVAR_VT_ROOT : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  20                                   I_MPI_COMPILER_CONFIG_DIR : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  21                                                  I_MPI_LINK : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  22                                      I_MPI_DEBUG_INFO_STRIP : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  23                                                I_MPI_CFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  24                                              I_MPI_CXXFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  25                                               I_MPI_FCFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  26                                                I_MPI_FFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  27                                               I_MPI_LDFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  28                                             I_MPI_FORT_BIND : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  29                                           I_MPI_AUTH_METHOD : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  30                               I_MPI_HYDRA_COLLECTIVE_LAUNCH : @Default:# 1
20220327 015824.556 INFO             PET1 index=  31                                  I_MPI_HYDRA_UNIQUE_PROXIES : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  32                                        I_MPI_FAULT_CONTINUE : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  33                                   I_MPI_FAULT_NODE_CONTINUE : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  34                                                I_MPI_MPIRUN : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  35                                            I_MPI_BIND_ORDER : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  36                                             I_MPI_BIND_NUMA : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  37                                     I_MPI_BIND_WIN_ALLOCATE : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  38                                      I_MPI_HYDRA_NAMESERVER : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  39                                        I_MPI_JOB_CHECK_LIBS : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  40                                    I_MPI_HYDRA_SERVICE_PORT : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  41                                           I_MPI_HYDRA_DEBUG : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  42                                             I_MPI_HYDRA_ENV : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  43                                           I_MPI_JOB_TIMEOUT : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  44                                       I_MPI_MPIEXEC_TIMEOUT : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  45                                   I_MPI_JOB_STARTUP_TIMEOUT : Set this environment variable to make mpiexec.hydra terminate$ the job in <timeout> seconds if some processes are not launched. @Default:# -1
20220327 015824.556 INFO             PET1 index=  46                                    I_MPI_JOB_TIMEOUT_SIGNAL : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  47                                      I_MPI_JOB_ABORT_SIGNAL : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  48                                I_MPI_JOB_SIGNAL_PROPAGATION : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  49                                  I_MPI_HYDRA_BOOTSTRAP_EXEC : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  50                       I_MPI_HYDRA_BOOTSTRAP_EXEC_EXTRA_ARGS : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  51                              I_MPI_HYDRA_BOOTSTRAP_AUTOFORK : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  52                                             I_MPI_HYDRA_RMK : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  53                                     I_MPI_HYDRA_PMI_CONNECT : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  54                                         I_MPI_HYDRA_TOPOLIB : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  55                                            I_MPI_PORT_RANGE : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  56                         I_MPI_JOB_RESPECT_PROCESS_PLACEMENT : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  57                                                I_MPI_TMPDIR : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  58                                           I_MPI_HYDRA_DEMUX : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  59                                           I_MPI_HYDRA_IFACE : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  60                                I_MPI_HYDRA_GDB_REMOTE_SHELL : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  61                                   I_MPI_HYDRA_PMI_AGGREGATE : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  62                                        I_MPI_JOB_TRACE_LIBS : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  63                                       I_MPI_HYDRA_HOST_FILE : Set the host file to run the application.$ Syntax$ I_MPI_HYDRA_HOST_FILE=<arg>$ Arguments$ <arg> - String parameter$ -----------------------------------------------------------------------$ <hostsfile> - The full or relative path to the host file$ ----------------------------------------------------------------------- @Default:# not defined
20220327 015824.556 INFO             PET1 index=  64                                     I_MPI_HYDRA_HOSTS_GROUP : This environment variable allows to set node ranges using brackets,$ commas, and dashes (like in Slurm* Workload Manager). @Default:# not defined
20220327 015824.556 INFO             PET1 index=  65                                               I_MPI_PERHOST : Define the default behavior for the -perhost option of the mpiexec.hydra$ command.$ Syntax$ I_MPI_PERHOST=<value>$ Arguments$ <value> - Define a value used for -perhost by default$ -----------------------------------------------------------------------$ <integer > 0> - Exact value for the option$ <all>         - All logical CPUs on the node$ <allcores>    - All cores (physical CPUs) on the node. This is the$  default value.$ ----------------------------------------------------------------------- @Default:# not defined
20220327 015824.556 INFO             PET1 index=  66                                                 I_MPI_GTOOL : Specify the tools to be launched for selected ranks. An alternative to$ this variable is the -gtool option$ Syntax$ I_MPI_GTOOL="<command line for a tool 1>:<ranks set 1>[=exclusive]$ [@arch 1];<command line for a tool 2>:<ranks set 2>[=exclusive]$ [@arch 2]; â€¦ ;<command line for a tool n>:<ranks set n>[=exclusive]$ [@arch n]"$ Arguments$ <arg> - Specify a tool launch command, including parameters.$ -----------------------------------------------------------------------$ <command line>     - Specify tool launch command, including parameters$ <rank set>         - Specify the range of ranks that are involved in the$  tool execution. Separate ranks with a comma or use the '-' symbol for$ a set ofcontiguous ranks. To run the tool forall ranks, use the all$  argument.$ [=exclusive]       - All cores (physical CPUs) on the node. This is$ the default value.$ [@arch]            - Specify the architecture on which the tool runs$  optional). For a given <rank set>, if you specify this argument,$ the tool is launched
20220327 015824.556 INFO             PET1 index=  67                                    I_MPI_HYDRA_BRANCH_COUNT : Set this environment variable to restrict the number of child management$ processes launched by the mpiexec.hydra operation or by each pmi_proxy$ anagement process.$ Syntax$ I_MPI_HYDRA_BRANCH_COUNT=<num>$ Arguments$ <value> - Number$ -----------------------------------------------------------------------$ <n> >= 0 - The default value is -1 if less than 128 nodes are used. $ This value also means that there is no hierarchical structure$ The default value is 32 if more than 127 nodes are used$ ----------------------------------------------------------------------- @Default:# not defined
20220327 015824.556 INFO             PET1 index=  68                                       I_MPI_HYDRA_BOOTSTRAP : Set this environment variable to specify the bootstrap server Syntax$ I_MPI_HYDRA_BOOTSTRAP=<arg>$ Arguments$ <arg> - String parameter$ -----------------------------------------------------------------------$ <ssh>     - Use secure shell. This is the default value$ <rsh>     - Use remote shell$ <pdsh>    - Use parallel distributed shell$ <pbsdsh>  - Use Torque* and PBS* pbsdsh command$ <fork>    - Use fork call$ <slurm>   - Use SLURM* srun command$ <ll>      - Use LoadLeveler* llspawn.stdio command$ <lsf>     - Use LSF* blaunch command$ <sge>     - Use Univa* Grid Engine* qrsh command$ ----------------------------------------------------------------------- @Default:# not defined
20220327 015824.556 INFO             PET1 index=  69                                              I_MPI_PIN_UNIT : @Default:# not defined
20220327 015824.556 INFO             PET1 index=  70                                                 I_MPI_STATS :
20220327 015824.556 INFO             PET1 index=  71                                             I_MPI_TIMER_ART : @Default:# 1
20220327 015824.556 INFO             PET1 index=  72                                   I_MPI_OFFLOAD_DOMAIN_SIZE : Control the number of devices/tiles per MPI rank
20220327 015824.556 INFO             PET1 index=  73                                          I_MPI_OFFLOAD_CELL : Variable to choose the base unit: tile or device
20220327 015824.556 INFO             PET1 index=  74                                       I_MPI_OFFLOAD_DEVICES : Variable to select available devices
20220327 015824.556 INFO             PET1 index=  75                                   I_MPI_OFFLOAD_DEVICE_LIST : A comma-separated list of tiles and/or ranges of tiles.$ The process with the i-th rank is pinned to the i-th tile in the list.$ If I_MPI_OFFLOAD_CELL=device then it is comma-separated list of devices.
20220327 015824.556 INFO             PET1 index=  76                                        I_MPI_OFFLOAD_DOMAIN : Define domains through the comma separated list of hexadecimal numbers (domain masks).
20220327 015824.556 INFO             PET1 index=  77                               I_MPI_OFFLOAD_INFO_SET_NTILES : Set the number of tiles
20220327 015824.556 INFO             PET1 index=  78                                I_MPI_OFFLOAD_INFO_SET_NGPUS : Set the number of gpus
20220327 015824.556 INFO             PET1 index=  79                           I_MPI_OFFLOAD_INFO_SET_NNUMANODES : Set the number of numanodes
20220327 015824.556 INFO             PET1 index=  80                               I_MPI_OFFLOAD_INFO_SET_GPU_ID : Set gpu id for each tile
20220327 015824.556 INFO             PET1 index=  81                 I_MPI_OFFLOAD_INFO_SET_NUMANODE_ID_FOR_GPUS : Set numanode id for each gpu
20220327 015824.556 INFO             PET1 index=  82                I_MPI_OFFLOAD_INFO_SET_NUMANODE_ID_FOR_RANKS : Set numanode id for each rank
20220327 015824.556 INFO             PET1 index=  83                            I_MPI_OFFLOAD_INFO_SET_VENDOR_ID : Set vendor id for each device
20220327 015824.557 INFO             PET1 index=  84                                         MPIR_CVAR_INTEL_PIN : Turn on/off process pinning.$ Syntax$ I_MPI_PIN=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 - Enable process pinning$ > disable | no | off | 0 - Disable processes pinning$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN @Default:# on
20220327 015824.557 INFO             PET1 index=  85                          MPIR_CVAR_INTEL_PIN_SHOW_REAL_MASK : Turn on/off real masks pinning print.$ Syntax$ I_MPI_PIN_SHOW_REAL_MASK=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 - Enable real pinning print$ > disable | no | off | 0 - Disable real pinning print$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_SHOW_REAL_MASK @Default:# on
20220327 015824.557 INFO             PET1 index=  86                          MPIR_CVAR_INTEL_PIN_PROCESSOR_LIST : Define a processor subset and the mapping rules for MPI processes within$ this subset.$ Syntax$ I_MPI_PIN_PROCESSOR_LIST=<value>$ The environment variable value has the following syntax forms:$ 1. <proclist>$ 2. [<procset>][:[grain=<grain>][,shift=<shift>]$ [,preoffset=<preoffset>][,postoffset=<postoffset>]$ 3. [<procset>][:map=<map>]$ @Alias:# I_MPI_PIN_PROCESSOR_LIST @Default:# not defined
20220327 015824.557 INFO             PET1 index=  87                  MPIR_CVAR_INTEL_PIN_PROCESSOR_EXCLUDE_LIST : Define a subset of logical processors to be excluded for the pinning$ capability on the intended hosts.$ Syntax$ I_MPI_PIN_PROCESSOR_EXCLUDE_LIST=<proclist>$ Arguments$ <proclist> - A comma-separated list of logical processor numbers$ and/or ranges of processors.$ -----------------------------------------------------------------------$ > <l> - Processor with logical number <l>.$ > <l>-<m> - Range of processors with logical numbers from <l> to <m>.$ > <k>,<l>-<m> - Processors <k>, as well as <l> through <m>.$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_PROCESSOR_EXCLUDE_LIST @Default:# not defined
20220327 015824.557 INFO             PET1 index=  88                                    MPIR_CVAR_INTEL_PIN_CELL : Set this environment variable to define the pinning resolution$ granularity. I_MPI_PIN_CELL specifies the minimal processor cell$ allocated when an MPI process is running.$ Syntax$ I_MPI_PIN_CELL=<cell>$ Arguments$ <cell> - Specify the resolution granularity$ -----------------------------------------------------------------------$ > unit - Basic processor unit (logical CPU)$ > core - Physical processor core$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_CELL @Default:# unit
20220327 015824.557 INFO             PET1 index=  89                          MPIR_CVAR_INTEL_PIN_RESPECT_CPUSET : Respect the process affinity mask.$ Syntax$ I_MPI_PIN_RESPECT_CPUSET=<value>$ Arguments$ <value> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 - Respect the process affinity mask$ > disable | no | off | 0 - Do not respect the process affinity mask$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_RESPECT_CPUSET @Default:# on
20220327 015824.557 INFO             PET1 index=  90                             MPIR_CVAR_INTEL_PIN_RESPECT_HCA : In the presence of Intel(R) Omni-Path Architecture (Intel(R) OPA) or$ Infiniband architecture* host channel adapter (IBA* HCA),$ adjust the pinning according to the location of adapter.$ Syntax$ I_MPI_PIN_RESPECT_HCA=<value>$ Arguments$ <value> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 Use the location of IBA HCA if available$ > disable | no | off | 0 Do not use the location of IBA HCA$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_RESPECT_HCA @Default:# on
20220327 015824.557 INFO             PET1 index=  91                                  MPIR_CVAR_INTEL_PIN_DOMAIN : Intel(R) MPI Library provides environment variable to control process pinning for hybrid MPI/OpenMP* applications. This environment variable is used to define a number of non-overlapping subsets (domains) of logical processors on a node, and a set of rules on how MPI processes are bound to these domains by the following formula: one MPI process per one domain. Multi-core Shape:$ I_MPI_PIN_DOMAIN=<mc-shape>$ <mc-shape> - Define domains through multi-core terms.$ -----------------------------------------------------------------------$ > core - Each domain consists of the logical processors that share a$ particular core. The number of domains on a node is equal to the number$ of cores on the node.$ > socket | sock - Each domain consists of the logical processors that$ share a particular socket. The number of domains on a node is equal to$ the number of sockets on the node.$ > numa - Each domain consists of the logical processors that share a$ particular NUMA node. The number of domains on a machine is equal to$
20220327 015824.557 INFO             PET1 index=  92                                   MPIR_CVAR_INTEL_PIN_ORDER : Set this environment variable to define the mapping order for MPI$ processes to domains as specified by the$ I_MPI_PIN_DOMAIN environment variable.$ Syntax$ I_MPI_PIN_ORDER=<order>$ <order> - Specify the ranking order$ -----------------------------------------------------------------------$ > range - The domains are ordered according to the processor's BIOS$ numbering. This is a platform dependent numbering$ > scatter - The domains are ordered so that adjacent domains have$ minimal sharing of common resources$ > compact - The domains are ordered so that adjacent domains share$ common resources as much as possible. This is the default value$ > spread - The domains are ordered consecutively with the possibility$ not to share common resources$ > bunch - The processes are mapped proportionally to sockets and the$ domains are ordered as close as possible on the sockets$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_ORDER @Default:# compact
20220327 015824.557 INFO             PET1 index=  93                                   MPIR_CVAR_IMPI_HBW_POLICY : @Alias:# I_MPI_HBW_POLICY
20220327 015824.557 INFO             PET1 index=  94                          MPIR_CVAR_IMPI_INTERNAL_MEM_POLICY : @Alias:# I_MPI_INTERNAL_MEM_POLICY
20220327 015824.557 INFO             PET1 index=  95                                 MPIR_CVAR_IMPI_STATIC_BUILD : @Alias:# I_MPI_STATIC_BUILD
20220327 015824.557 INFO             PET1 index=  96                     MPIR_CVAR_IMPI_RETURN_INTERNAL_MEM_NUMA : @Alias:# I_MPI_RETURN_INTERNAL_MEM_NUMA
20220327 015824.557 INFO             PET1 index=  97                                   MPIR_CVAR_OFFLOAD_TOPOLIB : @Alias:# I_MPI_OFFLOAD_TOPOLIB
20220327 015824.557 INFO             PET1 index=  98                                        MPIR_CVAR_ENABLE_GPU : Control support of buffers offloaded to GPU/accelerator in MPI calls.$ Syntax$ I_MPI_OFFLOAD=<value>$ Arguments$ <value> - choice$ -----------------------------------------------------------------------$ <0> - Disabled$ <1> - Enabled only if Level Zero library is loaded at MPI_Init() time$ <2> - Enabled, will fail if Level Zero library is not loadable$ ----------------------------------------------------------------------- @Alias:# I_MPI_OFFLOAD @Default:# 0
20220327 015824.557 INFO             PET1 index=  99                        MPIR_CVAR_ENABLE_GPU_BUFFER_CHECKING : Turn on/off bounds checking of the offloaded buffers.$ @Alias:# I_MPI_OFFLOAD_BUFFER_CHECKING @Default:# 1
20220327 015824.557 INFO             PET1 index= 100                        MPIR_CVAR_OFFLOAD_LEVEL_ZERO_LIBRARY : Specify name or full path to Level Zero ze_loader library.$ @Alias:# I_MPI_OFFLOAD_LEVEL_ZERO_LIBRARY
20220327 015824.557 INFO             PET1 index= 101                            MPIR_CVAR_INTEL_EXTRA_FILESYSTEM : Turn on/off native parallel file systems support.$ Syntax$ I_MPI_EXTRA_FILESYSTEM=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 - Enable native support for parallel file$ systems.$ > disable | no | off | 0 - Disable native support for parallel file$ systems.$ ----------------------------------------------------------------------- @Alias:# I_MPI_EXTRA_FILESYSTEM @Default:# off
20220327 015824.557 INFO             PET1 index= 102                      MPIR_CVAR_INTEL_EXTRA_FILESYSTEM_FORCE : Force filesystem recognition logic.$ Syntax$ I_MPI_EXTRA_FILESYSTEM_FORCE=<ufs|nfs|gpfs|panfs|lustre|daos>$ @Alias:# I_MPI_EXTRA_FILESYSTEM_FORCE @Default:# not defined
20220327 015824.557 INFO             PET1 index= 103                                     MPIR_CVAR_INTEL_FABRICS : Select the particular fabrics to be used.$ Syntax$ I_MPI_FABRICS=<ofi|shm:ofi>$ Arguments$ <fabric> -  Define a network fabric.$ -----------------------------------------------------------------------$ > shm - Shared memory transport (used for intra-node$ communication only).$ > ofi - OpenFabrics Interfaces* (OFI)-capable network fabrics, such as$ Intel(R) True Scale Fabric, Intel(R) Omni-Path Architecture, InfiniBand*$ and Ethernet (through OFI$ API).$ ----------------------------------------------------------------------- @Alias:# I_MPI_FABRICS @Default:# shm:ofi
20220327 015824.557 INFO             PET1 index= 104                                       MPIR_CVAR_IMPI_MALLOC : Enable or disable the Intel MPI private memory allocator.$ ----------------------------------------------------------------------- @Alias:# I_MPI_MALLOC @Default:# 1
20220327 015824.557 INFO             PET1 index= 105                                    MPIR_CVAR_INTEL_SHM_HEAP : Enable or disable the Intel MPI shared memory allocator.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP @Default:# -1
20220327 015824.557 INFO             PET1 index= 106                                MPIR_CVAR_INTEL_SHM_HEAP_OPT : Shared memory heap optimization: "rank", "numa".$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_OPT @Default:# -1
20220327 015824.557 INFO             PET1 index= 107                              MPIR_CVAR_INTEL_SHM_HEAP_VSIZE : Set shared memory heap virtual size (in MB).$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_VSIZE @Default:# -1
20220327 015824.557 INFO             PET1 index= 108                              MPIR_CVAR_INTEL_SHM_HEAP_CSIZE : Set shared memory heap cache size (in MB).$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_CSIZE @Default:# -1
20220327 015824.557 INFO             PET1 index= 109                  MPIR_CVAR_INTEL_SHM_HEAP_NCONTIG_THRESHOLD : Set non-contig object size threshold for use shared memory allocator.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_NCONTIG_THRESHOLD @Default:# -1
20220327 015824.557 INFO             PET1 index= 110                          MPIR_CVAR_INTEL_SHM_HEAP_THRESHOLD : Set object size threshold for use shared memory allocator.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_THRESHOLD @Default:# -1
20220327 015824.557 INFO             PET1 index= 111        MPIR_CVAR_INTEL_SHM_RECV_HEAP_SHORT_MEMCPY_THRESHOLD : Threshold for short size messages receive via SHM HEAP transport.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_HEAP_SHORT_MEMCPY_THRESHOLD @Default:# -1
20220327 015824.557 INFO             PET1 index= 112      MPIR_CVAR_INTEL_SHM_RECV_HEAP_REGULAR_MEMCPY_THRESHOLD : Threshold for regular size message receive via SHM HEAP transport.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_HEAP_REGULAR_MEMCPY_THRESHOLD @Default:# -1
20220327 015824.557 INFO             PET1 index= 113            MPIR_CVAR_INTEL_SHM_RECV_INTER_HEAP_SHORT_MEMCPY : Name of memory copy function for short message receive via SHM HEAP transport, inter socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTER_HEAP_SHORT_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET1 index= 114            MPIR_CVAR_INTEL_SHM_RECV_INTRA_HEAP_SHORT_MEMCPY : Name of memory copy function for short message receive via SHM HEAP transport, intra socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTRA_HEAP_SHORT_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET1 index= 115          MPIR_CVAR_INTEL_SHM_RECV_INTER_HEAP_REGULAR_MEMCPY : Name of memory copy function for regular receive messages via SHM HEAP transport, inter socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTER_HEAP_REGULAR_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET1 index= 116          MPIR_CVAR_INTEL_SHM_RECV_INTRA_HEAP_REGULAR_MEMCPY : Name of memory copy function for regular receive messages via SHM HEAP transport, intra socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTRA_HEAP_REGULAR_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET1 index= 117                  MPIR_CVAR_INTEL_SHM_RECV_INTER_HEAP_MEMCPY : Name of memory copy function for receive messages via SHM HEAP transport, inter socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTER_HEAP_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET1 index= 118                  MPIR_CVAR_INTEL_SHM_RECV_INTRA_HEAP_MEMCPY : Name of memory copy function for receive messages via SHM HEAP transport, intra socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTRA_HEAP_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET1 index= 119                               MPIR_CVAR_CH4_SHM_POSIX_EAGER : Select a shared memory transport to be used.$ Syntax$ I_MPI_SHM=<transport>$ Arguments$ <transport> - Define a shared memory transport solution.$ -----------------------------------------------------------------------$ > disable | no | off | 0 - Do not use shared memory transport.$ > auto - Select a shared memory transport solution automatically.$ > bdw_sse - The shared memory transport solution tuned for Intel(R)$ microarchitecture code name Broadwell. The SSE/SSE2/SSE3 instruction$ set is used.$ > bdw_avx2 - The shared memory transport solution tuned for Intel(R)$ microarchitecture code name Broadwell. The AVX2 instruction set is used.$ > skx_sse - The shared memory transport solution tuned for Intel(R)$ Xeon(R) processors based on Intel(R) microarchitecture code name Skylake.$ The SSE/SSE2/SSE3 instruction set is used.$ > skx_avx2 - The shared memory transport solution tuned for Intel(R)$ Xeon(R) processors based on Intel(R) microarchitecture code name Skylake.$ The AVX2 instruction set is used.$ > skx_av
20220327 015824.557 INFO             PET1 index= 120                                     MPIR_CVAR_INTEL_SHM_OPT : Select a shared memory transport optimization strategy to be used.$ Syntax$ I_MPI_SHM_OPT=<optimization_strategy>$ Arguments$ <optimization_strategy> - Define a shared memory transport optimization strategy.$ -----------------------------------------------------------------------$ > dynamic - Let shared memory transport make decision in runtime.$ > intra - Optimize intra socket message passing.$ > inter - Optimize inter socket message passing.$ -----------------------------------------------------------------------$ @Alias:# I_MPI_SHM_OPT @Default:# dynamic
20220327 015824.557 INFO             PET1 index= 121                           MPIR_CVAR_INTEL_SHM_CELL_FWD_SIZE : Change the size of a shared memory forward cell. @Alias:# I_MPI_SHM_CELL_FWD_SIZE @Default:# -1
20220327 015824.557 INFO             PET1 index= 122                           MPIR_CVAR_INTEL_SHM_CELL_BWD_SIZE : Change the size of a shared memory backward cell. @Alias:# I_MPI_SHM_CELL_BWD_SIZE @Default:# -1
20220327 015824.557 INFO             PET1 index= 123                           MPIR_CVAR_INTEL_SHM_CELL_EXT_SIZE : Change the size of a shared memory extended cell. @Alias:# I_MPI_SHM_CELL_EXT_SIZE @Default:# -1
20220327 015824.557 INFO             PET1 index= 124                            MPIR_CVAR_INTEL_SHM_CELL_FWD_NUM : Change the number of forward cells in the shared memory transport (per rank). @Alias:# I_MPI_SHM_CELL_FWD_NUM @Default:# -1
20220327 015824.557 INFO             PET1 index= 125                       MPIR_CVAR_INTEL_SHM_CELL_FWD_HOLD_NUM : Change the number of hold forward cells in the shared memory transport (per rank). @Alias:# I_MPI_SHM_CELL_FWD_HOLD_NUM @Default:# -1
20220327 015824.557 INFO             PET1 index= 126                            MPIR_CVAR_INTEL_SHM_CELL_BWD_NUM : Change the number of backward cells in the shared memory transport (per rank). @Alias:# I_MPI_SHM_CELL_BWD_NUM @Default:# -1
20220327 015824.557 INFO             PET1 index= 127                     MPIR_CVAR_INTEL_SHM_CELL_BWD_NUMA_AWARE : Use NUMA aware backward cells (1 : true, 0 : false, -1 : auto) (per rank). @Alias:# I_MPI_SHM_CELL_BWD_NUMA_AWARE @Default:# -1
20220327 015824.557 INFO             PET1 index= 128                      MPIR_CVAR_INTEL_SHM_CELL_EXT_NUM_TOTAL : Change the total number of extended cells in the shared memory$ transport. @Alias:# I_MPI_SHM_CELL_EXT_NUM_TOTAL @Default:# -1
20220327 015824.557 INFO             PET1 index= 129                            MPIR_CVAR_INTEL_SHM_MCDRAM_LIMIT : Change the size of the shared memory bound to the multi-channel DRAM (MCDRAM) (size per rank). @Alias:# I_MPI_SHM_MCDRAM_LIMIT @Default:# -1
20220327 015824.557 INFO             PET1 index= 130                         MPIR_CVAR_INTEL_SHM_SEND_SPIN_COUNT : Control the spin count value for the shared memory transport for sending messages. @Alias:# I_MPI_SHM_SEND_SPIN_COUNT @Default:# -1
20220327 015824.557 INFO             PET1 index= 131                         MPIR_CVAR_INTEL_SHM_RECV_SPIN_COUNT : Control the spin count value for the shared memory transport for receiving messages. @Alias:# I_MPI_SHM_RECV_SPIN_COUNT @Default:# -1
20220327 015824.557 INFO             PET1 index= 132                          MPIR_CVAR_INTEL_SHM_FILE_PREFIX_4K : Mount point of 4K page size shm files. @Alias:# I_MPI_SHM_FILE_PREFIX_4K @Default:# ""
20220327 015824.557 INFO             PET1 index= 133                          MPIR_CVAR_INTEL_SHM_FILE_PREFIX_2M : Mount point of 2M huge page size shm files. @Alias:# I_MPI_SHM_FILE_PREFIX_2M @Default:# ""
20220327 015824.557 INFO             PET1 index= 134                          MPIR_CVAR_INTEL_SHM_FILE_PREFIX_1G : Mount point of 1G huge page size shm files. @Alias:# I_MPI_SHM_FILE_PREFIX_1G @Default:# ""
20220327 015824.557 INFO             PET1 index= 135                                   MPIR_CVAR_INTEL_SHM_PAUSE : The number of pauses repeated. @Alias:# I_MPI_SHM_PAUSE
20220327 015824.557 INFO             PET1 index= 136                         MPIR_CVAR_INTEL_SHM_EAGER_THRESHOLD : Eager threshold. @Alias:# I_MPI_SHM_EAGER_THRESHOLD
20220327 015824.557 INFO             PET1 index= 137                               MPIR_CVAR_INTEL_SHM_RING_SIZE : @Alias:# I_MPI_SHM_RING_SIZE
20220327 015824.557 INFO             PET1 index= 138                      MPIR_CVAR_INTEL_SHM_RING_ACK_THRESHOLD : @Alias:# I_MPI_SHM_RING_ACK_THRESHOLD
20220327 015824.557 INFO             PET1 index= 139                          MPIR_CVAR_INTEL_SHM_CELL_FWD_FIRST : @Alias:# I_MPI_SHM_CELL_FWD_FIRST
20220327 015824.557 INFO             PET1 index= 140                      MPIR_CVAR_INTEL_SHM_PROFILER_DIRECTORY : @Alias:# I_MPI_SHM_PROFILER_DIRECTORY
20220327 015824.557 INFO             PET1 index= 141                         MPIR_CVAR_INTEL_SHM_TRACE_DIRECTORY : @Alias:# I_MPI_SHM_TRACE_DIRECTORY
20220327 015824.557 INFO             PET1 index= 142                              MPIR_CVAR_INTEL_SHM_FRAME_SIZE : @Alias:# I_MPI_SHM_FRAME_SIZE
20220327 015824.557 INFO             PET1 index= 143                         MPIR_CVAR_INTEL_SHM_FRAME_THRESHOLD : @Alias:# I_MPI_SHM_FRAME_THRESHOLD
20220327 015824.557 INFO             PET1 index= 144                        MPIR_CVAR_INTEL_SHM_SEND_TINY_MEMCPY : @Alias:# I_MPI_SHM_SEND_TINY_MEMCPY
20220327 015824.557 INFO             PET1 index= 145                  MPIR_CVAR_INTEL_SHM_SEND_INTRA_RING_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_RING_MEMCPY
20220327 015824.557 INFO             PET1 index= 146                  MPIR_CVAR_INTEL_SHM_SEND_INTER_RING_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_RING_MEMCPY
20220327 015824.557 INFO             PET1 index= 147                  MPIR_CVAR_INTEL_SHM_RECV_INTRA_RING_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_RING_MEMCPY
20220327 015824.557 INFO             PET1 index= 148                  MPIR_CVAR_INTEL_SHM_RECV_INTER_RING_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_RING_MEMCPY
20220327 015824.557 INFO             PET1 index= 149              MPIR_CVAR_INTEL_SHM_SEND_INTRA_CELL_FWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_CELL_FWD_MEMCPY
20220327 015824.557 INFO             PET1 index= 150              MPIR_CVAR_INTEL_SHM_SEND_INTER_CELL_FWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_CELL_FWD_MEMCPY
20220327 015824.557 INFO             PET1 index= 151              MPIR_CVAR_INTEL_SHM_SEND_INTRA_CELL_BWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_CELL_BWD_MEMCPY
20220327 015824.557 INFO             PET1 index= 152              MPIR_CVAR_INTEL_SHM_SEND_INTER_CELL_BWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_CELL_BWD_MEMCPY
20220327 015824.557 INFO             PET1 index= 153                  MPIR_CVAR_INTEL_SHM_RECV_INTRA_CELL_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_CELL_MEMCPY
20220327 015824.557 INFO             PET1 index= 154                  MPIR_CVAR_INTEL_SHM_RECV_INTER_CELL_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_CELL_MEMCPY
20220327 015824.557 INFO             PET1 index= 155          MPIR_CVAR_INTEL_SHM_RECV_INTRA_CELL_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_CELL_REGULAR_MEMCPY
20220327 015824.557 INFO             PET1 index= 156          MPIR_CVAR_INTEL_SHM_RECV_INTER_CELL_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_CELL_REGULAR_MEMCPY
20220327 015824.557 INFO             PET1 index= 157             MPIR_CVAR_INTEL_SHM_SEND_INTRA_FRAME_FWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_FRAME_FWD_MEMCPY
20220327 015824.557 INFO             PET1 index= 158             MPIR_CVAR_INTEL_SHM_SEND_INTER_FRAME_FWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_FRAME_FWD_MEMCPY
20220327 015824.557 INFO             PET1 index= 159             MPIR_CVAR_INTEL_SHM_SEND_INTRA_FRAME_BWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_FRAME_BWD_MEMCPY
20220327 015824.557 INFO             PET1 index= 160             MPIR_CVAR_INTEL_SHM_SEND_INTER_FRAME_BWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_FRAME_BWD_MEMCPY
20220327 015824.557 INFO             PET1 index= 161                 MPIR_CVAR_INTEL_SHM_RECV_INTRA_FRAME_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_FRAME_MEMCPY
20220327 015824.557 INFO             PET1 index= 162                 MPIR_CVAR_INTEL_SHM_RECV_INTER_FRAME_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_FRAME_MEMCPY
20220327 015824.557 INFO             PET1 index= 163     MPIR_CVAR_INTEL_SHM_SEND_INTRA_FRAME_FWD_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_FRAME_FWD_REGULAR_MEMCPY
20220327 015824.557 INFO             PET1 index= 164     MPIR_CVAR_INTEL_SHM_SEND_INTER_FRAME_FWD_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_FRAME_FWD_REGULAR_MEMCPY
20220327 015824.557 INFO             PET1 index= 165     MPIR_CVAR_INTEL_SHM_SEND_INTRA_FRAME_BWD_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_FRAME_BWD_REGULAR_MEMCPY
20220327 015824.557 INFO             PET1 index= 166     MPIR_CVAR_INTEL_SHM_SEND_INTER_FRAME_BWD_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_FRAME_BWD_REGULAR_MEMCPY
20220327 015824.557 INFO             PET1 index= 167         MPIR_CVAR_INTEL_SHM_RECV_INTRA_FRAME_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_FRAME_REGULAR_MEMCPY
20220327 015824.557 INFO             PET1 index= 168         MPIR_CVAR_INTEL_SHM_RECV_INTER_FRAME_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_FRAME_REGULAR_MEMCPY
20220327 015824.557 INFO             PET1 index= 169                       MPIR_CVAR_INTEL_SHM_INPLACE_THRESHOLD : @Alias:# I_MPI_SHM_INPLACE_THRESHOLD
20220327 015824.557 INFO             PET1 index= 170              MPIR_CVAR_INTEL_SHM_SEND_TINY_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_TINY_MEMCPY_THRESHOLD
20220327 015824.557 INFO             PET1 index= 171      MPIR_CVAR_INTEL_SHM_RECV_CELL_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_RECV_CELL_REGULAR_MEMCPY_THRESHOLD
20220327 015824.557 INFO             PET1 index= 172MPIR_CVAR_INTEL_SHM_SEND_INTER_UNIDIR_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_INTER_UNIDIR_REGULAR_MEMCPY_THRESHOLD
20220327 015824.557 INFO             PET1 index= 173MPIR_CVAR_INTEL_SHM_SEND_INTER_BIDIR_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_INTER_BIDIR_REGULAR_MEMCPY_THRESHOLD
20220327 015824.557 INFO             PET1 index= 174MPIR_CVAR_INTEL_SHM_SEND_INTRA_UNIDIR_REGULAR_MEMCPY_THRESHOLD_MIN : @Alias:# I_MPI_SHM_SEND_INTRA_UNIDIR_REGULAR_MEMCPY_THRESHOLD_MIN
20220327 015824.557 INFO             PET1 index= 175MPIR_CVAR_INTEL_SHM_SEND_INTRA_UNIDIR_REGULAR_MEMCPY_THRESHOLD_MAX : @Alias:# I_MPI_SHM_SEND_INTRA_UNIDIR_REGULAR_MEMCPY_THRESHOLD_MAX
20220327 015824.557 INFO             PET1 index= 176MPIR_CVAR_INTEL_SHM_SEND_INTRA_BIDIR_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_INTRA_BIDIR_REGULAR_MEMCPY_THRESHOLD
20220327 015824.557 INFO             PET1 index= 177     MPIR_CVAR_INTEL_SHM_RECV_FRAME_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_RECV_FRAME_REGULAR_MEMCPY_THRESHOLD
20220327 015824.557 INFO             PET1 index= 178          MPIR_CVAR_INTEL_SHM_CELL_FWD_UNIDIR_INTRA_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_UNIDIR_INTRA_CAPACITY
20220327 015824.557 INFO             PET1 index= 179  MPIR_CVAR_INTEL_SHM_CELL_FWD_UNIDIR_INTER_REGULAR_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_UNIDIR_INTER_REGULAR_CAPACITY
20220327 015824.557 INFO             PET1 index= 180MPIR_CVAR_INTEL_SHM_CELL_FWD_UNIDIR_INTER_NONTEMPORAL_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_UNIDIR_INTER_NONTEMPORAL_CAPACITY
20220327 015824.557 INFO             PET1 index= 181           MPIR_CVAR_INTEL_SHM_CELL_FWD_BIDIR_INTRA_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_BIDIR_INTRA_CAPACITY
20220327 015824.557 INFO             PET1 index= 182   MPIR_CVAR_INTEL_SHM_CELL_FWD_BIDIR_INTER_REGULAR_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_BIDIR_INTER_REGULAR_CAPACITY
20220327 015824.557 INFO             PET1 index= 183MPIR_CVAR_INTEL_SHM_CELL_FWD_BIDIR_INTER_NONTEMPORAL_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_BIDIR_INTER_NONTEMPORAL_CAPACITY
20220327 015824.558 INFO             PET1 index= 184MPIR_CVAR_INTEL_SHM_SEND_FWD_FRAME_NONTEMPORAL_MEMCPY_THRESHOLD_MIN : @Alias:# I_MPI_SHM_SEND_FWD_FRAME_NONTEMPORAL_MEMCPY_THRESHOLD_MIN
20220327 015824.558 INFO             PET1 index= 185MPIR_CVAR_INTEL_SHM_SEND_FWD_FRAME_NONTEMPORAL_MEMCPY_THRESHOLD_MAX : @Alias:# I_MPI_SHM_SEND_FWD_FRAME_NONTEMPORAL_MEMCPY_THRESHOLD_MAX
20220327 015824.558 INFO             PET1 index= 186MPIR_CVAR_INTEL_SHM_SEND_FWD_CELL_NONTEMPORAL_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_FWD_CELL_NONTEMPORAL_MEMCPY_THRESHOLD
20220327 015824.558 INFO             PET1 index= 187                                MPIR_CVAR_INTEL_SHM_HEAP_THP : @Alias:# I_MPI_SHM_HEAP_THP
20220327 015824.558 INFO             PET1 index= 188                                     MPIR_CVAR_INTEL_SHM_THP : @Alias:# I_MPI_SHM_THP
20220327 015824.558 INFO             PET1 index= 189                                  MPIR_CVAR_OFI_USE_PROVIDER : Define the name of the OFI provider to load.$ Syntax$ I_MPI_OFI_PROVIDER=<name>$ Arguments$ <name> - The name of the OFI provider to load @Alias:# I_MPI_OFI_PROVIDER @Default:# not defined
20220327 015824.558 INFO             PET1 index= 190                                MPIR_CVAR_OFI_DUMP_PROVIDERS : Control the capability of printing information about all OFI providers$ and their attributes from an OFI library.$ Syntax$ I_MPI_OFI_PROVIDER_DUMP=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > yes | on | 1 - Print the list of all OFI providers and their$ attributes from an OFI library$ > no | off | 0 - No action$ ----------------------------------------------------------------------- @Alias:# I_MPI_OFI_PROVIDER_DUMP @Default:# off
20220327 015824.558 INFO             PET1 index= 191                        MPIR_CVAR_CH4_OFI_ENABLE_DIRECT_RECV : Control the capability of the direct receive in the OFI fabric.$ Syntax$ I_MPI_OFI_DRECV=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > 1 - Enable direct receive$ > 0 - Disable direct receive$ ----------------------------------------------------------------------- @Alias:# I_MPI_OFI_DRECV @Default:# not defined
20220327 015824.558 INFO             PET1 index= 192                                  MPIR_CVAR_OFI_MAX_MSG_SIZE : @Alias:# I_MPI_OFI_MAX_MSG_SIZE
20220327 015824.558 INFO             PET1 index= 193                                  MPIR_CVAR_OFI_LMT_WIN_SIZE : @Alias:# I_MPI_OFI_LMT_WIN_SIZE
20220327 015824.558 INFO             PET1 index= 194                    MPIR_CVAR_CH4_OFI_ISEND_INJECT_THRESHOLD : @Alias:# I_MPI_OFI_ISEND_INJECT_THRESHOLD
20220327 015824.558 INFO             PET1 index= 195                     MPIR_CVAR_CH4_OFI_ADDRESS_EXCHANGE_MODE : @Alias:# I_MPI_STARTUP_MODE
20220327 015824.558 INFO             PET1 index= 196                     MPIR_CVAR_CH4_OFI_LARGE_SCALE_THRESHOLD : @Alias:# I_MPI_LARGE_SCALE_THRESHOLD
20220327 015824.558 INFO             PET1 index= 197                   MPIR_CVAR_CH4_OFI_EXTREME_SCALE_THRESHOLD : @Alias:# I_MPI_EXTREME_SCALE_THRESHOLD
20220327 015824.558 INFO             PET1 index= 198                        MPIR_CVAR_CH4_OFI_DYNAMIC_CONNECTION : @Alias:# I_MPI_DYNAMIC_CONNECTION
20220327 015824.558 INFO             PET1 index= 199                              MPIR_CVAR_CH4_OFI_EXPERIMENTAL : @Alias:# I_MPI_OFI_EXPERIMENTAL @Default:# 0
20220327 015824.558 INFO             PET1 index= 200                     MPIR_CVAR_CH4_OFI_CAPABILITY_SETS_DEBUG : Prints out the configuration of each capability selected via the capability sets interface.
20220327 015824.558 INFO             PET1 index= 201                               MPIR_CVAR_CH4_OFI_ENABLE_DATA : Enable immediate data fields in OFI to transmit source rank outside of the match bits
20220327 015824.558 INFO             PET1 index= 202                           MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE : If true, the OFI addressing information will be stored with an FI_AV_TABLE. If false, an FI_AV_MAP will be used.
20220327 015824.558 INFO             PET1 index= 203                 MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS : If true, use OFI scalable endpoints.
20220327 015824.558 INFO             PET1 index= 204                             MPIR_CVAR_CH4_OFI_MAX_ENDPOINTS : Specifies the maximum number of OFI endpoints that can be used by the OFI provider. The default value is -1, indicating that no value is set.
20220327 015824.558 INFO             PET1 index= 205                        MPIR_CVAR_CH4_OFI_ENABLE_MR_SCALABLE : If true, MR_SCALABLE for OFI memory regions. If false, MR_BASIC for OFI memory regions.
20220327 015824.558 INFO             PET1 index= 206                             MPIR_CVAR_CH4_OFI_ENABLE_TAGGED : If true, use tagged message transmission functions in OFI.
20220327 015824.558 INFO             PET1 index= 207                                 MPIR_CVAR_CH4_OFI_ENABLE_AM : If true, enable OFI active message support.
20220327 015824.558 INFO             PET1 index= 208                                MPIR_CVAR_CH4_OFI_ENABLE_RMA : If true, enable OFI RMA support for MPI RMA operations. OFI support for basic RMA is always required to implement large messgage transfers in the active message code path.
20220327 015824.558 INFO             PET1 index= 209                            MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS : If true, enable OFI Atomics support.
20220327 015824.558 INFO             PET1 index= 210                       MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS : Specifies the maximum number of iovecs that can be used by the OFI provider for fetch_atomic operations. The default value is -1, indicating that no value is set.
20220327 015824.558 INFO             PET1 index= 211                 MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS : If true, enable MPI data auto progress.
20220327 015824.558 INFO             PET1 index= 212              MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS : If true, enable MPI control auto progress.
20220327 015824.558 INFO             PET1 index= 213                       MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK : If true, enable iovec for pt2pt.
20220327 015824.558 INFO             PET1 index= 214                                 MPIR_CVAR_CH4_OFI_RANK_BITS : Specifies the number of bits that will be used for matching the MPI rank. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20220327 015824.558 INFO             PET1 index= 215                                  MPIR_CVAR_CH4_OFI_TAG_BITS : Specifies the number of bits that will be used for matching the user tag. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20220327 015824.558 INFO             PET1 index= 216                             MPIR_CVAR_CH4_OFI_MAJOR_VERSION : Specifies the major version of the OFI library. The default is the major version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20220327 015824.558 INFO             PET1 index= 217                             MPIR_CVAR_CH4_OFI_MINOR_VERSION : Specifies the major version of the OFI library. The default is the minor version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20220327 015824.558 INFO             PET1 index= 218                             MPIR_CVAR_CH4_OFI_ZERO_OP_FLAGS : Zeroes rx_attr.op_flags and tx_attr.op_flags, disables use of FI_SELECTIVE_COMPLETION. Can give more optimized behavior of underlying provider.
20220327 015824.558 INFO             PET1 index= 219                            MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS : Specifies the number of buffers for receiving active messages.
20220327 015824.558 INFO             PET1 index= 220                        MPIR_CVAR_INTEL_COLL_DIRECT_PROGRESS : @Alias:# I_MPI_COLL_DIRECT_PROGRESS
20220327 015824.558 INFO             PET1 index= 221                                      MPIR_CVAR_ENABLE_HCOLL : @Alias:# I_MPI_COLL_EXTERNAL
20220327 015824.558 INFO             PET1 index= 222                                     MPIR_CVAR_USE_ALLGATHER : Control selection of MPI_Allgather algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-6 @Alias:# I_MPI_ADJUST_ALLGATHER @Default:# -1
20220327 015824.558 INFO             PET1 index= 223                                MPIR_CVAR_USE_ALLGATHER_LIST : @Alias:# I_MPI_ADJUST_ALLGATHER_LIST @Default:# -1
20220327 015824.558 INFO             PET1 index= 224                         MPIR_CVAR_USE_ALLGATHER_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLGATHER_COMPOSITION @Default:# -1
20220327 015824.558 INFO             PET1 index= 225               MPIR_CVAR_ALLGATHER_COMPOSITION_DELTA_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLGATHER_COMPOSITION_DELTA_SEGSIZE @Default:# -1
20220327 015824.558 INFO             PET1 index= 226                             MPIR_CVAR_USE_ALLGATHER_NETWORK : range: 0-5 @Alias:# I_MPI_ADJUST_ALLGATHER_NETWORK @Default:# -1
20220327 015824.558 INFO             PET1 index= 227                                MPIR_CVAR_USE_ALLGATHER_NODE : range: 0-4 @Alias:# I_MPI_ADJUST_ALLGATHER_NODE @Default:# -1
20220327 015824.558 INFO             PET1 index= 228                                    MPIR_CVAR_USE_ALLGATHERV : Control selection of MPI_Allgatherv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_ALLGATHERV @Default:# -1
20220327 015824.558 INFO             PET1 index= 229                               MPIR_CVAR_USE_ALLGATHERV_LIST : @Alias:# I_MPI_ADJUST_ALLGATHERV_LIST @Default:# -1
20220327 015824.558 INFO             PET1 index= 230                        MPIR_CVAR_USE_ALLGATHERV_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLGATHERV_COMPOSITION @Default:# -1
20220327 015824.558 INFO             PET1 index= 231                            MPIR_CVAR_USE_ALLGATHERV_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_ALLGATHERV_NETWORK @Default:# -1
20220327 015824.558 INFO             PET1 index= 232                               MPIR_CVAR_USE_ALLGATHERV_NODE : range: 0-3 @Alias:# I_MPI_ADJUST_ALLGATHERV_NODE @Default:# -1
20220327 015824.558 INFO             PET1 index= 233                   MPIR_CVAR_ALLGATHER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ALLGATHER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET1 index= 234                      MPIR_CVAR_ALLGATHER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ALLGATHER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET1 index= 235                    MPIR_CVAR_SCATTERV_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_SCATTERV_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET1 index= 236                       MPIR_CVAR_SCATTERV_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_SCATTERV_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET1 index= 237                     MPIR_CVAR_SCATTER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_SCATTER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET1 index= 238                        MPIR_CVAR_SCATTER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_SCATTER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET1 index= 239                                     MPIR_CVAR_USE_ALLREDUCE : Control selection of MPI_Allreduce algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-25 @Alias:# I_MPI_ADJUST_ALLREDUCE @Default:# -1
20220327 015824.558 INFO             PET1 index= 240                                MPIR_CVAR_USE_ALLREDUCE_LIST : @Alias:# I_MPI_ADJUST_ALLREDUCE_LIST @Default:# -1
20220327 015824.558 INFO             PET1 index= 241                         MPIR_CVAR_USE_ALLREDUCE_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLREDUCE_COMPOSITION @Default:# -1
20220327 015824.558 INFO             PET1 index= 242                             MPIR_CVAR_USE_ALLREDUCE_NETWORK : range: 0-16 @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK @Default:# -1
20220327 015824.558 INFO             PET1 index= 243                                MPIR_CVAR_USE_ALLREDUCE_NODE : range: 0-9 @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE @Default:# -1
20220327 015824.558 INFO             PET1 index= 244               MPIR_CVAR_ALLREDUCE_NETWORK_MULTIPLYING_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_MULTIPLYING_RADIX @Default:# -1
20220327 015824.558 INFO             PET1 index= 245                  MPIR_CVAR_ALLREDUCE_NODE_MULTIPLYING_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_MULTIPLYING_RADIX @Default:# -1
20220327 015824.558 INFO             PET1 index= 246                   MPIR_CVAR_ALLREDUCE_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET1 index= 247                      MPIR_CVAR_ALLREDUCE_NETWORK_KARY_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_KARY_RADIX @Default:# -1
20220327 015824.558 INFO             PET1 index= 248                      MPIR_CVAR_ALLREDUCE_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET1 index= 249                         MPIR_CVAR_ALLREDUCE_NODE_KARY_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_KARY_RADIX @Default:# -1
20220327 015824.558 INFO             PET1 index= 250                    MPIR_CVAR_ALLREDUCE_NODE_SHM_GATHER_TYPE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHM_GATHER_TYPE @Default:# -1
20220327 015824.558 INFO             PET1 index= 251                   MPIR_CVAR_ALLREDUCE_NODE_SHM_GATHER_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHM_GATHER_RADIX @Default:# -1
20220327 015824.558 INFO             PET1 index= 252                   MPIR_CVAR_ALLREDUCE_NODE_SHM_RELEASE_TYPE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHM_RELEASE_TYPE @Default:# -1
20220327 015824.558 INFO             PET1 index= 253                  MPIR_CVAR_ALLREDUCE_NODE_SHM_RELEASE_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHM_RELEASE_RADIX @Default:# -1
20220327 015824.558 INFO             PET1 index= 254                MPIR_CVAR_ALLREDUCE_NETWORK_SHUMILIN_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_SHUMILIN_SEGSIZE @Default:# -1
20220327 015824.558 INFO             PET1 index= 255                   MPIR_CVAR_ALLREDUCE_NODE_SHUMILIN_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHUMILIN_SEGSIZE @Default:# -1
20220327 015824.558 INFO             PET1 index= 256                              MPIR_CVAR_ALLREDUCE_ZETA_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_ZETA_RADIX @Default:# -1
20220327 015824.558 INFO             PET1 index= 257                           MPIR_CVAR_ALLREDUCE_ZETA_SHM_TYPE : @Alias:# I_MPI_ADJUST_ALLREDUCE_ZETA_SHM_TYPE @Default:# -1
20220327 015824.558 INFO             PET1 index= 258                              MPIR_CVAR_ALLREDUCE_IOTA_NLEAD : @Alias:# I_MPI_ADJUST_ALLREDUCE_IOTA_NLEAD @Default:# -1
20220327 015824.558 INFO             PET1 index= 259               MPIR_CVAR_ALLREDUCE_NODE_NREDUCE_NB_ALLGATHER : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_NREDUCE_NB_ALLGATHER @Default:# -1
20220327 015824.558 INFO             PET1 index= 260            MPIR_CVAR_ALLREDUCE_NETWORK_NREDUCE_NB_ALLGATHER : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_NREDUCE_NB_ALLGATHER @Default:# -1
20220327 015824.558 INFO             PET1 index= 261                MPIR_CVAR_ALLREDUCE_NODE_NREDUCE_NB_ALLTOALL : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_NREDUCE_NB_ALLTOALL @Default:# -1
20220327 015824.558 INFO             PET1 index= 262             MPIR_CVAR_ALLREDUCE_NETWORK_NREDUCE_NB_ALLTOALL : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_NREDUCE_NB_ALLTOALL @Default:# -1
20220327 015824.558 INFO             PET1 index= 263                    MPIR_CVAR_ALLREDUCE_NODE_NREDUCE_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_NREDUCE_SEGSIZE @Default:# -1
20220327 015824.558 INFO             PET1 index= 264                 MPIR_CVAR_ALLREDUCE_NETWORK_NREDUCE_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_NREDUCE_SEGSIZE @Default:# -1
20220327 015824.558 INFO             PET1 index= 265                                      MPIR_CVAR_USE_ALLTOALL : Control selection of MPI_Alltoall algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-13 @Alias:# I_MPI_ADJUST_ALLTOALL @Default:# -1
20220327 015824.558 INFO             PET1 index= 266                                 MPIR_CVAR_USE_ALLTOALL_LIST : @Alias:# I_MPI_ADJUST_ALLTOALL_LIST @Default:# -1
20220327 015824.558 INFO             PET1 index= 267                          MPIR_CVAR_USE_ALLTOALL_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLTOALL_COMPOSITION @Default:# -1
20220327 015824.558 INFO             PET1 index= 268                              MPIR_CVAR_USE_ALLTOALL_NETWORK : range: 0-7 @Alias:# I_MPI_ADJUST_ALLTOALL_NETWORK @Default:# -1
20220327 015824.558 INFO             PET1 index= 269                                 MPIR_CVAR_USE_ALLTOALL_NODE : range: 0-6 @Alias:# I_MPI_ADJUST_ALLTOALL_NODE @Default:# -1
20220327 015824.558 INFO             PET1 index= 270                MPIR_CVAR_ALLTOALL_COMPOSITION_GAMMA_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLTOALL_COMPOSITION_GAMMA_SEGSIZE @Default:# -1
20220327 015824.558 INFO             PET1 index= 271                                 MPIR_CVAR_ALLTOALL_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALL_SCATTERED_THROTTLE @Default:# -1
20220327 015824.558 INFO             PET1 index= 272               MPIR_CVAR_ALLTOALL_NETWORK_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALL_NETWORK_SCATTERED_THROTTLE @Default:# -1
20220327 015824.558 INFO             PET1 index= 273                  MPIR_CVAR_ALLTOALL_NODE_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALL_NODE_SCATTERED_THROTTLE @Default:# -1
20220327 015824.558 INFO             PET1 index= 274                             MPIR_CVAR_ALLTOALL_BRUCKS_RADIX : @Alias:# I_MPI_ADJUST_ALLTOALL_BRUCKS_RADIX @Default:# -1
20220327 015824.558 INFO             PET1 index= 275                     MPIR_CVAR_ALLTOALL_NETWORK_BRUCKS_RADIX : @Alias:# I_MPI_ADJUST_ALLTOALL_NETWORK_BRUCKS_RADIX @Default:# -1
20220327 015824.558 INFO             PET1 index= 276                        MPIR_CVAR_ALLTOALL_NODE_BRUCKS_RADIX : @Alias:# I_MPI_ADJUST_ALLTOALL_NODE_BRUCKS_RADIX @Default:# -1
20220327 015824.558 INFO             PET1 index= 277                                     MPIR_CVAR_USE_ALLTOALLV : Control selection of MPI_Alltoallv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-9 @Alias:# I_MPI_ADJUST_ALLTOALLV @Default:# -1
20220327 015824.558 INFO             PET1 index= 278                                MPIR_CVAR_USE_ALLTOALLV_LIST : @Alias:# I_MPI_ADJUST_ALLTOALLV_LIST @Default:# -1
20220327 015824.558 INFO             PET1 index= 279                         MPIR_CVAR_USE_ALLTOALLV_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLTOALLV_COMPOSITION @Default:# -1
20220327 015824.558 INFO             PET1 index= 280                             MPIR_CVAR_USE_ALLTOALLV_NETWORK : range: 0-6 @Alias:# I_MPI_ADJUST_ALLTOALLV_NETWORK @Default:# -1
20220327 015824.558 INFO             PET1 index= 281                                MPIR_CVAR_USE_ALLTOALLV_NODE : range: 0-5 @Alias:# I_MPI_ADJUST_ALLTOALLV_NODE @Default:# -1
20220327 015824.558 INFO             PET1 index= 282                                MPIR_CVAR_ALLTOALLV_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLV_SCATTERED_THROTTLE @Default:# -1
20220327 015824.558 INFO             PET1 index= 283              MPIR_CVAR_ALLTOALLV_NETWORK_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLV_NETWORK_SCATTERED_THROTTLE @Default:# -1
20220327 015824.558 INFO             PET1 index= 284                 MPIR_CVAR_ALLTOALLV_NODE_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLV_NODE_SCATTERED_THROTTLE @Default:# -1
20220327 015824.558 INFO             PET1 index= 285                                     MPIR_CVAR_USE_ALLTOALLW : Control selection of MPI_Alltoallw algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-6 @Alias:# I_MPI_ADJUST_ALLTOALLW @Default:# -1
20220327 015824.558 INFO             PET1 index= 286                                MPIR_CVAR_USE_ALLTOALLW_LIST : @Alias:# I_MPI_ADJUST_ALLTOALLW_LIST @Default:# -1
20220327 015824.559 INFO             PET1 index= 287                         MPIR_CVAR_USE_ALLTOALLW_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLTOALLW_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET1 index= 288                             MPIR_CVAR_USE_ALLTOALLW_NETWORK : @Alias:# I_MPI_ADJUST_ALLTOALLW_NETWORK @Default:# -1
20220327 015824.559 INFO             PET1 index= 289                                MPIR_CVAR_USE_ALLTOALLW_NODE : @Alias:# I_MPI_ADJUST_ALLTOALLW_NODE @Default:# -1
20220327 015824.559 INFO             PET1 index= 290                                MPIR_CVAR_ALLTOALLW_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLW_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET1 index= 291              MPIR_CVAR_ALLTOALLW_NETWORK_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLW_NETWORK_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET1 index= 292                 MPIR_CVAR_ALLTOALLW_NODE_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLW_NODE_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET1 index= 293                                       MPIR_CVAR_USE_BARRIER : Control selection of MPI_Barrier algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-11 @Alias:# I_MPI_ADJUST_BARRIER @Default:# -1
20220327 015824.559 INFO             PET1 index= 294                                  MPIR_CVAR_USE_BARRIER_LIST : @Alias:# I_MPI_ADJUST_BARRIER_LIST @Default:# -1
20220327 015824.559 INFO             PET1 index= 295                           MPIR_CVAR_USE_BARRIER_COMPOSITION : @Alias:# I_MPI_ADJUST_BARRIER_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET1 index= 296                               MPIR_CVAR_USE_BARRIER_NETWORK : range: 0-6 @Alias:# I_MPI_ADJUST_BARRIER_NETWORK @Default:# -1
20220327 015824.559 INFO             PET1 index= 297                                  MPIR_CVAR_USE_BARRIER_NODE : range: 0-4 @Alias:# I_MPI_ADJUST_BARRIER_NODE @Default:# -1
20220327 015824.559 INFO             PET1 index= 298                 MPIR_CVAR_BARRIER_NETWORK_MULTIPLYING_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NETWORK_MULTIPLYING_RADIX @Default:# -1
20220327 015824.559 INFO             PET1 index= 299                     MPIR_CVAR_BARRIER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET1 index= 300                        MPIR_CVAR_BARRIER_NETWORK_KARY_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NETWORK_KARY_RADIX @Default:# -1
20220327 015824.559 INFO             PET1 index= 301          MPIR_CVAR_BARRIER_NETWORK_RECURSIVE_EXCHANGE_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NETWORK_RECURSIVE_EXCHANGE_RADIX @Default:# -1
20220327 015824.559 INFO             PET1 index= 302                        MPIR_CVAR_BARRIER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET1 index= 303                           MPIR_CVAR_BARRIER_NODE_KARY_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_KARY_RADIX @Default:# -1
20220327 015824.559 INFO             PET1 index= 304             MPIR_CVAR_BARRIER_NODE_RECURSIVE_EXCHANGE_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_RECURSIVE_EXCHANGE_RADIX @Default:# -1
20220327 015824.559 INFO             PET1 index= 305                      MPIR_CVAR_BARRIER_NODE_SHM_GATHER_TYPE : @Alias:# I_MPI_ADJUST_BARRIER_NODE_SHM_GATHER_TYPE @Default:# -1
20220327 015824.559 INFO             PET1 index= 306                     MPIR_CVAR_BARRIER_NODE_SHM_GATHER_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_SHM_GATHER_RADIX @Default:# -1
20220327 015824.559 INFO             PET1 index= 307                     MPIR_CVAR_BARRIER_NODE_SHM_RELEASE_TYPE : @Alias:# I_MPI_ADJUST_BARRIER_NODE_SHM_RELEASE_TYPE @Default:# -1
20220327 015824.559 INFO             PET1 index= 308                    MPIR_CVAR_BARRIER_NODE_SHM_RELEASE_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_SHM_RELEASE_RADIX @Default:# -1
20220327 015824.559 INFO             PET1 index= 309                                MPIR_CVAR_BARRIER_ZETA_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_ZETA_RADIX @Default:# -1
20220327 015824.559 INFO             PET1 index= 310                             MPIR_CVAR_BARRIER_ZETA_SHM_TYPE : @Alias:# I_MPI_ADJUST_BARRIER_ZETA_SHM_TYPE @Default:# -1
20220327 015824.559 INFO             PET1 index= 311                                         MPIR_CVAR_USE_BCAST : Control selection of MPI_Bcast algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-18 @Alias:# I_MPI_ADJUST_BCAST @Default:# -1
20220327 015824.559 INFO             PET1 index= 312                                    MPIR_CVAR_USE_BCAST_LIST : @Alias:# I_MPI_ADJUST_BCAST_LIST @Default:# -1
20220327 015824.559 INFO             PET1 index= 313                             MPIR_CVAR_USE_BCAST_COMPOSITION : @Alias:# I_MPI_ADJUST_BCAST_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET1 index= 314                                 MPIR_CVAR_USE_BCAST_NETWORK : range: 0-13 @Alias:# I_MPI_ADJUST_BCAST_NETWORK @Default:# -1
20220327 015824.559 INFO             PET1 index= 315                                    MPIR_CVAR_USE_BCAST_NODE : range: 0-9 @Alias:# I_MPI_ADJUST_BCAST_NODE @Default:# -1
20220327 015824.559 INFO             PET1 index= 316                               MPIR_CVAR_BCAST_EPSILON_NRAIL : @Alias:# I_MPI_ADJUST_BCAST_EPSILON_NRAIL @Default:# -1
20220327 015824.559 INFO             PET1 index= 317                          MPIR_CVAR_BCAST_NETWORK_KARY_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_KARY_RADIX @Default:# -1
20220327 015824.559 INFO             PET1 index= 318                       MPIR_CVAR_BCAST_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET1 index= 319                        MPIR_CVAR_BCAST_NETWORK_KARY_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_KARY_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET1 index= 320                     MPIR_CVAR_BCAST_NETWORK_KNOMIAL_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_KNOMIAL_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET1 index= 321                             MPIR_CVAR_BCAST_NODE_KARY_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NODE_KARY_RADIX @Default:# -1
20220327 015824.559 INFO             PET1 index= 322                          MPIR_CVAR_BCAST_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET1 index= 323                           MPIR_CVAR_BCAST_NODE_KARY_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_KARY_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET1 index= 324                        MPIR_CVAR_BCAST_NODE_KNOMIAL_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_KNOMIAL_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET1 index= 325                          MPIR_CVAR_BCAST_NETWORK_TREE_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_TREE_RADIX @Default:# -1
20220327 015824.559 INFO             PET1 index= 326                        MPIR_CVAR_BCAST_NETWORK_TREE_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_TREE_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET1 index= 327                           MPIR_CVAR_BCAST_NETWORK_TREE_TYPE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_TREE_TYPE @Default:# -1
20220327 015824.559 INFO             PET1 index= 328                       MPIR_CVAR_BCAST_NETWORK_TREE_THROTTLE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_TREE_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET1 index= 329                       MPIR_CVAR_BCAST_NODE_SHUMILIN_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHUMILIN_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET1 index= 330                        MPIR_CVAR_BCAST_NODE_SHM_GATHER_TYPE : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHM_GATHER_TYPE @Default:# -1
20220327 015824.559 INFO             PET1 index= 331                       MPIR_CVAR_BCAST_NODE_SHM_GATHER_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHM_GATHER_RADIX @Default:# -1
20220327 015824.559 INFO             PET1 index= 332                       MPIR_CVAR_BCAST_NODE_SHM_RELEASE_TYPE : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHM_RELEASE_TYPE @Default:# -1
20220327 015824.559 INFO             PET1 index= 333                      MPIR_CVAR_BCAST_NODE_SHM_RELEASE_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHM_RELEASE_RADIX @Default:# -1
20220327 015824.559 INFO             PET1 index= 334                    MPIR_CVAR_BCAST_NETWORK_SHUMILIN_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_SHUMILIN_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET1 index= 335                 MPIR_CVAR_BCAST_NODE_NUMA_AWARE_MEMCPY_ARCH : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_MEMCPY_ARCH @Default:# -1
20220327 015824.559 INFO             PET1 index= 336                   MPIR_CVAR_BCAST_NODE_NUMA_AWARE_FRAME_NUM : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_FRAME_NUM
20220327 015824.559 INFO             PET1 index= 337                  MPIR_CVAR_BCAST_NODE_NUMA_AWARE_FRAME_SIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_FRAME_SIZE
20220327 015824.559 INFO             PET1 index= 338  MPIR_CVAR_BCAST_NODE_NUMA_AWARE_RECV_NONTEMPORAL_THRESHOLD : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_NONTEMPORAL_THRESHOLD
20220327 015824.559 INFO             PET1 index= 339      MPIR_CVAR_BCAST_NODE_NUMA_AWARE_FRAME_NONTEMPORAL_SIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_FRAME_NONTEMPORAL_SIZE
20220327 015824.559 INFO             PET1 index= 340 MPIR_CVAR_BCAST_NODE_NUMA_AWARE_TINY_MESSAGE_SIZE_THRESHOLD : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_TINY_MESSAGE_SIZE_THRESHOLD
20220327 015824.559 INFO             PET1 index= 341MPIR_CVAR_BCAST_NODE_NUMA_AWARE_SHM_HEAP_MESSAGE_SIZE_THRESHOLD : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_SHM_HEAP_MESSAGE_SIZE_THRESHOLD
20220327 015824.559 INFO             PET1 index= 342                                        MPIR_CVAR_USE_EXSCAN : Control selection of MPI_Exscan algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_EXSCAN @Default:# -1
20220327 015824.559 INFO             PET1 index= 343                                   MPIR_CVAR_USE_EXSCAN_LIST : @Alias:# I_MPI_ADJUST_EXSCAN_LIST @Default:# -1
20220327 015824.559 INFO             PET1 index= 344                            MPIR_CVAR_USE_EXSCAN_COMPOSITION : @Alias:# I_MPI_ADJUST_EXSCAN_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET1 index= 345                                MPIR_CVAR_USE_EXSCAN_NETWORK : @Alias:# I_MPI_ADJUST_EXSCAN_NETWORK @Default:# -1
20220327 015824.559 INFO             PET1 index= 346                                   MPIR_CVAR_USE_EXSCAN_NODE : @Alias:# I_MPI_ADJUST_EXSCAN_NODE @Default:# -1
20220327 015824.559 INFO             PET1 index= 347                                        MPIR_CVAR_USE_GATHER : Control selection of MPI_Gather algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_GATHER @Default:# -1
20220327 015824.559 INFO             PET1 index= 348                                   MPIR_CVAR_USE_GATHER_LIST : @Alias:# I_MPI_ADJUST_GATHER_LIST @Default:# -1
20220327 015824.559 INFO             PET1 index= 349                            MPIR_CVAR_USE_GATHER_COMPOSITION : @Alias:# I_MPI_ADJUST_GATHER_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET1 index= 350                                MPIR_CVAR_USE_GATHER_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_GATHER_NETWORK @Default:# -1
20220327 015824.559 INFO             PET1 index= 351                                   MPIR_CVAR_USE_GATHER_NODE : range: 0-3 @Alias:# I_MPI_ADJUST_GATHER_NODE @Default:# -1
20220327 015824.559 INFO             PET1 index= 352            MPIR_CVAR_GATHER_NODE_BINOMIAL_SEGMENTED_SEGSIZE : @Alias:# I_MPI_ADJUST_GATHER_NODE_BINOMIAL_SEGMENTED_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET1 index= 353         MPIR_CVAR_GATHER_NETWORK_BINOMIAL_SEGMENTED_SEGSIZE : @Alias:# I_MPI_ADJUST_GATHER_NETWORK_BINOMIAL_SEGMENTED_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET1 index= 354                                       MPIR_CVAR_USE_GATHERV : Control selection of MPI_Gatherv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-4 @Alias:# I_MPI_ADJUST_GATHERV @Default:# -1
20220327 015824.559 INFO             PET1 index= 355                                  MPIR_CVAR_USE_GATHERV_LIST : @Alias:# I_MPI_ADJUST_GATHERV_LIST @Default:# -1
20220327 015824.559 INFO             PET1 index= 356                           MPIR_CVAR_USE_GATHERV_COMPOSITION : @Alias:# I_MPI_ADJUST_GATHERV_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET1 index= 357                               MPIR_CVAR_USE_GATHERV_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_GATHERV_NETWORK @Default:# -1
20220327 015824.559 INFO             PET1 index= 358                                  MPIR_CVAR_USE_GATHERV_NODE : range: 0-2 @Alias:# I_MPI_ADJUST_GATHERV_NODE @Default:# -1
20220327 015824.559 INFO             PET1 index= 359                     MPIR_CVAR_GATHERV_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_GATHERV_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET1 index= 360                        MPIR_CVAR_GATHERV_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_GATHERV_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET1 index= 361                                MPIR_CVAR_USE_REDUCE_SCATTER : Control selection of MPI_Reduce_scatter algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-6 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER @Default:# -1
20220327 015824.559 INFO             PET1 index= 362                           MPIR_CVAR_USE_REDUCE_SCATTER_LIST : @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_LIST @Default:# -1
20220327 015824.559 INFO             PET1 index= 363                    MPIR_CVAR_USE_REDUCE_SCATTER_COMPOSITION : @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET1 index= 364                        MPIR_CVAR_USE_REDUCE_SCATTER_NETWORK : range: 0-5 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_NETWORK @Default:# -1
20220327 015824.559 INFO             PET1 index= 365                           MPIR_CVAR_USE_REDUCE_SCATTER_NODE : range: 0-4 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_NODE @Default:# -1
20220327 015824.559 INFO             PET1 index= 366                          MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK : Control selection of MPI_Reduce_scatter_block algorithm presets. Arguments$ <algid> - Algorithm identifier$ range: 0-4 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK @Default:# -1
20220327 015824.559 INFO             PET1 index= 367                     MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK_LIST : @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK_LIST @Default:# -1
20220327 015824.559 INFO             PET1 index= 368              MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK_COMPOSITION : @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET1 index= 369                  MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK_NETWORK @Default:# -1
20220327 015824.559 INFO             PET1 index= 370                     MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK_NODE : range: 0-3 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK_NODE @Default:# -1
20220327 015824.559 INFO             PET1 index= 371                                        MPIR_CVAR_USE_REDUCE : Control selection of MPI_Reduce algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-13 @Alias:# I_MPI_ADJUST_REDUCE @Default:# -1
20220327 015824.559 INFO             PET1 index= 372                                   MPIR_CVAR_USE_REDUCE_LIST : @Alias:# I_MPI_ADJUST_REDUCE_LIST @Default:# -1
20220327 015824.559 INFO             PET1 index= 373                            MPIR_CVAR_USE_REDUCE_COMPOSITION : @Alias:# I_MPI_ADJUST_REDUCE_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET1 index= 374                                MPIR_CVAR_USE_REDUCE_NETWORK : range: 0-10 @Alias:# I_MPI_ADJUST_REDUCE_NETWORK @Default:# -1
20220327 015824.559 INFO             PET1 index= 375                                   MPIR_CVAR_USE_REDUCE_NODE : range: 0-7 @Alias:# I_MPI_ADJUST_REDUCE_NODE @Default:# -1
20220327 015824.559 INFO             PET1 index= 376                         MPIR_CVAR_REDUCE_NETWORK_KARY_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KARY_RADIX @Default:# -1
20220327 015824.559 INFO             PET1 index= 377                      MPIR_CVAR_REDUCE_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET1 index= 378                      MPIR_CVAR_REDUCE_NETWORK_KARY_NBUFFERS : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KARY_NBUFFERS @Default:# -1
20220327 015824.559 INFO             PET1 index= 379                   MPIR_CVAR_REDUCE_NETWORK_KNOMIAL_NBUFFERS : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KNOMIAL_NBUFFERS @Default:# -1
20220327 015824.559 INFO             PET1 index= 380                       MPIR_CVAR_REDUCE_NETWORK_KARY_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KARY_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET1 index= 381                    MPIR_CVAR_REDUCE_NETWORK_KNOMIAL_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KNOMIAL_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET1 index= 382                       MPIR_CVAR_REDUCE_NETWORK_RING_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_RING_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET1 index= 383                            MPIR_CVAR_REDUCE_NODE_KARY_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KARY_RADIX @Default:# -1
20220327 015824.559 INFO             PET1 index= 384                         MPIR_CVAR_REDUCE_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET1 index= 385                         MPIR_CVAR_REDUCE_NODE_KARY_NBUFFERS : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KARY_NBUFFERS @Default:# -1
20220327 015824.559 INFO             PET1 index= 386                      MPIR_CVAR_REDUCE_NODE_KNOMIAL_NBUFFERS : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KNOMIAL_NBUFFERS @Default:# -1
20220327 015824.559 INFO             PET1 index= 387                          MPIR_CVAR_REDUCE_NODE_KARY_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KARY_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET1 index= 388                       MPIR_CVAR_REDUCE_NODE_KNOMIAL_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KNOMIAL_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET1 index= 389                          MPIR_CVAR_REDUCE_NODE_RING_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_RING_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET1 index= 390                       MPIR_CVAR_REDUCE_NODE_SHM_GATHER_TYPE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_SHM_GATHER_TYPE @Default:# -1
20220327 015824.559 INFO             PET1 index= 391                      MPIR_CVAR_REDUCE_NODE_SHM_GATHER_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NODE_SHM_GATHER_RADIX @Default:# -1
20220327 015824.560 INFO             PET1 index= 392                      MPIR_CVAR_REDUCE_NODE_SHM_RELEASE_TYPE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_SHM_RELEASE_TYPE @Default:# -1
20220327 015824.560 INFO             PET1 index= 393                     MPIR_CVAR_REDUCE_NODE_SHM_RELEASE_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NODE_SHM_RELEASE_RADIX @Default:# -1
20220327 015824.560 INFO             PET1 index= 394                                          MPIR_CVAR_USE_SCAN : Control selection of MPI_Scan algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_SCAN @Default:# -1
20220327 015824.560 INFO             PET1 index= 395                                     MPIR_CVAR_USE_SCAN_LIST : @Alias:# I_MPI_ADJUST_SCAN_LIST @Default:# -1
20220327 015824.560 INFO             PET1 index= 396                              MPIR_CVAR_USE_SCAN_COMPOSITION : @Alias:# I_MPI_ADJUST_SCAN_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET1 index= 397                                  MPIR_CVAR_USE_SCAN_NETWORK : @Alias:# I_MPI_ADJUST_SCAN_NETWORK @Default:# -1
20220327 015824.560 INFO             PET1 index= 398                                     MPIR_CVAR_USE_SCAN_NODE : @Alias:# I_MPI_ADJUST_SCAN_NODE @Default:# -1
20220327 015824.560 INFO             PET1 index= 399                                       MPIR_CVAR_USE_SCATTER : Control selection of MPI_Scatter algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_SCATTER @Default:# -1
20220327 015824.560 INFO             PET1 index= 400                                  MPIR_CVAR_USE_SCATTER_LIST : @Alias:# I_MPI_ADJUST_SCATTER_LIST @Default:# -1
20220327 015824.560 INFO             PET1 index= 401                           MPIR_CVAR_USE_SCATTER_COMPOSITION : @Alias:# I_MPI_ADJUST_SCATTER_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET1 index= 402                               MPIR_CVAR_USE_SCATTER_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_SCATTER_NETWORK @Default:# -1
20220327 015824.560 INFO             PET1 index= 403                                  MPIR_CVAR_USE_SCATTER_NODE : range: 0-3 @Alias:# I_MPI_ADJUST_SCATTER_NODE @Default:# -1
20220327 015824.560 INFO             PET1 index= 404                                      MPIR_CVAR_USE_SCATTERV : Control selection of MPI_Scatterv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-4 @Alias:# I_MPI_ADJUST_SCATTERV @Default:# -1
20220327 015824.560 INFO             PET1 index= 405                                 MPIR_CVAR_USE_SCATTERV_LIST : @Alias:# I_MPI_ADJUST_SCATTERV_LIST @Default:# -1
20220327 015824.560 INFO             PET1 index= 406                          MPIR_CVAR_USE_SCATTERV_COMPOSITION : @Alias:# I_MPI_ADJUST_SCATTERV_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET1 index= 407                              MPIR_CVAR_USE_SCATTERV_NETWORK : range: 0-3 @Alias:# I_MPI_ADJUST_SCATTERV_NETWORK @Default:# -1
20220327 015824.560 INFO             PET1 index= 408                                 MPIR_CVAR_USE_SCATTERV_NODE : range: 0-2 @Alias:# I_MPI_ADJUST_SCATTERV_NODE @Default:# -1
20220327 015824.560 INFO             PET1 index= 409                                    MPIR_CVAR_USE_IALLREDUCE : Control selection of MPI_Iallreduce algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-9 @Alias:# I_MPI_ADJUST_IALLREDUCE @Default:# -1
20220327 015824.560 INFO             PET1 index= 410                               MPIR_CVAR_USE_IALLREDUCE_LIST : @Alias:# I_MPI_ADJUST_IALLREDUCE_LIST @Default:# -1
20220327 015824.560 INFO             PET1 index= 411                        MPIR_CVAR_USE_IALLREDUCE_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLREDUCE_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET1 index= 412                            MPIR_CVAR_USE_IALLREDUCE_NETWORK : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK @Default:# -1
20220327 015824.560 INFO             PET1 index= 413                               MPIR_CVAR_USE_IALLREDUCE_NODE : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE @Default:# -1
20220327 015824.560 INFO             PET1 index= 414                   MPIR_CVAR_IALLREDUCE_KNOMIAL_REDUCE_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_KNOMIAL_REDUCE_RADIX @Default:# -1
20220327 015824.560 INFO             PET1 index= 415                    MPIR_CVAR_IALLREDUCE_KNOMIAL_BCAST_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_KNOMIAL_BCAST_RADIX @Default:# -1
20220327 015824.560 INFO             PET1 index= 416           MPIR_CVAR_IALLREDUCE_NETWORK_KNOMIAL_REDUCE_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK_KNOMIAL_REDUCE_RADIX @Default:# -1
20220327 015824.560 INFO             PET1 index= 417              MPIR_CVAR_IALLREDUCE_NODE_KNOMIAL_REDUCE_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE_KNOMIAL_REDUCE_RADIX @Default:# -1
20220327 015824.560 INFO             PET1 index= 418            MPIR_CVAR_IALLREDUCE_NETWORK_KNOMIAL_BCAST_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK_KNOMIAL_BCAST_RADIX @Default:# -1
20220327 015824.560 INFO             PET1 index= 419               MPIR_CVAR_IALLREDUCE_NODE_KNOMIAL_BCAST_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE_KNOMIAL_BCAST_RADIX @Default:# -1
20220327 015824.560 INFO             PET1 index= 420                 MPIR_CVAR_IALLREDUCE_NODE_NREDUCE_DO_GATHER : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE_NREDUCE_DO_GATHER @Default:# -1
20220327 015824.560 INFO             PET1 index= 421              MPIR_CVAR_IALLREDUCE_NETWORK_NREDUCE_DO_GATHER : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK_NREDUCE_DO_GATHER @Default:# -1
20220327 015824.560 INFO             PET1 index= 422              MPIR_CVAR_IALLREDUCE_NODE_NREDUCE_NB_ALLGATHER : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE_NREDUCE_NB_ALLGATHER @Default:# -1
20220327 015824.560 INFO             PET1 index= 423           MPIR_CVAR_IALLREDUCE_NETWORK_NREDUCE_NB_ALLGATHER : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK_NREDUCE_NB_ALLGATHER @Default:# -1
20220327 015824.560 INFO             PET1 index= 424                                        MPIR_CVAR_USE_IBCAST : Control selection of MPI_Ibcast algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_IBCAST @Default:# -1
20220327 015824.560 INFO             PET1 index= 425                                   MPIR_CVAR_USE_IBCAST_LIST : @Alias:# I_MPI_ADJUST_IBCAST_LIST @Default:# -1
20220327 015824.560 INFO             PET1 index= 426                            MPIR_CVAR_USE_IBCAST_COMPOSITION : @Alias:# I_MPI_ADJUST_IBCAST_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET1 index= 427                                MPIR_CVAR_USE_IBCAST_NETWORK : @Alias:# I_MPI_ADJUST_IBCAST_NETWORK @Default:# -1
20220327 015824.560 INFO             PET1 index= 428                                   MPIR_CVAR_USE_IBCAST_NODE : @Alias:# I_MPI_ADJUST_IBCAST_NODE @Default:# -1
20220327 015824.560 INFO             PET1 index= 429                              MPIR_CVAR_IBCAST_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IBCAST_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET1 index= 430                      MPIR_CVAR_IBCAST_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IBCAST_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET1 index= 431                         MPIR_CVAR_IBCAST_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IBCAST_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET1 index= 432                                       MPIR_CVAR_USE_IREDUCE : Control selection of MPI_Ireduce algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-4 @Alias:# I_MPI_ADJUST_IREDUCE @Default:# -1
20220327 015824.560 INFO             PET1 index= 433                                  MPIR_CVAR_USE_IREDUCE_LIST : @Alias:# I_MPI_ADJUST_IREDUCE_LIST @Default:# -1
20220327 015824.560 INFO             PET1 index= 434                           MPIR_CVAR_USE_IREDUCE_COMPOSITION : @Alias:# I_MPI_ADJUST_IREDUCE_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET1 index= 435                               MPIR_CVAR_USE_IREDUCE_NETWORK : @Alias:# I_MPI_ADJUST_IREDUCE_NETWORK @Default:# -1
20220327 015824.560 INFO             PET1 index= 436                                  MPIR_CVAR_USE_IREDUCE_NODE : @Alias:# I_MPI_ADJUST_IREDUCE_NODE @Default:# -1
20220327 015824.560 INFO             PET1 index= 437                             MPIR_CVAR_IREDUCE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IREDUCE_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET1 index= 438                     MPIR_CVAR_IREDUCE_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IREDUCE_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET1 index= 439                        MPIR_CVAR_IREDUCE_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IREDUCE_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET1 index= 440                                       MPIR_CVAR_USE_IGATHER : Control selection of MPI_Igather algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-2 @Alias:# I_MPI_ADJUST_IGATHER @Default:# -1
20220327 015824.560 INFO             PET1 index= 441                                  MPIR_CVAR_USE_IGATHER_LIST : @Alias:# I_MPI_ADJUST_IGATHER_LIST @Default:# -1
20220327 015824.560 INFO             PET1 index= 442                           MPIR_CVAR_USE_IGATHER_COMPOSITION : @Alias:# I_MPI_ADJUST_IGATHER_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET1 index= 443                               MPIR_CVAR_USE_IGATHER_NETWORK : @Alias:# I_MPI_ADJUST_IGATHER_NETWORK @Default:# -1
20220327 015824.560 INFO             PET1 index= 444                                  MPIR_CVAR_USE_IGATHER_NODE : @Alias:# I_MPI_ADJUST_IGATHER_NODE @Default:# -1
20220327 015824.560 INFO             PET1 index= 445                             MPIR_CVAR_IGATHER_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IGATHER_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET1 index= 446                     MPIR_CVAR_IGATHER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IGATHER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET1 index= 447                        MPIR_CVAR_IGATHER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IGATHER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET1 index= 448                                    MPIR_CVAR_USE_IALLGATHER : Control selection of MPI_Iallgather algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_IALLGATHER @Default:# -1
20220327 015824.560 INFO             PET1 index= 449                               MPIR_CVAR_USE_IALLGATHER_LIST : @Alias:# I_MPI_ADJUST_IALLGATHER_LIST @Default:# -1
20220327 015824.560 INFO             PET1 index= 450                        MPIR_CVAR_USE_IALLGATHER_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLGATHER_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET1 index= 451                            MPIR_CVAR_USE_IALLGATHER_NETWORK : @Alias:# I_MPI_ADJUST_IALLGATHER_NETWORK @Default:# -1
20220327 015824.560 INFO             PET1 index= 452                               MPIR_CVAR_USE_IALLGATHER_NODE : @Alias:# I_MPI_ADJUST_IALLGATHER_NODE @Default:# -1
20220327 015824.560 INFO             PET1 index= 453                  MPIR_CVAR_IALLGATHER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IALLGATHER_NETWORK_KNOMIAL_RADIX
20220327 015824.560 INFO             PET1 index= 454                     MPIR_CVAR_IALLGATHER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IALLGATHER_NODE_KNOMIAL_RADIX
20220327 015824.560 INFO             PET1 index= 455                                     MPIR_CVAR_USE_IALLTOALL : Control selection of MPI_Ialltoall algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-7 @Alias:# I_MPI_ADJUST_IALLTOALL @Default:# -1
20220327 015824.560 INFO             PET1 index= 456                                MPIR_CVAR_USE_IALLTOALL_LIST : @Alias:# I_MPI_ADJUST_IALLTOALL_LIST @Default:# -1
20220327 015824.560 INFO             PET1 index= 457                         MPIR_CVAR_USE_IALLTOALL_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLTOALL_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET1 index= 458                             MPIR_CVAR_USE_IALLTOALL_NETWORK : @Alias:# I_MPI_ADJUST_IALLTOALL_NETWORK @Default:# -1
20220327 015824.560 INFO             PET1 index= 459                                MPIR_CVAR_USE_IALLTOALL_NODE : @Alias:# I_MPI_ADJUST_IALLTOALL_NODE @Default:# -1
20220327 015824.560 INFO             PET1 index= 460              MPIR_CVAR_IALLTOALL_PERMUTED_SENDRECV_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALL_PERMUTED_SENDRECV_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET1 index= 461      MPIR_CVAR_IALLTOALL_NETWORK_PERMUTED_SENDRECV_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALL_NETWORK_PERMUTED_SENDRECV_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET1 index= 462         MPIR_CVAR_IALLTOALL_NODE_PERMUTED_SENDRECV_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALL_NODE_PERMUTED_SENDRECV_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET1 index= 463                                    MPIR_CVAR_USE_IALLTOALLV : Control selection of MPI_Ialltoallv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-6 @Alias:# I_MPI_ADJUST_IALLTOALLV @Default:# -1
20220327 015824.560 INFO             PET1 index= 464                               MPIR_CVAR_USE_IALLTOALLV_LIST : @Alias:# I_MPI_ADJUST_IALLTOALLV_LIST @Default:# -1
20220327 015824.560 INFO             PET1 index= 465                        MPIR_CVAR_USE_IALLTOALLV_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLTOALLV_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET1 index= 466                            MPIR_CVAR_USE_IALLTOALLV_NETWORK : @Alias:# I_MPI_ADJUST_IALLTOALLV_NETWORK @Default:# -1
20220327 015824.560 INFO             PET1 index= 467                               MPIR_CVAR_USE_IALLTOALLV_NODE : @Alias:# I_MPI_ADJUST_IALLTOALLV_NODE @Default:# -1
20220327 015824.560 INFO             PET1 index= 468                       MPIR_CVAR_IALLTOALLV_BLOCKED_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALLV_BLOCKED_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET1 index= 469               MPIR_CVAR_IALLTOALLV_NETWORK_BLOCKED_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALLV_NETWORK_BLOCKED_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET1 index= 470                                    MPIR_CVAR_USE_IALLTOALLW : Control selection of MPI_Ialltoallw algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-1 @Alias:# I_MPI_ADJUST_IALLTOALLW @Default:# -1
20220327 015824.560 INFO             PET1 index= 471                               MPIR_CVAR_USE_IALLTOALLW_LIST : @Alias:# I_MPI_ADJUST_IALLTOALLW_LIST @Default:# -1
20220327 015824.560 INFO             PET1 index= 472                        MPIR_CVAR_USE_IALLTOALLW_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLTOALLW_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET1 index= 473                            MPIR_CVAR_USE_IALLTOALLW_NETWORK : @Alias:# I_MPI_ADJUST_IALLTOALLW_NETWORK @Default:# -1
20220327 015824.560 INFO             PET1 index= 474                               MPIR_CVAR_USE_IALLTOALLW_NODE : @Alias:# I_MPI_ADJUST_IALLTOALLW_NODE @Default:# -1
20220327 015824.560 INFO             PET1 index= 475                       MPIR_CVAR_IALLTOALLW_BLOCKED_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALLW_BLOCKED_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET1 index= 476               MPIR_CVAR_IALLTOALLW_NETWORK_BLOCKED_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALLW_NETWORK_BLOCKED_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET1 index= 477                                      MPIR_CVAR_USE_IGATHERV : Control selection of MPI_Igatherv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_IGATHERV @Default:# -1
20220327 015824.560 INFO             PET1 index= 478                                 MPIR_CVAR_USE_IGATHERV_LIST : @Alias:# I_MPI_ADJUST_IGATHERV_LIST @Default:# -1
20220327 015824.560 INFO             PET1 index= 479                          MPIR_CVAR_USE_IGATHERV_COMPOSITION : @Alias:# I_MPI_ADJUST_IGATHERV_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET1 index= 480                              MPIR_CVAR_USE_IGATHERV_NETWORK : @Alias:# I_MPI_ADJUST_IGATHERV_NETWORK @Default:# -1
20220327 015824.560 INFO             PET1 index= 481                                 MPIR_CVAR_USE_IGATHERV_NODE : @Alias:# I_MPI_ADJUST_IGATHERV_NODE @Default:# -1
20220327 015824.560 INFO             PET1 index= 482                                     MPIR_CVAR_USE_ISCATTERV : Control selection of MPI_Iscatterv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-1 @Alias:# I_MPI_ADJUST_ISCATTERV @Default:# -1
20220327 015824.560 INFO             PET1 index= 483                                MPIR_CVAR_USE_ISCATTERV_LIST : @Alias:# I_MPI_ADJUST_ISCATTERV_LIST @Default:# -1
20220327 015824.560 INFO             PET1 index= 484                         MPIR_CVAR_USE_ISCATTERV_COMPOSITION : @Alias:# I_MPI_ADJUST_ISCATTERV_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET1 index= 485                             MPIR_CVAR_USE_ISCATTERV_NETWORK : @Alias:# I_MPI_ADJUST_ISCATTERV_NETWORK @Default:# -1
20220327 015824.560 INFO             PET1 index= 486                                MPIR_CVAR_USE_ISCATTERV_NODE : @Alias:# I_MPI_ADJUST_ISCATTERV_NODE @Default:# -1
20220327 015824.560 INFO             PET1 index= 487                                      MPIR_CVAR_USE_IBARRIER : Control selection of MPI_Ibarrier algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-2 @Alias:# I_MPI_ADJUST_IBARRIER @Default:# -1
20220327 015824.560 INFO             PET1 index= 488                                 MPIR_CVAR_USE_IBARRIER_LIST : @Alias:# I_MPI_ADJUST_IBARRIER_LIST @Default:# -1
20220327 015824.560 INFO             PET1 index= 489                          MPIR_CVAR_USE_IBARRIER_COMPOSITION : @Alias:# I_MPI_ADJUST_IBARRIER_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET1 index= 490                              MPIR_CVAR_USE_IBARRIER_NETWORK : @Alias:# I_MPI_ADJUST_IBARRIER_NETWORK @Default:# -1
20220327 015824.560 INFO             PET1 index= 491                                 MPIR_CVAR_USE_IBARRIER_NODE : @Alias:# I_MPI_ADJUST_IBARRIER_NODE @Default:# -1
20220327 015824.561 INFO             PET1 index= 492                                      MPIR_CVAR_USE_ISCATTER : Control selection of MPI_Iscatter algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-2 @Alias:# I_MPI_ADJUST_ISCATTER @Default:# -1
20220327 015824.561 INFO             PET1 index= 493                                 MPIR_CVAR_USE_ISCATTER_LIST : @Alias:# I_MPI_ADJUST_ISCATTER_LIST @Default:# -1
20220327 015824.561 INFO             PET1 index= 494                          MPIR_CVAR_USE_ISCATTER_COMPOSITION : @Alias:# I_MPI_ADJUST_ISCATTER_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET1 index= 495                              MPIR_CVAR_USE_ISCATTER_NETWORK : @Alias:# I_MPI_ADJUST_ISCATTER_NETWORK @Default:# -1
20220327 015824.561 INFO             PET1 index= 496                                 MPIR_CVAR_USE_ISCATTER_NODE : @Alias:# I_MPI_ADJUST_ISCATTER_NODE @Default:# -1
20220327 015824.561 INFO             PET1 index= 497                            MPIR_CVAR_ISCATTER_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ISCATTER_KNOMIAL_RADIX @Default:# -1
20220327 015824.561 INFO             PET1 index= 498                    MPIR_CVAR_ISCATTER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ISCATTER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.561 INFO             PET1 index= 499                       MPIR_CVAR_ISCATTER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ISCATTER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.561 INFO             PET1 index= 500                                   MPIR_CVAR_USE_IALLGATHERV : Control selection of MPI_Iallgatherv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_IALLGATHERV @Default:# -1
20220327 015824.561 INFO             PET1 index= 501                              MPIR_CVAR_USE_IALLGATHERV_LIST : @Alias:# I_MPI_ADJUST_IALLGATHERV_LIST @Default:# -1
20220327 015824.561 INFO             PET1 index= 502                       MPIR_CVAR_USE_IALLGATHERV_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLGATHERV_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET1 index= 503                           MPIR_CVAR_USE_IALLGATHERV_NETWORK : @Alias:# I_MPI_ADJUST_IALLGATHERV_NETWORK @Default:# -1
20220327 015824.561 INFO             PET1 index= 504                              MPIR_CVAR_USE_IALLGATHERV_NODE : @Alias:# I_MPI_ADJUST_IALLGATHERV_NODE @Default:# -1
20220327 015824.561 INFO             PET1 index= 505                                       MPIR_CVAR_USE_IEXSCAN : Control selection of MPI_Iexscan algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-1 @Alias:# I_MPI_ADJUST_IEXSCAN @Default:# -1
20220327 015824.561 INFO             PET1 index= 506                                  MPIR_CVAR_USE_IEXSCAN_LIST : @Alias:# I_MPI_ADJUST_IEXSCAN_LIST @Default:# -1
20220327 015824.561 INFO             PET1 index= 507                           MPIR_CVAR_USE_IEXSCAN_COMPOSITION : @Alias:# I_MPI_ADJUST_IEXSCAN_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET1 index= 508                               MPIR_CVAR_USE_IEXSCAN_NETWORK : @Alias:# I_MPI_ADJUST_IEXSCAN_NETWORK @Default:# -1
20220327 015824.561 INFO             PET1 index= 509                                  MPIR_CVAR_USE_IEXSCAN_NODE : @Alias:# I_MPI_ADJUST_IEXSCAN_NODE @Default:# -1
20220327 015824.561 INFO             PET1 index= 510                                         MPIR_CVAR_USE_ISCAN : Control selection of MPI_Iscan algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-1 @Alias:# I_MPI_ADJUST_ISCAN @Default:# -1
20220327 015824.561 INFO             PET1 index= 511                                    MPIR_CVAR_USE_ISCAN_LIST : @Alias:# I_MPI_ADJUST_ISCAN_LIST @Default:# -1
20220327 015824.561 INFO             PET1 index= 512                             MPIR_CVAR_USE_ISCAN_COMPOSITION : @Alias:# I_MPI_ADJUST_ISCAN_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET1 index= 513                                 MPIR_CVAR_USE_ISCAN_NETWORK : @Alias:# I_MPI_ADJUST_ISCAN_NETWORK @Default:# -1
20220327 015824.561 INFO             PET1 index= 514                                    MPIR_CVAR_USE_ISCAN_NODE : @Alias:# I_MPI_ADJUST_ISCAN_NODE @Default:# -1
20220327 015824.561 INFO             PET1 index= 515                               MPIR_CVAR_USE_IREDUCE_SCATTER : Control selection of MPI_Ireduce_scatter algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER @Default:# -1
20220327 015824.561 INFO             PET1 index= 516                          MPIR_CVAR_USE_IREDUCE_SCATTER_LIST : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_LIST @Default:# -1
20220327 015824.561 INFO             PET1 index= 517                   MPIR_CVAR_USE_IREDUCE_SCATTER_COMPOSITION : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET1 index= 518                       MPIR_CVAR_USE_IREDUCE_SCATTER_NETWORK : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_NETWORK @Default:# -1
20220327 015824.561 INFO             PET1 index= 519                          MPIR_CVAR_USE_IREDUCE_SCATTER_NODE : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_NODE @Default:# -1
20220327 015824.561 INFO             PET1 index= 520                         MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK : Control selection of MPI_Ireduce_scatter_block algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK @Default:# -1
20220327 015824.561 INFO             PET1 index= 521                    MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK_LIST : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK_LIST @Default:# -1
20220327 015824.561 INFO             PET1 index= 522             MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK_COMPOSITION : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET1 index= 523                 MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK_NETWORK : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK_NETWORK @Default:# -1
20220327 015824.561 INFO             PET1 index= 524                    MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK_NODE : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK_NODE @Default:# -1
20220327 015824.561 INFO             PET1 index= 525                               MPIR_CVAR_IMPI_SHMGR_DATASIZE : Define the size of shared memory area available for each rank for data placement. Messages greater than this value will not be processed by SHM-based collective operation, but will be processed by point-to-point based collective operation. The value must be a multiple of 4096. @Alias:# I_MPI_COLL_SHM_THRESHOLD @Default:# 16384
20220327 015824.561 INFO             PET1 index= 526                              MPIR_CVAR_IMPI_SHMGR_SPINCOUNT : @Alias:# I_MPI_COLL_SHM_PROGRESS_SPIN_COUNT
20220327 015824.561 INFO             PET1 index= 527                              MPIR_CVAR_INTEL_COLL_INTRANODE : @Alias:# I_MPI_COLL_INTRANODE
20220327 015824.561 INFO             PET1 index= 528                         MPIR_CVAR_ENABLE_EXPERIMENTAL_ALGOS : @Alias:# I_MPI_COLL_EXPERIMENTAL
20220327 015824.561 INFO             PET1 index= 529                                    MPIR_CVAR_IMPI_WAIT_MODE : @Alias:# I_MPI_WAIT_MODE
20220327 015824.561 INFO             PET1 index= 530                                 MPIR_CVAR_IMPI_THREAD_SLEEP : @Alias:# I_MPI_THREAD_SLEEP
20220327 015824.561 INFO             PET1 index= 531                          MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the short message algorithm will be used if the send buffer size is < this value (in bytes). (See also: MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE)
20220327 015824.561 INFO             PET1 index= 532                           MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the long message algorithm will be used if the send buffer size is >= this value (in bytes) (See also: MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE)
20220327 015824.561 INFO             PET1 index= 533                      MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE : The smallest message size that will be used for the pipelined, large-message, ring algorithm in the MPI_Allgatherv implementation.
20220327 015824.561 INFO             PET1 index= 534                          MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20220327 015824.561 INFO             PET1 index= 535                            MPIR_CVAR_ENABLE_SMP_COLLECTIVES : Enable SMP aware collective communication.
20220327 015824.561 INFO             PET1 index= 536                              MPIR_CVAR_ENABLE_SMP_ALLREDUCE : Enable SMP aware allreduce.
20220327 015824.561 INFO             PET1 index= 537                        MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE : Maximum message size for which SMP-aware allreduce is used.  A value of '0' uses SMP-aware allreduce for all message sizes.
20220327 015824.561 INFO             PET1 index= 538                           MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE : the short message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value (See also: MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE)
20220327 015824.561 INFO             PET1 index= 539                          MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE : the medium message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value and larger than MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE (See also: MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE)
20220327 015824.561 INFO             PET1 index= 540                                MPIR_CVAR_ENABLE_SMP_BARRIER : Enable SMP aware barrier.
20220327 015824.561 INFO             PET1 index= 541                                   MPIR_CVAR_BCAST_MIN_PROCS : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_SHORT_MSG_SIZE, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20220327 015824.561 INFO             PET1 index= 542                              MPIR_CVAR_BCAST_SHORT_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20220327 015824.561 INFO             PET1 index= 543                               MPIR_CVAR_BCAST_LONG_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_SHORT_MSG_SIZE)
20220327 015824.561 INFO             PET1 index= 544                                  MPIR_CVAR_ENABLE_SMP_BCAST : Enable SMP aware broadcast (See also: MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE)
20220327 015824.561 INFO             PET1 index= 545                            MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE : Maximum message size for which SMP-aware broadcast is used.  A value of '0' uses SMP-aware broadcast for all message sizes. (See also: MPIR_CVAR_ENABLE_SMP_BCAST)
20220327 015824.561 INFO             PET1 index= 546                            MPIR_CVAR_GATHER_VSMALL_MSG_SIZE : use a temporary buffer for intracommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE)
20220327 015824.561 INFO             PET1 index= 547                       MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_VSMALL_MSG_SIZE)
20220327 015824.561 INFO             PET1 index= 548                     MPIR_CVAR_GATHERV_INTER_SSEND_MIN_PROCS : Use Ssend (synchronous send) for intercommunicator MPI_Gatherv if the "group B" size is >= this value.  Specifying "-1" always avoids using Ssend.  For backwards compatibility, specifying "0" uses the default value.
20220327 015824.561 INFO             PET1 index= 549                             MPIR_CVAR_REDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20220327 015824.561 INFO             PET1 index= 550                                 MPIR_CVAR_ENABLE_SMP_REDUCE : Enable SMP aware reduce.
20220327 015824.561 INFO             PET1 index= 551                           MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE : Maximum message size for which SMP-aware reduce is used.  A value of '0' uses SMP-aware reduce for all message sizes.
20220327 015824.561 INFO             PET1 index= 552                      MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Scatter if the send buffer size is < this value (in bytes)
20220327 015824.561 INFO             PET1 index= 553                                  MPIR_CVAR_USE_CPU_PLATFORM : @Alias:# I_MPI_PLATFORM
20220327 015824.561 INFO             PET1 index= 554                          MPIR_CVAR_FAILURE_ON_COLL_FALLBACK : @Alias:# I_MPI_ADJUST_FAILURE_ON_COLL_FALLBACK
20220327 015824.561 INFO             PET1 index= 555                         MPIR_CVAR_FAILURE_ON_MATCH_FALLBACK : @Alias:# I_MPI_ADJUST_FAILURE_ON_MATCH_FALLBACK
20220327 015824.561 INFO             PET1 index= 556                           MPIR_CVAR_ADJUST_SENDRECV_REPLACE : @Alias:# I_MPI_ADJUST_SENDRECV_REPLACE
20220327 015824.561 INFO             PET1 index= 557                MPIR_CVAR_ADJUST_SENDRECV_REPLACE_FRAME_SIZE : @Alias:# I_MPI_ADJUST_SENDRECV_REPLACE_FRAME_SIZE
20220327 015824.561 INFO             PET1 index= 558                         MPIR_CVAR_NUMERICAL_REPRODUCIBILITY : @Alias:# I_MPI_CBWR
20220327 015824.561 INFO             PET1 index= 559                                    MPIR_CVAR_USE_TUNING_CH4 : @Alias:# I_MPI_TUNING
20220327 015824.561 INFO             PET1 index= 560                                    MPIR_CVAR_USE_TUNING_NET : @Alias:# I_MPI_TUNING_NETWORK
20220327 015824.561 INFO             PET1 index= 561                                    MPIR_CVAR_USE_TUNING_SHM : @Alias:# I_MPI_TUNING_NODE
20220327 015824.561 INFO             PET1 index= 562                                   MPIR_CVAR_DUMP_TUNING_CH4 : @Alias:# I_MPI_TUNING_COMPOSITION_DUMP
20220327 015824.561 INFO             PET1 index= 563                                   MPIR_CVAR_DUMP_TUNING_NET : @Alias:# I_MPI_TUNING_NETWORK_DUMP
20220327 015824.561 INFO             PET1 index= 564                                   MPIR_CVAR_DUMP_TUNING_SHM : @Alias:# I_MPI_TUNING_NODE_DUMP
20220327 015824.561 INFO             PET1 index= 565                                        MPIR_CVAR_BIN_TUNING : @Alias:# I_MPI_TUNING_BIN
20220327 015824.561 INFO             PET1 index= 566                                   MPIR_CVAR_BIN_DUMP_TUNING : @Alias:# I_MPI_TUNING_BIN_DUMP
20220327 015824.561 INFO             PET1 index= 567                                   MPIR_CVAR_TUNING_BIN_PATH : @Alias:# I_MPI_TUNING_BIN_PATH
20220327 015824.561 INFO             PET1 index= 568                            MPIR_CVAR_TUNING_COMPOSITION_PPN : @Alias:# I_MPI_TUNING_COMPOSITION_PPN
20220327 015824.561 INFO             PET1 index= 569                                MPIR_CVAR_TUNING_NETWORK_PPN : @Alias:# I_MPI_TUNING_NETWORK_PPN
20220327 015824.561 INFO             PET1 index= 570                                   MPIR_CVAR_TUNING_NODE_PPN : @Alias:# I_MPI_TUNING_NODE_PPN
20220327 015824.561 INFO             PET1 index= 571                 MPIR_CVAR_TUNING_COMPOSITION_COMM_HIERARCHY : @Alias:# I_MPI_TUNING_COMPOSITION_COMM_HIERARCHY
20220327 015824.561 INFO             PET1 index= 572                     MPIR_CVAR_TUNING_NETWORK_COMM_HIERARCHY : @Alias:# I_MPI_TUNING_NETWORK_COMM_HIERARCHY
20220327 015824.561 INFO             PET1 index= 573                        MPIR_CVAR_TUNING_NODE_COMM_HIERARCHY : @Alias:# I_MPI_TUNING_NODE_COMM_HIERARCHY
20220327 015824.561 INFO             PET1 index= 574                                       MPIR_CVAR_TUNING_MODE : @Alias:# I_MPI_TUNING_MODE
20220327 015824.561 INFO             PET1 index= 575                                MPIR_CVAR_TUNING_AUTO_POLICY : @Alias:# I_MPI_TUNING_AUTO_POLICY
20220327 015824.561 INFO             PET1 index= 576                             MPIR_CVAR_TUNING_AUTO_COMM_LIST : @Alias:# I_MPI_TUNING_AUTO_COMM_LIST
20220327 015824.561 INFO             PET1 index= 577                             MPIR_CVAR_TUNING_AUTO_COMM_USER : @Alias:# I_MPI_TUNING_AUTO_COMM_USER
20220327 015824.561 INFO             PET1 index= 578                          MPIR_CVAR_TUNING_AUTO_COMM_DEFAULT : @Alias:# I_MPI_TUNING_AUTO_COMM_DEFAULT
20220327 015824.561 INFO             PET1 index= 579                              MPIR_CVAR_TUNING_AUTO_ITER_NUM : @Alias:# I_MPI_TUNING_AUTO_ITER_NUM
20220327 015824.561 INFO             PET1 index= 580                           MPIR_CVAR_TUNING_AUTO_ITER_POLICY : @Alias:# I_MPI_TUNING_AUTO_ITER_POLICY
20220327 015824.561 INFO             PET1 index= 581                 MPIR_CVAR_TUNING_AUTO_ITER_POLICY_THRESHOLD : @Alias:# I_MPI_TUNING_AUTO_ITER_POLICY_THRESHOLD
20220327 015824.561 INFO             PET1 index= 582                                  MPIR_CVAR_TUNING_AUTO_SYNC : @Alias:# I_MPI_TUNING_AUTO_SYNC
20220327 015824.561 INFO             PET1 index= 583                          MPIR_CVAR_TUNING_AUTO_STORAGE_SIZE : @Alias:# I_MPI_TUNING_AUTO_STORAGE_SIZE
20220327 015824.561 INFO             PET1 index= 584                       MPIR_CVAR_TUNING_AUTO_WARMUP_ITER_NUM : @Alias:# I_MPI_TUNING_AUTO_WARMUP_ITER_NUM
20220327 015824.561 INFO             PET1 index= 585                                 MPIR_CVAR_TUNING_AUTO_SMART : @Alias:# I_MPI_TUNING_AUTO_SMART
20220327 015824.561 INFO             PET1 index= 586                                  MPIR_CVAR_TUNING_COLL_LIST : @Alias:# I_MPI_TUNING_COLL_LIST
20220327 015824.561 INFO             PET1 index= 587                               MPIR_CVAR_TUNING_COLL_VEC_OPS : @Alias:# I_MPI_TUNING_COLL_VEC_OPS
20220327 015824.561 INFO             PET1 index= 588                              MPIR_CVAR_DEFAULT_THREAD_LEVEL : @Alias:# I_MPI_THREAD_LEVEL
20220327 015824.561 INFO             PET1 index= 589                                      MPIR_CVAR_THREAD_SPLIT : Control the MPI_THREAD_SPLIT model support. @Alias:# I_MPI_THREAD_SPLIT @Default:# false
20220327 015824.561 INFO             PET1 index= 590                                    MPIR_CVAR_THREAD_RUNTIME : Control threading runtimes support. @Alias:# I_MPI_THREAD_RUNTIME @Default:# generic
20220327 015824.561 INFO             PET1 index= 591                                     MPIR_CVAR_THREAD_ID_KEY : Set the MPI info object key that is used to explicitly define the application $thread_id for a communicator. @Alias:# I_MPI_THREAD_ID_KEY @Default:# -1
20220327 015824.561 INFO             PET1 index= 592                                        MPIR_CVAR_THREAD_MAX : Set the maximum number of application threads per rank. @Alias:# I_MPI_THREAD_MAX @Default:# -1
20220327 015824.561 INFO             PET1 index= 593                                    MPIR_CVAR_ASYNC_PROGRESS : Enables asynchronous progress threads. @Alias:# I_MPI_ASYNC_PROGRESS @Default:# false
20220327 015824.561 INFO             PET1 index= 594                          MPIR_CVAR_CH4_MAX_PROGRESS_THREADS : Specifies the maximum number of progress threads. @Alias:# I_MPI_ASYNC_PROGRESS_THREADS @Default:# 1
20220327 015824.562 INFO             PET1 index= 595                      MPIR_CVAR_CH4_PROGRESS_THREAD_AFFINITY : Specifies affinity for all progress threads of local processes. @Alias:# I_MPI_ASYNC_PROGRESS_PIN @Default:# not defined
20220327 015824.562 INFO             PET1 index= 596                                         MPIR_CVAR_EP_ID_KEY : Set the MPI info object key that is used to explicitly define the progress $thread_id for a communicator. @Alias:# I_MPI_ASYNC_PROGRESS_ID_KEY @Default:# thread_id
20220327 015824.562 INFO             PET1 index= 597                                       MPIR_CVAR_THREAD_MODE : @Alias:# I_MPI_THREAD_MODE
20220327 015824.562 INFO             PET1 index= 598                                 MPIR_CVAR_THREAD_LOCK_LEVEL : @Alias:# I_MPI_THREAD_LOCK_LEVEL
20220327 015824.562 INFO             PET1 index= 599                                  MPIR_CVAR_CH4_OFI_MAX_VCIS : @Alias:# I_MPI_THREAD_EP_MAX
20220327 015824.562 INFO             PET1 index= 600                                       MPIR_CVAR_INTEL_DEBUG : Print out debugging information when an MPI program starts running.$ Syntax$ I_MPI_DEBUG=<level>$ Arguments$ <level> - indicate level of debug information provided @Alias:# I_MPI_DEBUG @Default:# 0
20220327 015824.562 INFO             PET1 index= 601                                     MPIR_CVAR_DEBUG_VERSION : Print Intel MPI version. @Alias:# I_MPI_PRINT_VERSION @Default:# 0
20220327 015824.562 INFO             PET1 index= 602                                    MPIR_CVAR_ERROR_CHECKING : @Alias:# I_MPI_ERROR_CHECKING
20220327 015824.562 INFO             PET1 index= 603                                        MPIR_CVAR_MULTI_INIT : @Alias:# I_MPI_MULTI_INIT
20220327 015824.562 INFO             PET1 index= 604                               MPIR_CVAR_REMOVED_VAR_WARNING : Print out a warning if a removed environment variable is set. @Alias:# I_MPI_REMOVED_VAR_WARNING @Default:# 1
20220327 015824.562 INFO             PET1 index= 605                                MPIR_CVAR_VAR_CHECK_SPELLING : Print out a warning if an unknown environment variable is set. @Alias:# I_MPI_VAR_CHECK_SPELLING @Default:# 1
20220327 015824.562 INFO             PET1 index= 606                           MPIR_CVAR_INTEL_MPI_COMPATIBILITY : Select the runtime compatibility mode.$ Syntax$ I_MPI_COMPATIBILITY=<value>$ Arguments$ <value> - Define compatibility mode$ -----------------------------------------------------------------------$ <not defined> - The MPI-3.1 standard compatibility$ <3> - The Intel® MPI Library 3.x compatible mode$ <4> - The Intel® MPI Library 4.x compatible mode$ <5> - The Intel® MPI Library 5.x compatible mode$ ----------------------------------------------------------------------- @Alias:# I_MPI_COMPATIBILITY @Default:# 5
20220327 015824.562 INFO             PET1 index= 607                          MPIR_CVAR_IMPI_PROGRESS_SPIN_COUNT : @Alias:# I_MPI_SPIN_COUNT
20220327 015824.562 INFO             PET1 index= 608                         MPIR_CVAR_IMPI_PROGRESS_PAUSE_COUNT : @Alias:# I_MPI_PAUSE_COUNT
20220327 015824.562 INFO             PET1 index= 609                                 MPIR_CVAR_IMPI_THREAD_YIELD : @Alias:# I_MPI_THREAD_YIELD
20220327 015824.562 INFO             PET1 index= 610                                      MPIR_CVAR_SILENT_ABORT : Do not print abort warning message @Alias:# I_MPI_SILENT_ABORT
20220327 015824.562 INFO             PET1 index= 611                                  MPIR_CVAR_JOB_IDLE_TIMEOUT : Abort job if idle time is larger than the threshold in seconds. @Alias:# I_MPI_JOB_IDLE_TIMEOUT
20220327 015824.562 INFO             PET1 index= 612                              MPIR_CVAR_PMI_VALUE_LENGTH_MAX : Set PMI buffer length as minimum of variable value and PMI_KVS_Get_value_length_max(). @Alias:# I_MPI_PMI_VALUE_LENGTH_MAX
20220327 015824.562 INFO             PET1 index= 613                                       MPIR_CVAR_PMI_LIBRARY : Specify the name to third party implementation of the PMI library. @Alias:# I_MPI_PMI_LIBRARY
20220327 015824.562 INFO             PET1 index= 614                                               MPIR_CVAR_PMI : Select PMI version. Choices: auto, pmi1, pmi2, pmix.$ By default pmi version will be chosen automatically. @Alias:# I_MPI_PMI
20220327 015824.562 INFO             PET1 index= 615                                 MPIR_CVAR_NODEMAP_ALGORITHM : Select algorithm for nodemap creation. Choices: pmi_process_mapping, slurm, pmi_alltoall, auto. @Alias:# I_MPI_NODEMAP_ALGORITHM
20220327 015824.562 INFO             PET1 index= 616                                      MPIR_CVAR_ASYNC_REDUCE : @Alias:# I_MPI_ASYNC_REDUCE @Verbosity:# hidden
20220327 015824.562 INFO             PET1 index= 617                            MPIR_CVAR_CH4_MAX_REDUCE_THREADS : @Alias:# I_MPI_ASYNC_REDUCE_THREADS @Verbosity:# hidden
20220327 015824.562 INFO             PET1 index= 618                      MPIR_CVAR_ASYNC_REDUCE_COUNT_THRESHOLD : @Alias:# I_MPI_ASYNC_REDUCE_COUNT_THRESHOLD @Verbosity:# hidden
20220327 015824.562 INFO             PET1 index= 619                        MPIR_CVAR_CH4_REDUCE_THREAD_AFFINITY : @Alias:# I_MPI_ASYNC_REDUCE_PIN @Verbosity:# hidden
20220327 015824.562 INFO             PET1 --- VMK::logSystem() end ---------------------------------
20220327 015824.562 INFO             PET1 main: --- VMK::log() start -------------------------------------
20220327 015824.562 INFO             PET1 main: vm located at: 0x87ae60
20220327 015824.562 INFO             PET1 main: petCount=6 localPet=1 mypthid=140737352203136 currentSsiPe=4
20220327 015824.562 INFO             PET1 main: Current system level affinity pinning for local PET:
20220327 015824.562 INFO             PET1 main:  SSIPE=4
20220327 015824.562 INFO             PET1 main:  SSIPE=5
20220327 015824.562 INFO             PET1 main:  SSIPE=6
20220327 015824.562 INFO             PET1 main:  SSIPE=7
20220327 015824.562 INFO             PET1 main: Current system level OMP_NUM_THREADS setting for local PET: 1
20220327 015824.562 INFO             PET1 main: ssiCount=1 localSsi=0
20220327 015824.562 INFO             PET1 main: mpionly=1 threadsflag=0
20220327 015824.562 INFO             PET1 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220327 015824.562 INFO             PET1 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220327 015824.562 INFO             PET1 main:  PE=0 SSI=0 SSIPE=0
20220327 015824.562 INFO             PET1 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220327 015824.562 INFO             PET1 main:  PE=1 SSI=0 SSIPE=1
20220327 015824.562 INFO             PET1 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220327 015824.562 INFO             PET1 main:  PE=2 SSI=0 SSIPE=2
20220327 015824.562 INFO             PET1 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220327 015824.562 INFO             PET1 main:  PE=3 SSI=0 SSIPE=3
20220327 015824.562 INFO             PET1 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220327 015824.562 INFO             PET1 main:  PE=4 SSI=0 SSIPE=4
20220327 015824.562 INFO             PET1 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220327 015824.562 INFO             PET1 main:  PE=5 SSI=0 SSIPE=5
20220327 015824.562 INFO             PET1 main: --- VMK::log() end ---------------------------------------
20220327 015824.565 INFO             PET1 Executing 'userm1_setvm'
20220327 015824.566 INFO             PET1 Executing 'userm1_register'
20220327 015824.566 INFO             PET1 Executing 'userm2_setvm'
20220327 015824.566 DEBUG            PET1 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220327 015824.566 DEBUG            PET1 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220327 015824.649 INFO             PET1 Entering 'user1_run'
20220327 015824.649 INFO             PET1 model1: --- VMK::log() start -------------------------------------
20220327 015824.649 INFO             PET1 model1: vm located at: 0x9fb9f0
20220327 015824.649 INFO             PET1 model1: petCount=6 localPet=1 mypthid=140737352203136 currentSsiPe=1
20220327 015824.649 INFO             PET1 model1: Current system level affinity pinning for local PET:
20220327 015824.649 INFO             PET1 model1:  SSIPE=1
20220327 015824.649 INFO             PET1 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220327 015824.649 INFO             PET1 model1: ssiCount=1 localSsi=0
20220327 015824.649 INFO             PET1 model1: mpionly=1 threadsflag=0
20220327 015824.649 INFO             PET1 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220327 015824.649 INFO             PET1 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220327 015824.649 INFO             PET1 model1:  PE=0 SSI=0 SSIPE=0
20220327 015824.649 INFO             PET1 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220327 015824.649 INFO             PET1 model1:  PE=1 SSI=0 SSIPE=1
20220327 015824.649 INFO             PET1 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220327 015824.649 INFO             PET1 model1:  PE=2 SSI=0 SSIPE=2
20220327 015824.649 INFO             PET1 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220327 015824.649 INFO             PET1 model1:  PE=3 SSI=0 SSIPE=3
20220327 015824.649 INFO             PET1 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220327 015824.649 INFO             PET1 model1:  PE=4 SSI=0 SSIPE=4
20220327 015824.649 INFO             PET1 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220327 015824.649 INFO             PET1 model1:  PE=5 SSI=0 SSIPE=5
20220327 015824.649 INFO             PET1 model1: --- VMK::log() end ---------------------------------------
20220327 015824.649 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220327 015824.775 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220327 015824.900 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220327 015825.025 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220327 015825.150 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220327 015825.275 INFO             PET1 Exiting 'user1_run'
20220327 015826.769 INFO             PET1 Entering 'user1_run'
20220327 015826.769 INFO             PET1 model1: --- VMK::log() start -------------------------------------
20220327 015826.769 INFO             PET1 model1: vm located at: 0x9fb9f0
20220327 015826.769 INFO             PET1 model1: petCount=6 localPet=1 mypthid=140737352203136 currentSsiPe=1
20220327 015826.769 INFO             PET1 model1: Current system level affinity pinning for local PET:
20220327 015826.769 INFO             PET1 model1:  SSIPE=1
20220327 015826.769 INFO             PET1 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220327 015826.769 INFO             PET1 model1: ssiCount=1 localSsi=0
20220327 015826.769 INFO             PET1 model1: mpionly=1 threadsflag=0
20220327 015826.769 INFO             PET1 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220327 015826.769 INFO             PET1 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220327 015826.769 INFO             PET1 model1:  PE=0 SSI=0 SSIPE=0
20220327 015826.769 INFO             PET1 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220327 015826.769 INFO             PET1 model1:  PE=1 SSI=0 SSIPE=1
20220327 015826.769 INFO             PET1 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220327 015826.769 INFO             PET1 model1:  PE=2 SSI=0 SSIPE=2
20220327 015826.769 INFO             PET1 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220327 015826.769 INFO             PET1 model1:  PE=3 SSI=0 SSIPE=3
20220327 015826.769 INFO             PET1 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220327 015826.769 INFO             PET1 model1:  PE=4 SSI=0 SSIPE=4
20220327 015826.769 INFO             PET1 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220327 015826.769 INFO             PET1 model1:  PE=5 SSI=0 SSIPE=5
20220327 015826.769 INFO             PET1 model1: --- VMK::log() end ---------------------------------------
20220327 015826.769 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220327 015826.894 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220327 015827.020 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220327 015827.145 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220327 015827.272 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220327 015827.398 INFO             PET1 Exiting 'user1_run'
20220327 015828.897 INFO             PET1  NUMBER_OF_PROCESSORS           6
20220327 015828.897 INFO             PET1  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220327 015828.898 INFO             PET1 Finalizing ESMF
20220327 015824.554 INFO             PET2 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220327 015824.554 INFO             PET2 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220327 015824.554 INFO             PET2 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220327 015824.554 INFO             PET2 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220327 015824.554 INFO             PET2 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220327 015824.554 INFO             PET2 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220327 015824.554 INFO             PET2 Running with ESMF Version   : v8.3.0b10-105-ga5295e34ae
20220327 015824.554 INFO             PET2 ESMF library build date/time: "Mar 27 2022" "01:23:54"
20220327 015824.554 INFO             PET2 ESMF library build location : /gpfsm/dnb04/projects/p98/mpotts/esmf/intel_19.1.3_intelmpi_O_develop
20220327 015824.554 INFO             PET2 ESMF_COMM                   : intelmpi
20220327 015824.555 INFO             PET2 ESMF_MOAB                   : enabled
20220327 015824.555 INFO             PET2 ESMF_LAPACK                 : enabled
20220327 015824.555 INFO             PET2 ESMF_NETCDF                 : enabled
20220327 015824.555 INFO             PET2 ESMF_PNETCDF                : disabled
20220327 015824.555 INFO             PET2 ESMF_PIO                    : enabled
20220327 015824.555 INFO             PET2 ESMF_YAMLCPP                : enabled
20220327 015824.555 INFO             PET2 --- VMK::logSystem() start -------------------------------
20220327 015824.555 INFO             PET2 esmfComm=intelmpi
20220327 015824.555 INFO             PET2 isPthreadsEnabled=1
20220327 015824.555 INFO             PET2 isOpenMPEnabled=1
20220327 015824.555 INFO             PET2 isOpenACCEnabled=0
20220327 015824.556 INFO             PET2 isSsiSharedMemoryEnabled=1
20220327 015824.556 INFO             PET2 ssiCount=1 peCount=6
20220327 015824.556 INFO             PET2 PE=0 SSI=0 SSIPE=0
20220327 015824.556 INFO             PET2 PE=1 SSI=0 SSIPE=1
20220327 015824.556 INFO             PET2 PE=2 SSI=0 SSIPE=2
20220327 015824.556 INFO             PET2 PE=3 SSI=0 SSIPE=3
20220327 015824.556 INFO             PET2 PE=4 SSI=0 SSIPE=4
20220327 015824.556 INFO             PET2 PE=5 SSI=0 SSIPE=5
20220327 015824.556 INFO             PET2 --- VMK::logSystem() MPI Control Variables ---------------
20220327 015824.556 INFO             PET2 index=   0                                          I_MPI_DEBUG_OUTPUT : @Default:# not defined
20220327 015824.556 INFO             PET2 index=   1                                        I_MPI_DEBUG_COREDUMP : @Default:# 1
20220327 015824.556 INFO             PET2 index=   2                                          I_MPI_LIBRARY_KIND : @Default:# not defined
20220327 015824.556 INFO             PET2 index=   3                                  I_MPI_OFI_LIBRARY_INTERNAL : @Default:# not defined
20220327 015824.556 INFO             PET2 index=   4                                            I_MPI_CC_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET2 index=   5                                           I_MPI_CXX_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET2 index=   6                                            I_MPI_FC_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET2 index=   7                                           I_MPI_F77_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET2 index=   8                                           I_MPI_F90_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET2 index=   9                                         I_MPI_TRACE_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  10                                         I_MPI_CHECK_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  11                                        I_MPI_CHECK_COMPILER : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  12                                                    I_MPI_CC : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  13                                                   I_MPI_CXX : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  14                                                    I_MPI_FC : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  15                                                   I_MPI_F90 : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  16                                                   I_MPI_F77 : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  17                                                  I_MPI_ROOT : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  18                                           I_MPI_ONEAPI_ROOT : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  19                                           MPIR_CVAR_VT_ROOT : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  20                                   I_MPI_COMPILER_CONFIG_DIR : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  21                                                  I_MPI_LINK : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  22                                      I_MPI_DEBUG_INFO_STRIP : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  23                                                I_MPI_CFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  24                                              I_MPI_CXXFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  25                                               I_MPI_FCFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  26                                                I_MPI_FFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  27                                               I_MPI_LDFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  28                                             I_MPI_FORT_BIND : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  29                                           I_MPI_AUTH_METHOD : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  30                               I_MPI_HYDRA_COLLECTIVE_LAUNCH : @Default:# 1
20220327 015824.556 INFO             PET2 index=  31                                  I_MPI_HYDRA_UNIQUE_PROXIES : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  32                                        I_MPI_FAULT_CONTINUE : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  33                                   I_MPI_FAULT_NODE_CONTINUE : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  34                                                I_MPI_MPIRUN : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  35                                            I_MPI_BIND_ORDER : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  36                                             I_MPI_BIND_NUMA : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  37                                     I_MPI_BIND_WIN_ALLOCATE : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  38                                      I_MPI_HYDRA_NAMESERVER : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  39                                        I_MPI_JOB_CHECK_LIBS : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  40                                    I_MPI_HYDRA_SERVICE_PORT : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  41                                           I_MPI_HYDRA_DEBUG : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  42                                             I_MPI_HYDRA_ENV : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  43                                           I_MPI_JOB_TIMEOUT : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  44                                       I_MPI_MPIEXEC_TIMEOUT : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  45                                   I_MPI_JOB_STARTUP_TIMEOUT : Set this environment variable to make mpiexec.hydra terminate$ the job in <timeout> seconds if some processes are not launched. @Default:# -1
20220327 015824.556 INFO             PET2 index=  46                                    I_MPI_JOB_TIMEOUT_SIGNAL : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  47                                      I_MPI_JOB_ABORT_SIGNAL : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  48                                I_MPI_JOB_SIGNAL_PROPAGATION : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  49                                  I_MPI_HYDRA_BOOTSTRAP_EXEC : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  50                       I_MPI_HYDRA_BOOTSTRAP_EXEC_EXTRA_ARGS : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  51                              I_MPI_HYDRA_BOOTSTRAP_AUTOFORK : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  52                                             I_MPI_HYDRA_RMK : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  53                                     I_MPI_HYDRA_PMI_CONNECT : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  54                                         I_MPI_HYDRA_TOPOLIB : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  55                                            I_MPI_PORT_RANGE : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  56                         I_MPI_JOB_RESPECT_PROCESS_PLACEMENT : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  57                                                I_MPI_TMPDIR : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  58                                           I_MPI_HYDRA_DEMUX : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  59                                           I_MPI_HYDRA_IFACE : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  60                                I_MPI_HYDRA_GDB_REMOTE_SHELL : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  61                                   I_MPI_HYDRA_PMI_AGGREGATE : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  62                                        I_MPI_JOB_TRACE_LIBS : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  63                                       I_MPI_HYDRA_HOST_FILE : Set the host file to run the application.$ Syntax$ I_MPI_HYDRA_HOST_FILE=<arg>$ Arguments$ <arg> - String parameter$ -----------------------------------------------------------------------$ <hostsfile> - The full or relative path to the host file$ ----------------------------------------------------------------------- @Default:# not defined
20220327 015824.556 INFO             PET2 index=  64                                     I_MPI_HYDRA_HOSTS_GROUP : This environment variable allows to set node ranges using brackets,$ commas, and dashes (like in Slurm* Workload Manager). @Default:# not defined
20220327 015824.556 INFO             PET2 index=  65                                               I_MPI_PERHOST : Define the default behavior for the -perhost option of the mpiexec.hydra$ command.$ Syntax$ I_MPI_PERHOST=<value>$ Arguments$ <value> - Define a value used for -perhost by default$ -----------------------------------------------------------------------$ <integer > 0> - Exact value for the option$ <all>         - All logical CPUs on the node$ <allcores>    - All cores (physical CPUs) on the node. This is the$  default value.$ ----------------------------------------------------------------------- @Default:# not defined
20220327 015824.556 INFO             PET2 index=  66                                                 I_MPI_GTOOL : Specify the tools to be launched for selected ranks. An alternative to$ this variable is the -gtool option$ Syntax$ I_MPI_GTOOL="<command line for a tool 1>:<ranks set 1>[=exclusive]$ [@arch 1];<command line for a tool 2>:<ranks set 2>[=exclusive]$ [@arch 2]; â€¦ ;<command line for a tool n>:<ranks set n>[=exclusive]$ [@arch n]"$ Arguments$ <arg> - Specify a tool launch command, including parameters.$ -----------------------------------------------------------------------$ <command line>     - Specify tool launch command, including parameters$ <rank set>         - Specify the range of ranks that are involved in the$  tool execution. Separate ranks with a comma or use the '-' symbol for$ a set ofcontiguous ranks. To run the tool forall ranks, use the all$  argument.$ [=exclusive]       - All cores (physical CPUs) on the node. This is$ the default value.$ [@arch]            - Specify the architecture on which the tool runs$  optional). For a given <rank set>, if you specify this argument,$ the tool is launched
20220327 015824.556 INFO             PET2 index=  67                                    I_MPI_HYDRA_BRANCH_COUNT : Set this environment variable to restrict the number of child management$ processes launched by the mpiexec.hydra operation or by each pmi_proxy$ anagement process.$ Syntax$ I_MPI_HYDRA_BRANCH_COUNT=<num>$ Arguments$ <value> - Number$ -----------------------------------------------------------------------$ <n> >= 0 - The default value is -1 if less than 128 nodes are used. $ This value also means that there is no hierarchical structure$ The default value is 32 if more than 127 nodes are used$ ----------------------------------------------------------------------- @Default:# not defined
20220327 015824.556 INFO             PET2 index=  68                                       I_MPI_HYDRA_BOOTSTRAP : Set this environment variable to specify the bootstrap server Syntax$ I_MPI_HYDRA_BOOTSTRAP=<arg>$ Arguments$ <arg> - String parameter$ -----------------------------------------------------------------------$ <ssh>     - Use secure shell. This is the default value$ <rsh>     - Use remote shell$ <pdsh>    - Use parallel distributed shell$ <pbsdsh>  - Use Torque* and PBS* pbsdsh command$ <fork>    - Use fork call$ <slurm>   - Use SLURM* srun command$ <ll>      - Use LoadLeveler* llspawn.stdio command$ <lsf>     - Use LSF* blaunch command$ <sge>     - Use Univa* Grid Engine* qrsh command$ ----------------------------------------------------------------------- @Default:# not defined
20220327 015824.556 INFO             PET2 index=  69                                              I_MPI_PIN_UNIT : @Default:# not defined
20220327 015824.556 INFO             PET2 index=  70                                                 I_MPI_STATS :
20220327 015824.556 INFO             PET2 index=  71                                             I_MPI_TIMER_ART : @Default:# 1
20220327 015824.556 INFO             PET2 index=  72                                   I_MPI_OFFLOAD_DOMAIN_SIZE : Control the number of devices/tiles per MPI rank
20220327 015824.556 INFO             PET2 index=  73                                          I_MPI_OFFLOAD_CELL : Variable to choose the base unit: tile or device
20220327 015824.556 INFO             PET2 index=  74                                       I_MPI_OFFLOAD_DEVICES : Variable to select available devices
20220327 015824.556 INFO             PET2 index=  75                                   I_MPI_OFFLOAD_DEVICE_LIST : A comma-separated list of tiles and/or ranges of tiles.$ The process with the i-th rank is pinned to the i-th tile in the list.$ If I_MPI_OFFLOAD_CELL=device then it is comma-separated list of devices.
20220327 015824.556 INFO             PET2 index=  76                                        I_MPI_OFFLOAD_DOMAIN : Define domains through the comma separated list of hexadecimal numbers (domain masks).
20220327 015824.556 INFO             PET2 index=  77                               I_MPI_OFFLOAD_INFO_SET_NTILES : Set the number of tiles
20220327 015824.556 INFO             PET2 index=  78                                I_MPI_OFFLOAD_INFO_SET_NGPUS : Set the number of gpus
20220327 015824.556 INFO             PET2 index=  79                           I_MPI_OFFLOAD_INFO_SET_NNUMANODES : Set the number of numanodes
20220327 015824.556 INFO             PET2 index=  80                               I_MPI_OFFLOAD_INFO_SET_GPU_ID : Set gpu id for each tile
20220327 015824.556 INFO             PET2 index=  81                 I_MPI_OFFLOAD_INFO_SET_NUMANODE_ID_FOR_GPUS : Set numanode id for each gpu
20220327 015824.556 INFO             PET2 index=  82                I_MPI_OFFLOAD_INFO_SET_NUMANODE_ID_FOR_RANKS : Set numanode id for each rank
20220327 015824.556 INFO             PET2 index=  83                            I_MPI_OFFLOAD_INFO_SET_VENDOR_ID : Set vendor id for each device
20220327 015824.557 INFO             PET2 index=  84                                         MPIR_CVAR_INTEL_PIN : Turn on/off process pinning.$ Syntax$ I_MPI_PIN=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 - Enable process pinning$ > disable | no | off | 0 - Disable processes pinning$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN @Default:# on
20220327 015824.557 INFO             PET2 index=  85                          MPIR_CVAR_INTEL_PIN_SHOW_REAL_MASK : Turn on/off real masks pinning print.$ Syntax$ I_MPI_PIN_SHOW_REAL_MASK=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 - Enable real pinning print$ > disable | no | off | 0 - Disable real pinning print$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_SHOW_REAL_MASK @Default:# on
20220327 015824.557 INFO             PET2 index=  86                          MPIR_CVAR_INTEL_PIN_PROCESSOR_LIST : Define a processor subset and the mapping rules for MPI processes within$ this subset.$ Syntax$ I_MPI_PIN_PROCESSOR_LIST=<value>$ The environment variable value has the following syntax forms:$ 1. <proclist>$ 2. [<procset>][:[grain=<grain>][,shift=<shift>]$ [,preoffset=<preoffset>][,postoffset=<postoffset>]$ 3. [<procset>][:map=<map>]$ @Alias:# I_MPI_PIN_PROCESSOR_LIST @Default:# not defined
20220327 015824.557 INFO             PET2 index=  87                  MPIR_CVAR_INTEL_PIN_PROCESSOR_EXCLUDE_LIST : Define a subset of logical processors to be excluded for the pinning$ capability on the intended hosts.$ Syntax$ I_MPI_PIN_PROCESSOR_EXCLUDE_LIST=<proclist>$ Arguments$ <proclist> - A comma-separated list of logical processor numbers$ and/or ranges of processors.$ -----------------------------------------------------------------------$ > <l> - Processor with logical number <l>.$ > <l>-<m> - Range of processors with logical numbers from <l> to <m>.$ > <k>,<l>-<m> - Processors <k>, as well as <l> through <m>.$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_PROCESSOR_EXCLUDE_LIST @Default:# not defined
20220327 015824.557 INFO             PET2 index=  88                                    MPIR_CVAR_INTEL_PIN_CELL : Set this environment variable to define the pinning resolution$ granularity. I_MPI_PIN_CELL specifies the minimal processor cell$ allocated when an MPI process is running.$ Syntax$ I_MPI_PIN_CELL=<cell>$ Arguments$ <cell> - Specify the resolution granularity$ -----------------------------------------------------------------------$ > unit - Basic processor unit (logical CPU)$ > core - Physical processor core$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_CELL @Default:# unit
20220327 015824.557 INFO             PET2 index=  89                          MPIR_CVAR_INTEL_PIN_RESPECT_CPUSET : Respect the process affinity mask.$ Syntax$ I_MPI_PIN_RESPECT_CPUSET=<value>$ Arguments$ <value> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 - Respect the process affinity mask$ > disable | no | off | 0 - Do not respect the process affinity mask$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_RESPECT_CPUSET @Default:# on
20220327 015824.557 INFO             PET2 index=  90                             MPIR_CVAR_INTEL_PIN_RESPECT_HCA : In the presence of Intel(R) Omni-Path Architecture (Intel(R) OPA) or$ Infiniband architecture* host channel adapter (IBA* HCA),$ adjust the pinning according to the location of adapter.$ Syntax$ I_MPI_PIN_RESPECT_HCA=<value>$ Arguments$ <value> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 Use the location of IBA HCA if available$ > disable | no | off | 0 Do not use the location of IBA HCA$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_RESPECT_HCA @Default:# on
20220327 015824.557 INFO             PET2 index=  91                                  MPIR_CVAR_INTEL_PIN_DOMAIN : Intel(R) MPI Library provides environment variable to control process pinning for hybrid MPI/OpenMP* applications. This environment variable is used to define a number of non-overlapping subsets (domains) of logical processors on a node, and a set of rules on how MPI processes are bound to these domains by the following formula: one MPI process per one domain. Multi-core Shape:$ I_MPI_PIN_DOMAIN=<mc-shape>$ <mc-shape> - Define domains through multi-core terms.$ -----------------------------------------------------------------------$ > core - Each domain consists of the logical processors that share a$ particular core. The number of domains on a node is equal to the number$ of cores on the node.$ > socket | sock - Each domain consists of the logical processors that$ share a particular socket. The number of domains on a node is equal to$ the number of sockets on the node.$ > numa - Each domain consists of the logical processors that share a$ particular NUMA node. The number of domains on a machine is equal to$
20220327 015824.557 INFO             PET2 index=  92                                   MPIR_CVAR_INTEL_PIN_ORDER : Set this environment variable to define the mapping order for MPI$ processes to domains as specified by the$ I_MPI_PIN_DOMAIN environment variable.$ Syntax$ I_MPI_PIN_ORDER=<order>$ <order> - Specify the ranking order$ -----------------------------------------------------------------------$ > range - The domains are ordered according to the processor's BIOS$ numbering. This is a platform dependent numbering$ > scatter - The domains are ordered so that adjacent domains have$ minimal sharing of common resources$ > compact - The domains are ordered so that adjacent domains share$ common resources as much as possible. This is the default value$ > spread - The domains are ordered consecutively with the possibility$ not to share common resources$ > bunch - The processes are mapped proportionally to sockets and the$ domains are ordered as close as possible on the sockets$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_ORDER @Default:# compact
20220327 015824.557 INFO             PET2 index=  93                                   MPIR_CVAR_IMPI_HBW_POLICY : @Alias:# I_MPI_HBW_POLICY
20220327 015824.557 INFO             PET2 index=  94                          MPIR_CVAR_IMPI_INTERNAL_MEM_POLICY : @Alias:# I_MPI_INTERNAL_MEM_POLICY
20220327 015824.557 INFO             PET2 index=  95                                 MPIR_CVAR_IMPI_STATIC_BUILD : @Alias:# I_MPI_STATIC_BUILD
20220327 015824.557 INFO             PET2 index=  96                     MPIR_CVAR_IMPI_RETURN_INTERNAL_MEM_NUMA : @Alias:# I_MPI_RETURN_INTERNAL_MEM_NUMA
20220327 015824.557 INFO             PET2 index=  97                                   MPIR_CVAR_OFFLOAD_TOPOLIB : @Alias:# I_MPI_OFFLOAD_TOPOLIB
20220327 015824.557 INFO             PET2 index=  98                                        MPIR_CVAR_ENABLE_GPU : Control support of buffers offloaded to GPU/accelerator in MPI calls.$ Syntax$ I_MPI_OFFLOAD=<value>$ Arguments$ <value> - choice$ -----------------------------------------------------------------------$ <0> - Disabled$ <1> - Enabled only if Level Zero library is loaded at MPI_Init() time$ <2> - Enabled, will fail if Level Zero library is not loadable$ ----------------------------------------------------------------------- @Alias:# I_MPI_OFFLOAD @Default:# 0
20220327 015824.557 INFO             PET2 index=  99                        MPIR_CVAR_ENABLE_GPU_BUFFER_CHECKING : Turn on/off bounds checking of the offloaded buffers.$ @Alias:# I_MPI_OFFLOAD_BUFFER_CHECKING @Default:# 1
20220327 015824.557 INFO             PET2 index= 100                        MPIR_CVAR_OFFLOAD_LEVEL_ZERO_LIBRARY : Specify name or full path to Level Zero ze_loader library.$ @Alias:# I_MPI_OFFLOAD_LEVEL_ZERO_LIBRARY
20220327 015824.557 INFO             PET2 index= 101                            MPIR_CVAR_INTEL_EXTRA_FILESYSTEM : Turn on/off native parallel file systems support.$ Syntax$ I_MPI_EXTRA_FILESYSTEM=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 - Enable native support for parallel file$ systems.$ > disable | no | off | 0 - Disable native support for parallel file$ systems.$ ----------------------------------------------------------------------- @Alias:# I_MPI_EXTRA_FILESYSTEM @Default:# off
20220327 015824.557 INFO             PET2 index= 102                      MPIR_CVAR_INTEL_EXTRA_FILESYSTEM_FORCE : Force filesystem recognition logic.$ Syntax$ I_MPI_EXTRA_FILESYSTEM_FORCE=<ufs|nfs|gpfs|panfs|lustre|daos>$ @Alias:# I_MPI_EXTRA_FILESYSTEM_FORCE @Default:# not defined
20220327 015824.557 INFO             PET2 index= 103                                     MPIR_CVAR_INTEL_FABRICS : Select the particular fabrics to be used.$ Syntax$ I_MPI_FABRICS=<ofi|shm:ofi>$ Arguments$ <fabric> -  Define a network fabric.$ -----------------------------------------------------------------------$ > shm - Shared memory transport (used for intra-node$ communication only).$ > ofi - OpenFabrics Interfaces* (OFI)-capable network fabrics, such as$ Intel(R) True Scale Fabric, Intel(R) Omni-Path Architecture, InfiniBand*$ and Ethernet (through OFI$ API).$ ----------------------------------------------------------------------- @Alias:# I_MPI_FABRICS @Default:# shm:ofi
20220327 015824.557 INFO             PET2 index= 104                                       MPIR_CVAR_IMPI_MALLOC : Enable or disable the Intel MPI private memory allocator.$ ----------------------------------------------------------------------- @Alias:# I_MPI_MALLOC @Default:# 1
20220327 015824.557 INFO             PET2 index= 105                                    MPIR_CVAR_INTEL_SHM_HEAP : Enable or disable the Intel MPI shared memory allocator.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP @Default:# -1
20220327 015824.557 INFO             PET2 index= 106                                MPIR_CVAR_INTEL_SHM_HEAP_OPT : Shared memory heap optimization: "rank", "numa".$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_OPT @Default:# -1
20220327 015824.557 INFO             PET2 index= 107                              MPIR_CVAR_INTEL_SHM_HEAP_VSIZE : Set shared memory heap virtual size (in MB).$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_VSIZE @Default:# -1
20220327 015824.557 INFO             PET2 index= 108                              MPIR_CVAR_INTEL_SHM_HEAP_CSIZE : Set shared memory heap cache size (in MB).$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_CSIZE @Default:# -1
20220327 015824.557 INFO             PET2 index= 109                  MPIR_CVAR_INTEL_SHM_HEAP_NCONTIG_THRESHOLD : Set non-contig object size threshold for use shared memory allocator.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_NCONTIG_THRESHOLD @Default:# -1
20220327 015824.557 INFO             PET2 index= 110                          MPIR_CVAR_INTEL_SHM_HEAP_THRESHOLD : Set object size threshold for use shared memory allocator.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_THRESHOLD @Default:# -1
20220327 015824.557 INFO             PET2 index= 111        MPIR_CVAR_INTEL_SHM_RECV_HEAP_SHORT_MEMCPY_THRESHOLD : Threshold for short size messages receive via SHM HEAP transport.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_HEAP_SHORT_MEMCPY_THRESHOLD @Default:# -1
20220327 015824.557 INFO             PET2 index= 112      MPIR_CVAR_INTEL_SHM_RECV_HEAP_REGULAR_MEMCPY_THRESHOLD : Threshold for regular size message receive via SHM HEAP transport.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_HEAP_REGULAR_MEMCPY_THRESHOLD @Default:# -1
20220327 015824.557 INFO             PET2 index= 113            MPIR_CVAR_INTEL_SHM_RECV_INTER_HEAP_SHORT_MEMCPY : Name of memory copy function for short message receive via SHM HEAP transport, inter socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTER_HEAP_SHORT_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET2 index= 114            MPIR_CVAR_INTEL_SHM_RECV_INTRA_HEAP_SHORT_MEMCPY : Name of memory copy function for short message receive via SHM HEAP transport, intra socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTRA_HEAP_SHORT_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET2 index= 115          MPIR_CVAR_INTEL_SHM_RECV_INTER_HEAP_REGULAR_MEMCPY : Name of memory copy function for regular receive messages via SHM HEAP transport, inter socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTER_HEAP_REGULAR_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET2 index= 116          MPIR_CVAR_INTEL_SHM_RECV_INTRA_HEAP_REGULAR_MEMCPY : Name of memory copy function for regular receive messages via SHM HEAP transport, intra socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTRA_HEAP_REGULAR_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET2 index= 117                  MPIR_CVAR_INTEL_SHM_RECV_INTER_HEAP_MEMCPY : Name of memory copy function for receive messages via SHM HEAP transport, inter socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTER_HEAP_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET2 index= 118                  MPIR_CVAR_INTEL_SHM_RECV_INTRA_HEAP_MEMCPY : Name of memory copy function for receive messages via SHM HEAP transport, intra socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTRA_HEAP_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET2 index= 119                               MPIR_CVAR_CH4_SHM_POSIX_EAGER : Select a shared memory transport to be used.$ Syntax$ I_MPI_SHM=<transport>$ Arguments$ <transport> - Define a shared memory transport solution.$ -----------------------------------------------------------------------$ > disable | no | off | 0 - Do not use shared memory transport.$ > auto - Select a shared memory transport solution automatically.$ > bdw_sse - The shared memory transport solution tuned for Intel(R)$ microarchitecture code name Broadwell. The SSE/SSE2/SSE3 instruction$ set is used.$ > bdw_avx2 - The shared memory transport solution tuned for Intel(R)$ microarchitecture code name Broadwell. The AVX2 instruction set is used.$ > skx_sse - The shared memory transport solution tuned for Intel(R)$ Xeon(R) processors based on Intel(R) microarchitecture code name Skylake.$ The SSE/SSE2/SSE3 instruction set is used.$ > skx_avx2 - The shared memory transport solution tuned for Intel(R)$ Xeon(R) processors based on Intel(R) microarchitecture code name Skylake.$ The AVX2 instruction set is used.$ > skx_av
20220327 015824.557 INFO             PET2 index= 120                                     MPIR_CVAR_INTEL_SHM_OPT : Select a shared memory transport optimization strategy to be used.$ Syntax$ I_MPI_SHM_OPT=<optimization_strategy>$ Arguments$ <optimization_strategy> - Define a shared memory transport optimization strategy.$ -----------------------------------------------------------------------$ > dynamic - Let shared memory transport make decision in runtime.$ > intra - Optimize intra socket message passing.$ > inter - Optimize inter socket message passing.$ -----------------------------------------------------------------------$ @Alias:# I_MPI_SHM_OPT @Default:# dynamic
20220327 015824.557 INFO             PET2 index= 121                           MPIR_CVAR_INTEL_SHM_CELL_FWD_SIZE : Change the size of a shared memory forward cell. @Alias:# I_MPI_SHM_CELL_FWD_SIZE @Default:# -1
20220327 015824.557 INFO             PET2 index= 122                           MPIR_CVAR_INTEL_SHM_CELL_BWD_SIZE : Change the size of a shared memory backward cell. @Alias:# I_MPI_SHM_CELL_BWD_SIZE @Default:# -1
20220327 015824.557 INFO             PET2 index= 123                           MPIR_CVAR_INTEL_SHM_CELL_EXT_SIZE : Change the size of a shared memory extended cell. @Alias:# I_MPI_SHM_CELL_EXT_SIZE @Default:# -1
20220327 015824.557 INFO             PET2 index= 124                            MPIR_CVAR_INTEL_SHM_CELL_FWD_NUM : Change the number of forward cells in the shared memory transport (per rank). @Alias:# I_MPI_SHM_CELL_FWD_NUM @Default:# -1
20220327 015824.557 INFO             PET2 index= 125                       MPIR_CVAR_INTEL_SHM_CELL_FWD_HOLD_NUM : Change the number of hold forward cells in the shared memory transport (per rank). @Alias:# I_MPI_SHM_CELL_FWD_HOLD_NUM @Default:# -1
20220327 015824.557 INFO             PET2 index= 126                            MPIR_CVAR_INTEL_SHM_CELL_BWD_NUM : Change the number of backward cells in the shared memory transport (per rank). @Alias:# I_MPI_SHM_CELL_BWD_NUM @Default:# -1
20220327 015824.557 INFO             PET2 index= 127                     MPIR_CVAR_INTEL_SHM_CELL_BWD_NUMA_AWARE : Use NUMA aware backward cells (1 : true, 0 : false, -1 : auto) (per rank). @Alias:# I_MPI_SHM_CELL_BWD_NUMA_AWARE @Default:# -1
20220327 015824.557 INFO             PET2 index= 128                      MPIR_CVAR_INTEL_SHM_CELL_EXT_NUM_TOTAL : Change the total number of extended cells in the shared memory$ transport. @Alias:# I_MPI_SHM_CELL_EXT_NUM_TOTAL @Default:# -1
20220327 015824.557 INFO             PET2 index= 129                            MPIR_CVAR_INTEL_SHM_MCDRAM_LIMIT : Change the size of the shared memory bound to the multi-channel DRAM (MCDRAM) (size per rank). @Alias:# I_MPI_SHM_MCDRAM_LIMIT @Default:# -1
20220327 015824.557 INFO             PET2 index= 130                         MPIR_CVAR_INTEL_SHM_SEND_SPIN_COUNT : Control the spin count value for the shared memory transport for sending messages. @Alias:# I_MPI_SHM_SEND_SPIN_COUNT @Default:# -1
20220327 015824.557 INFO             PET2 index= 131                         MPIR_CVAR_INTEL_SHM_RECV_SPIN_COUNT : Control the spin count value for the shared memory transport for receiving messages. @Alias:# I_MPI_SHM_RECV_SPIN_COUNT @Default:# -1
20220327 015824.557 INFO             PET2 index= 132                          MPIR_CVAR_INTEL_SHM_FILE_PREFIX_4K : Mount point of 4K page size shm files. @Alias:# I_MPI_SHM_FILE_PREFIX_4K @Default:# ""
20220327 015824.557 INFO             PET2 index= 133                          MPIR_CVAR_INTEL_SHM_FILE_PREFIX_2M : Mount point of 2M huge page size shm files. @Alias:# I_MPI_SHM_FILE_PREFIX_2M @Default:# ""
20220327 015824.557 INFO             PET2 index= 134                          MPIR_CVAR_INTEL_SHM_FILE_PREFIX_1G : Mount point of 1G huge page size shm files. @Alias:# I_MPI_SHM_FILE_PREFIX_1G @Default:# ""
20220327 015824.557 INFO             PET2 index= 135                                   MPIR_CVAR_INTEL_SHM_PAUSE : The number of pauses repeated. @Alias:# I_MPI_SHM_PAUSE
20220327 015824.557 INFO             PET2 index= 136                         MPIR_CVAR_INTEL_SHM_EAGER_THRESHOLD : Eager threshold. @Alias:# I_MPI_SHM_EAGER_THRESHOLD
20220327 015824.557 INFO             PET2 index= 137                               MPIR_CVAR_INTEL_SHM_RING_SIZE : @Alias:# I_MPI_SHM_RING_SIZE
20220327 015824.557 INFO             PET2 index= 138                      MPIR_CVAR_INTEL_SHM_RING_ACK_THRESHOLD : @Alias:# I_MPI_SHM_RING_ACK_THRESHOLD
20220327 015824.557 INFO             PET2 index= 139                          MPIR_CVAR_INTEL_SHM_CELL_FWD_FIRST : @Alias:# I_MPI_SHM_CELL_FWD_FIRST
20220327 015824.557 INFO             PET2 index= 140                      MPIR_CVAR_INTEL_SHM_PROFILER_DIRECTORY : @Alias:# I_MPI_SHM_PROFILER_DIRECTORY
20220327 015824.557 INFO             PET2 index= 141                         MPIR_CVAR_INTEL_SHM_TRACE_DIRECTORY : @Alias:# I_MPI_SHM_TRACE_DIRECTORY
20220327 015824.557 INFO             PET2 index= 142                              MPIR_CVAR_INTEL_SHM_FRAME_SIZE : @Alias:# I_MPI_SHM_FRAME_SIZE
20220327 015824.557 INFO             PET2 index= 143                         MPIR_CVAR_INTEL_SHM_FRAME_THRESHOLD : @Alias:# I_MPI_SHM_FRAME_THRESHOLD
20220327 015824.557 INFO             PET2 index= 144                        MPIR_CVAR_INTEL_SHM_SEND_TINY_MEMCPY : @Alias:# I_MPI_SHM_SEND_TINY_MEMCPY
20220327 015824.557 INFO             PET2 index= 145                  MPIR_CVAR_INTEL_SHM_SEND_INTRA_RING_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_RING_MEMCPY
20220327 015824.557 INFO             PET2 index= 146                  MPIR_CVAR_INTEL_SHM_SEND_INTER_RING_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_RING_MEMCPY
20220327 015824.557 INFO             PET2 index= 147                  MPIR_CVAR_INTEL_SHM_RECV_INTRA_RING_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_RING_MEMCPY
20220327 015824.557 INFO             PET2 index= 148                  MPIR_CVAR_INTEL_SHM_RECV_INTER_RING_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_RING_MEMCPY
20220327 015824.557 INFO             PET2 index= 149              MPIR_CVAR_INTEL_SHM_SEND_INTRA_CELL_FWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_CELL_FWD_MEMCPY
20220327 015824.557 INFO             PET2 index= 150              MPIR_CVAR_INTEL_SHM_SEND_INTER_CELL_FWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_CELL_FWD_MEMCPY
20220327 015824.557 INFO             PET2 index= 151              MPIR_CVAR_INTEL_SHM_SEND_INTRA_CELL_BWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_CELL_BWD_MEMCPY
20220327 015824.557 INFO             PET2 index= 152              MPIR_CVAR_INTEL_SHM_SEND_INTER_CELL_BWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_CELL_BWD_MEMCPY
20220327 015824.557 INFO             PET2 index= 153                  MPIR_CVAR_INTEL_SHM_RECV_INTRA_CELL_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_CELL_MEMCPY
20220327 015824.557 INFO             PET2 index= 154                  MPIR_CVAR_INTEL_SHM_RECV_INTER_CELL_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_CELL_MEMCPY
20220327 015824.557 INFO             PET2 index= 155          MPIR_CVAR_INTEL_SHM_RECV_INTRA_CELL_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_CELL_REGULAR_MEMCPY
20220327 015824.557 INFO             PET2 index= 156          MPIR_CVAR_INTEL_SHM_RECV_INTER_CELL_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_CELL_REGULAR_MEMCPY
20220327 015824.557 INFO             PET2 index= 157             MPIR_CVAR_INTEL_SHM_SEND_INTRA_FRAME_FWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_FRAME_FWD_MEMCPY
20220327 015824.557 INFO             PET2 index= 158             MPIR_CVAR_INTEL_SHM_SEND_INTER_FRAME_FWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_FRAME_FWD_MEMCPY
20220327 015824.557 INFO             PET2 index= 159             MPIR_CVAR_INTEL_SHM_SEND_INTRA_FRAME_BWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_FRAME_BWD_MEMCPY
20220327 015824.557 INFO             PET2 index= 160             MPIR_CVAR_INTEL_SHM_SEND_INTER_FRAME_BWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_FRAME_BWD_MEMCPY
20220327 015824.557 INFO             PET2 index= 161                 MPIR_CVAR_INTEL_SHM_RECV_INTRA_FRAME_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_FRAME_MEMCPY
20220327 015824.557 INFO             PET2 index= 162                 MPIR_CVAR_INTEL_SHM_RECV_INTER_FRAME_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_FRAME_MEMCPY
20220327 015824.557 INFO             PET2 index= 163     MPIR_CVAR_INTEL_SHM_SEND_INTRA_FRAME_FWD_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_FRAME_FWD_REGULAR_MEMCPY
20220327 015824.557 INFO             PET2 index= 164     MPIR_CVAR_INTEL_SHM_SEND_INTER_FRAME_FWD_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_FRAME_FWD_REGULAR_MEMCPY
20220327 015824.557 INFO             PET2 index= 165     MPIR_CVAR_INTEL_SHM_SEND_INTRA_FRAME_BWD_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_FRAME_BWD_REGULAR_MEMCPY
20220327 015824.557 INFO             PET2 index= 166     MPIR_CVAR_INTEL_SHM_SEND_INTER_FRAME_BWD_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_FRAME_BWD_REGULAR_MEMCPY
20220327 015824.557 INFO             PET2 index= 167         MPIR_CVAR_INTEL_SHM_RECV_INTRA_FRAME_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_FRAME_REGULAR_MEMCPY
20220327 015824.557 INFO             PET2 index= 168         MPIR_CVAR_INTEL_SHM_RECV_INTER_FRAME_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_FRAME_REGULAR_MEMCPY
20220327 015824.557 INFO             PET2 index= 169                       MPIR_CVAR_INTEL_SHM_INPLACE_THRESHOLD : @Alias:# I_MPI_SHM_INPLACE_THRESHOLD
20220327 015824.557 INFO             PET2 index= 170              MPIR_CVAR_INTEL_SHM_SEND_TINY_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_TINY_MEMCPY_THRESHOLD
20220327 015824.557 INFO             PET2 index= 171      MPIR_CVAR_INTEL_SHM_RECV_CELL_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_RECV_CELL_REGULAR_MEMCPY_THRESHOLD
20220327 015824.557 INFO             PET2 index= 172MPIR_CVAR_INTEL_SHM_SEND_INTER_UNIDIR_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_INTER_UNIDIR_REGULAR_MEMCPY_THRESHOLD
20220327 015824.557 INFO             PET2 index= 173MPIR_CVAR_INTEL_SHM_SEND_INTER_BIDIR_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_INTER_BIDIR_REGULAR_MEMCPY_THRESHOLD
20220327 015824.557 INFO             PET2 index= 174MPIR_CVAR_INTEL_SHM_SEND_INTRA_UNIDIR_REGULAR_MEMCPY_THRESHOLD_MIN : @Alias:# I_MPI_SHM_SEND_INTRA_UNIDIR_REGULAR_MEMCPY_THRESHOLD_MIN
20220327 015824.557 INFO             PET2 index= 175MPIR_CVAR_INTEL_SHM_SEND_INTRA_UNIDIR_REGULAR_MEMCPY_THRESHOLD_MAX : @Alias:# I_MPI_SHM_SEND_INTRA_UNIDIR_REGULAR_MEMCPY_THRESHOLD_MAX
20220327 015824.557 INFO             PET2 index= 176MPIR_CVAR_INTEL_SHM_SEND_INTRA_BIDIR_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_INTRA_BIDIR_REGULAR_MEMCPY_THRESHOLD
20220327 015824.557 INFO             PET2 index= 177     MPIR_CVAR_INTEL_SHM_RECV_FRAME_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_RECV_FRAME_REGULAR_MEMCPY_THRESHOLD
20220327 015824.557 INFO             PET2 index= 178          MPIR_CVAR_INTEL_SHM_CELL_FWD_UNIDIR_INTRA_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_UNIDIR_INTRA_CAPACITY
20220327 015824.557 INFO             PET2 index= 179  MPIR_CVAR_INTEL_SHM_CELL_FWD_UNIDIR_INTER_REGULAR_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_UNIDIR_INTER_REGULAR_CAPACITY
20220327 015824.557 INFO             PET2 index= 180MPIR_CVAR_INTEL_SHM_CELL_FWD_UNIDIR_INTER_NONTEMPORAL_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_UNIDIR_INTER_NONTEMPORAL_CAPACITY
20220327 015824.557 INFO             PET2 index= 181           MPIR_CVAR_INTEL_SHM_CELL_FWD_BIDIR_INTRA_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_BIDIR_INTRA_CAPACITY
20220327 015824.557 INFO             PET2 index= 182   MPIR_CVAR_INTEL_SHM_CELL_FWD_BIDIR_INTER_REGULAR_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_BIDIR_INTER_REGULAR_CAPACITY
20220327 015824.557 INFO             PET2 index= 183MPIR_CVAR_INTEL_SHM_CELL_FWD_BIDIR_INTER_NONTEMPORAL_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_BIDIR_INTER_NONTEMPORAL_CAPACITY
20220327 015824.558 INFO             PET2 index= 184MPIR_CVAR_INTEL_SHM_SEND_FWD_FRAME_NONTEMPORAL_MEMCPY_THRESHOLD_MIN : @Alias:# I_MPI_SHM_SEND_FWD_FRAME_NONTEMPORAL_MEMCPY_THRESHOLD_MIN
20220327 015824.558 INFO             PET2 index= 185MPIR_CVAR_INTEL_SHM_SEND_FWD_FRAME_NONTEMPORAL_MEMCPY_THRESHOLD_MAX : @Alias:# I_MPI_SHM_SEND_FWD_FRAME_NONTEMPORAL_MEMCPY_THRESHOLD_MAX
20220327 015824.558 INFO             PET2 index= 186MPIR_CVAR_INTEL_SHM_SEND_FWD_CELL_NONTEMPORAL_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_FWD_CELL_NONTEMPORAL_MEMCPY_THRESHOLD
20220327 015824.558 INFO             PET2 index= 187                                MPIR_CVAR_INTEL_SHM_HEAP_THP : @Alias:# I_MPI_SHM_HEAP_THP
20220327 015824.558 INFO             PET2 index= 188                                     MPIR_CVAR_INTEL_SHM_THP : @Alias:# I_MPI_SHM_THP
20220327 015824.558 INFO             PET2 index= 189                                  MPIR_CVAR_OFI_USE_PROVIDER : Define the name of the OFI provider to load.$ Syntax$ I_MPI_OFI_PROVIDER=<name>$ Arguments$ <name> - The name of the OFI provider to load @Alias:# I_MPI_OFI_PROVIDER @Default:# not defined
20220327 015824.558 INFO             PET2 index= 190                                MPIR_CVAR_OFI_DUMP_PROVIDERS : Control the capability of printing information about all OFI providers$ and their attributes from an OFI library.$ Syntax$ I_MPI_OFI_PROVIDER_DUMP=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > yes | on | 1 - Print the list of all OFI providers and their$ attributes from an OFI library$ > no | off | 0 - No action$ ----------------------------------------------------------------------- @Alias:# I_MPI_OFI_PROVIDER_DUMP @Default:# off
20220327 015824.558 INFO             PET2 index= 191                        MPIR_CVAR_CH4_OFI_ENABLE_DIRECT_RECV : Control the capability of the direct receive in the OFI fabric.$ Syntax$ I_MPI_OFI_DRECV=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > 1 - Enable direct receive$ > 0 - Disable direct receive$ ----------------------------------------------------------------------- @Alias:# I_MPI_OFI_DRECV @Default:# not defined
20220327 015824.558 INFO             PET2 index= 192                                  MPIR_CVAR_OFI_MAX_MSG_SIZE : @Alias:# I_MPI_OFI_MAX_MSG_SIZE
20220327 015824.558 INFO             PET2 index= 193                                  MPIR_CVAR_OFI_LMT_WIN_SIZE : @Alias:# I_MPI_OFI_LMT_WIN_SIZE
20220327 015824.558 INFO             PET2 index= 194                    MPIR_CVAR_CH4_OFI_ISEND_INJECT_THRESHOLD : @Alias:# I_MPI_OFI_ISEND_INJECT_THRESHOLD
20220327 015824.558 INFO             PET2 index= 195                     MPIR_CVAR_CH4_OFI_ADDRESS_EXCHANGE_MODE : @Alias:# I_MPI_STARTUP_MODE
20220327 015824.558 INFO             PET2 index= 196                     MPIR_CVAR_CH4_OFI_LARGE_SCALE_THRESHOLD : @Alias:# I_MPI_LARGE_SCALE_THRESHOLD
20220327 015824.558 INFO             PET2 index= 197                   MPIR_CVAR_CH4_OFI_EXTREME_SCALE_THRESHOLD : @Alias:# I_MPI_EXTREME_SCALE_THRESHOLD
20220327 015824.558 INFO             PET2 index= 198                        MPIR_CVAR_CH4_OFI_DYNAMIC_CONNECTION : @Alias:# I_MPI_DYNAMIC_CONNECTION
20220327 015824.558 INFO             PET2 index= 199                              MPIR_CVAR_CH4_OFI_EXPERIMENTAL : @Alias:# I_MPI_OFI_EXPERIMENTAL @Default:# 0
20220327 015824.558 INFO             PET2 index= 200                     MPIR_CVAR_CH4_OFI_CAPABILITY_SETS_DEBUG : Prints out the configuration of each capability selected via the capability sets interface.
20220327 015824.558 INFO             PET2 index= 201                               MPIR_CVAR_CH4_OFI_ENABLE_DATA : Enable immediate data fields in OFI to transmit source rank outside of the match bits
20220327 015824.558 INFO             PET2 index= 202                           MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE : If true, the OFI addressing information will be stored with an FI_AV_TABLE. If false, an FI_AV_MAP will be used.
20220327 015824.558 INFO             PET2 index= 203                 MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS : If true, use OFI scalable endpoints.
20220327 015824.558 INFO             PET2 index= 204                             MPIR_CVAR_CH4_OFI_MAX_ENDPOINTS : Specifies the maximum number of OFI endpoints that can be used by the OFI provider. The default value is -1, indicating that no value is set.
20220327 015824.558 INFO             PET2 index= 205                        MPIR_CVAR_CH4_OFI_ENABLE_MR_SCALABLE : If true, MR_SCALABLE for OFI memory regions. If false, MR_BASIC for OFI memory regions.
20220327 015824.558 INFO             PET2 index= 206                             MPIR_CVAR_CH4_OFI_ENABLE_TAGGED : If true, use tagged message transmission functions in OFI.
20220327 015824.558 INFO             PET2 index= 207                                 MPIR_CVAR_CH4_OFI_ENABLE_AM : If true, enable OFI active message support.
20220327 015824.558 INFO             PET2 index= 208                                MPIR_CVAR_CH4_OFI_ENABLE_RMA : If true, enable OFI RMA support for MPI RMA operations. OFI support for basic RMA is always required to implement large messgage transfers in the active message code path.
20220327 015824.558 INFO             PET2 index= 209                            MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS : If true, enable OFI Atomics support.
20220327 015824.558 INFO             PET2 index= 210                       MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS : Specifies the maximum number of iovecs that can be used by the OFI provider for fetch_atomic operations. The default value is -1, indicating that no value is set.
20220327 015824.558 INFO             PET2 index= 211                 MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS : If true, enable MPI data auto progress.
20220327 015824.558 INFO             PET2 index= 212              MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS : If true, enable MPI control auto progress.
20220327 015824.558 INFO             PET2 index= 213                       MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK : If true, enable iovec for pt2pt.
20220327 015824.558 INFO             PET2 index= 214                                 MPIR_CVAR_CH4_OFI_RANK_BITS : Specifies the number of bits that will be used for matching the MPI rank. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20220327 015824.558 INFO             PET2 index= 215                                  MPIR_CVAR_CH4_OFI_TAG_BITS : Specifies the number of bits that will be used for matching the user tag. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20220327 015824.558 INFO             PET2 index= 216                             MPIR_CVAR_CH4_OFI_MAJOR_VERSION : Specifies the major version of the OFI library. The default is the major version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20220327 015824.558 INFO             PET2 index= 217                             MPIR_CVAR_CH4_OFI_MINOR_VERSION : Specifies the major version of the OFI library. The default is the minor version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20220327 015824.558 INFO             PET2 index= 218                             MPIR_CVAR_CH4_OFI_ZERO_OP_FLAGS : Zeroes rx_attr.op_flags and tx_attr.op_flags, disables use of FI_SELECTIVE_COMPLETION. Can give more optimized behavior of underlying provider.
20220327 015824.558 INFO             PET2 index= 219                            MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS : Specifies the number of buffers for receiving active messages.
20220327 015824.558 INFO             PET2 index= 220                        MPIR_CVAR_INTEL_COLL_DIRECT_PROGRESS : @Alias:# I_MPI_COLL_DIRECT_PROGRESS
20220327 015824.558 INFO             PET2 index= 221                                      MPIR_CVAR_ENABLE_HCOLL : @Alias:# I_MPI_COLL_EXTERNAL
20220327 015824.558 INFO             PET2 index= 222                                     MPIR_CVAR_USE_ALLGATHER : Control selection of MPI_Allgather algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-6 @Alias:# I_MPI_ADJUST_ALLGATHER @Default:# -1
20220327 015824.558 INFO             PET2 index= 223                                MPIR_CVAR_USE_ALLGATHER_LIST : @Alias:# I_MPI_ADJUST_ALLGATHER_LIST @Default:# -1
20220327 015824.558 INFO             PET2 index= 224                         MPIR_CVAR_USE_ALLGATHER_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLGATHER_COMPOSITION @Default:# -1
20220327 015824.558 INFO             PET2 index= 225               MPIR_CVAR_ALLGATHER_COMPOSITION_DELTA_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLGATHER_COMPOSITION_DELTA_SEGSIZE @Default:# -1
20220327 015824.558 INFO             PET2 index= 226                             MPIR_CVAR_USE_ALLGATHER_NETWORK : range: 0-5 @Alias:# I_MPI_ADJUST_ALLGATHER_NETWORK @Default:# -1
20220327 015824.558 INFO             PET2 index= 227                                MPIR_CVAR_USE_ALLGATHER_NODE : range: 0-4 @Alias:# I_MPI_ADJUST_ALLGATHER_NODE @Default:# -1
20220327 015824.558 INFO             PET2 index= 228                                    MPIR_CVAR_USE_ALLGATHERV : Control selection of MPI_Allgatherv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_ALLGATHERV @Default:# -1
20220327 015824.558 INFO             PET2 index= 229                               MPIR_CVAR_USE_ALLGATHERV_LIST : @Alias:# I_MPI_ADJUST_ALLGATHERV_LIST @Default:# -1
20220327 015824.558 INFO             PET2 index= 230                        MPIR_CVAR_USE_ALLGATHERV_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLGATHERV_COMPOSITION @Default:# -1
20220327 015824.558 INFO             PET2 index= 231                            MPIR_CVAR_USE_ALLGATHERV_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_ALLGATHERV_NETWORK @Default:# -1
20220327 015824.558 INFO             PET2 index= 232                               MPIR_CVAR_USE_ALLGATHERV_NODE : range: 0-3 @Alias:# I_MPI_ADJUST_ALLGATHERV_NODE @Default:# -1
20220327 015824.558 INFO             PET2 index= 233                   MPIR_CVAR_ALLGATHER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ALLGATHER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET2 index= 234                      MPIR_CVAR_ALLGATHER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ALLGATHER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET2 index= 235                    MPIR_CVAR_SCATTERV_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_SCATTERV_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET2 index= 236                       MPIR_CVAR_SCATTERV_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_SCATTERV_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET2 index= 237                     MPIR_CVAR_SCATTER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_SCATTER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET2 index= 238                        MPIR_CVAR_SCATTER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_SCATTER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET2 index= 239                                     MPIR_CVAR_USE_ALLREDUCE : Control selection of MPI_Allreduce algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-25 @Alias:# I_MPI_ADJUST_ALLREDUCE @Default:# -1
20220327 015824.558 INFO             PET2 index= 240                                MPIR_CVAR_USE_ALLREDUCE_LIST : @Alias:# I_MPI_ADJUST_ALLREDUCE_LIST @Default:# -1
20220327 015824.558 INFO             PET2 index= 241                         MPIR_CVAR_USE_ALLREDUCE_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLREDUCE_COMPOSITION @Default:# -1
20220327 015824.558 INFO             PET2 index= 242                             MPIR_CVAR_USE_ALLREDUCE_NETWORK : range: 0-16 @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK @Default:# -1
20220327 015824.558 INFO             PET2 index= 243                                MPIR_CVAR_USE_ALLREDUCE_NODE : range: 0-9 @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE @Default:# -1
20220327 015824.558 INFO             PET2 index= 244               MPIR_CVAR_ALLREDUCE_NETWORK_MULTIPLYING_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_MULTIPLYING_RADIX @Default:# -1
20220327 015824.558 INFO             PET2 index= 245                  MPIR_CVAR_ALLREDUCE_NODE_MULTIPLYING_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_MULTIPLYING_RADIX @Default:# -1
20220327 015824.558 INFO             PET2 index= 246                   MPIR_CVAR_ALLREDUCE_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET2 index= 247                      MPIR_CVAR_ALLREDUCE_NETWORK_KARY_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_KARY_RADIX @Default:# -1
20220327 015824.558 INFO             PET2 index= 248                      MPIR_CVAR_ALLREDUCE_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET2 index= 249                         MPIR_CVAR_ALLREDUCE_NODE_KARY_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_KARY_RADIX @Default:# -1
20220327 015824.558 INFO             PET2 index= 250                    MPIR_CVAR_ALLREDUCE_NODE_SHM_GATHER_TYPE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHM_GATHER_TYPE @Default:# -1
20220327 015824.558 INFO             PET2 index= 251                   MPIR_CVAR_ALLREDUCE_NODE_SHM_GATHER_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHM_GATHER_RADIX @Default:# -1
20220327 015824.558 INFO             PET2 index= 252                   MPIR_CVAR_ALLREDUCE_NODE_SHM_RELEASE_TYPE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHM_RELEASE_TYPE @Default:# -1
20220327 015824.558 INFO             PET2 index= 253                  MPIR_CVAR_ALLREDUCE_NODE_SHM_RELEASE_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHM_RELEASE_RADIX @Default:# -1
20220327 015824.558 INFO             PET2 index= 254                MPIR_CVAR_ALLREDUCE_NETWORK_SHUMILIN_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_SHUMILIN_SEGSIZE @Default:# -1
20220327 015824.558 INFO             PET2 index= 255                   MPIR_CVAR_ALLREDUCE_NODE_SHUMILIN_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHUMILIN_SEGSIZE @Default:# -1
20220327 015824.558 INFO             PET2 index= 256                              MPIR_CVAR_ALLREDUCE_ZETA_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_ZETA_RADIX @Default:# -1
20220327 015824.558 INFO             PET2 index= 257                           MPIR_CVAR_ALLREDUCE_ZETA_SHM_TYPE : @Alias:# I_MPI_ADJUST_ALLREDUCE_ZETA_SHM_TYPE @Default:# -1
20220327 015824.558 INFO             PET2 index= 258                              MPIR_CVAR_ALLREDUCE_IOTA_NLEAD : @Alias:# I_MPI_ADJUST_ALLREDUCE_IOTA_NLEAD @Default:# -1
20220327 015824.558 INFO             PET2 index= 259               MPIR_CVAR_ALLREDUCE_NODE_NREDUCE_NB_ALLGATHER : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_NREDUCE_NB_ALLGATHER @Default:# -1
20220327 015824.558 INFO             PET2 index= 260            MPIR_CVAR_ALLREDUCE_NETWORK_NREDUCE_NB_ALLGATHER : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_NREDUCE_NB_ALLGATHER @Default:# -1
20220327 015824.558 INFO             PET2 index= 261                MPIR_CVAR_ALLREDUCE_NODE_NREDUCE_NB_ALLTOALL : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_NREDUCE_NB_ALLTOALL @Default:# -1
20220327 015824.558 INFO             PET2 index= 262             MPIR_CVAR_ALLREDUCE_NETWORK_NREDUCE_NB_ALLTOALL : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_NREDUCE_NB_ALLTOALL @Default:# -1
20220327 015824.558 INFO             PET2 index= 263                    MPIR_CVAR_ALLREDUCE_NODE_NREDUCE_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_NREDUCE_SEGSIZE @Default:# -1
20220327 015824.558 INFO             PET2 index= 264                 MPIR_CVAR_ALLREDUCE_NETWORK_NREDUCE_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_NREDUCE_SEGSIZE @Default:# -1
20220327 015824.558 INFO             PET2 index= 265                                      MPIR_CVAR_USE_ALLTOALL : Control selection of MPI_Alltoall algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-13 @Alias:# I_MPI_ADJUST_ALLTOALL @Default:# -1
20220327 015824.558 INFO             PET2 index= 266                                 MPIR_CVAR_USE_ALLTOALL_LIST : @Alias:# I_MPI_ADJUST_ALLTOALL_LIST @Default:# -1
20220327 015824.558 INFO             PET2 index= 267                          MPIR_CVAR_USE_ALLTOALL_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLTOALL_COMPOSITION @Default:# -1
20220327 015824.558 INFO             PET2 index= 268                              MPIR_CVAR_USE_ALLTOALL_NETWORK : range: 0-7 @Alias:# I_MPI_ADJUST_ALLTOALL_NETWORK @Default:# -1
20220327 015824.558 INFO             PET2 index= 269                                 MPIR_CVAR_USE_ALLTOALL_NODE : range: 0-6 @Alias:# I_MPI_ADJUST_ALLTOALL_NODE @Default:# -1
20220327 015824.558 INFO             PET2 index= 270                MPIR_CVAR_ALLTOALL_COMPOSITION_GAMMA_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLTOALL_COMPOSITION_GAMMA_SEGSIZE @Default:# -1
20220327 015824.558 INFO             PET2 index= 271                                 MPIR_CVAR_ALLTOALL_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALL_SCATTERED_THROTTLE @Default:# -1
20220327 015824.558 INFO             PET2 index= 272               MPIR_CVAR_ALLTOALL_NETWORK_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALL_NETWORK_SCATTERED_THROTTLE @Default:# -1
20220327 015824.558 INFO             PET2 index= 273                  MPIR_CVAR_ALLTOALL_NODE_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALL_NODE_SCATTERED_THROTTLE @Default:# -1
20220327 015824.558 INFO             PET2 index= 274                             MPIR_CVAR_ALLTOALL_BRUCKS_RADIX : @Alias:# I_MPI_ADJUST_ALLTOALL_BRUCKS_RADIX @Default:# -1
20220327 015824.558 INFO             PET2 index= 275                     MPIR_CVAR_ALLTOALL_NETWORK_BRUCKS_RADIX : @Alias:# I_MPI_ADJUST_ALLTOALL_NETWORK_BRUCKS_RADIX @Default:# -1
20220327 015824.558 INFO             PET2 index= 276                        MPIR_CVAR_ALLTOALL_NODE_BRUCKS_RADIX : @Alias:# I_MPI_ADJUST_ALLTOALL_NODE_BRUCKS_RADIX @Default:# -1
20220327 015824.558 INFO             PET2 index= 277                                     MPIR_CVAR_USE_ALLTOALLV : Control selection of MPI_Alltoallv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-9 @Alias:# I_MPI_ADJUST_ALLTOALLV @Default:# -1
20220327 015824.558 INFO             PET2 index= 278                                MPIR_CVAR_USE_ALLTOALLV_LIST : @Alias:# I_MPI_ADJUST_ALLTOALLV_LIST @Default:# -1
20220327 015824.558 INFO             PET2 index= 279                         MPIR_CVAR_USE_ALLTOALLV_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLTOALLV_COMPOSITION @Default:# -1
20220327 015824.558 INFO             PET2 index= 280                             MPIR_CVAR_USE_ALLTOALLV_NETWORK : range: 0-6 @Alias:# I_MPI_ADJUST_ALLTOALLV_NETWORK @Default:# -1
20220327 015824.558 INFO             PET2 index= 281                                MPIR_CVAR_USE_ALLTOALLV_NODE : range: 0-5 @Alias:# I_MPI_ADJUST_ALLTOALLV_NODE @Default:# -1
20220327 015824.558 INFO             PET2 index= 282                                MPIR_CVAR_ALLTOALLV_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLV_SCATTERED_THROTTLE @Default:# -1
20220327 015824.558 INFO             PET2 index= 283              MPIR_CVAR_ALLTOALLV_NETWORK_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLV_NETWORK_SCATTERED_THROTTLE @Default:# -1
20220327 015824.558 INFO             PET2 index= 284                 MPIR_CVAR_ALLTOALLV_NODE_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLV_NODE_SCATTERED_THROTTLE @Default:# -1
20220327 015824.558 INFO             PET2 index= 285                                     MPIR_CVAR_USE_ALLTOALLW : Control selection of MPI_Alltoallw algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-6 @Alias:# I_MPI_ADJUST_ALLTOALLW @Default:# -1
20220327 015824.558 INFO             PET2 index= 286                                MPIR_CVAR_USE_ALLTOALLW_LIST : @Alias:# I_MPI_ADJUST_ALLTOALLW_LIST @Default:# -1
20220327 015824.559 INFO             PET2 index= 287                         MPIR_CVAR_USE_ALLTOALLW_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLTOALLW_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET2 index= 288                             MPIR_CVAR_USE_ALLTOALLW_NETWORK : @Alias:# I_MPI_ADJUST_ALLTOALLW_NETWORK @Default:# -1
20220327 015824.559 INFO             PET2 index= 289                                MPIR_CVAR_USE_ALLTOALLW_NODE : @Alias:# I_MPI_ADJUST_ALLTOALLW_NODE @Default:# -1
20220327 015824.559 INFO             PET2 index= 290                                MPIR_CVAR_ALLTOALLW_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLW_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET2 index= 291              MPIR_CVAR_ALLTOALLW_NETWORK_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLW_NETWORK_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET2 index= 292                 MPIR_CVAR_ALLTOALLW_NODE_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLW_NODE_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET2 index= 293                                       MPIR_CVAR_USE_BARRIER : Control selection of MPI_Barrier algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-11 @Alias:# I_MPI_ADJUST_BARRIER @Default:# -1
20220327 015824.559 INFO             PET2 index= 294                                  MPIR_CVAR_USE_BARRIER_LIST : @Alias:# I_MPI_ADJUST_BARRIER_LIST @Default:# -1
20220327 015824.559 INFO             PET2 index= 295                           MPIR_CVAR_USE_BARRIER_COMPOSITION : @Alias:# I_MPI_ADJUST_BARRIER_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET2 index= 296                               MPIR_CVAR_USE_BARRIER_NETWORK : range: 0-6 @Alias:# I_MPI_ADJUST_BARRIER_NETWORK @Default:# -1
20220327 015824.559 INFO             PET2 index= 297                                  MPIR_CVAR_USE_BARRIER_NODE : range: 0-4 @Alias:# I_MPI_ADJUST_BARRIER_NODE @Default:# -1
20220327 015824.559 INFO             PET2 index= 298                 MPIR_CVAR_BARRIER_NETWORK_MULTIPLYING_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NETWORK_MULTIPLYING_RADIX @Default:# -1
20220327 015824.559 INFO             PET2 index= 299                     MPIR_CVAR_BARRIER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET2 index= 300                        MPIR_CVAR_BARRIER_NETWORK_KARY_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NETWORK_KARY_RADIX @Default:# -1
20220327 015824.559 INFO             PET2 index= 301          MPIR_CVAR_BARRIER_NETWORK_RECURSIVE_EXCHANGE_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NETWORK_RECURSIVE_EXCHANGE_RADIX @Default:# -1
20220327 015824.559 INFO             PET2 index= 302                        MPIR_CVAR_BARRIER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET2 index= 303                           MPIR_CVAR_BARRIER_NODE_KARY_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_KARY_RADIX @Default:# -1
20220327 015824.559 INFO             PET2 index= 304             MPIR_CVAR_BARRIER_NODE_RECURSIVE_EXCHANGE_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_RECURSIVE_EXCHANGE_RADIX @Default:# -1
20220327 015824.559 INFO             PET2 index= 305                      MPIR_CVAR_BARRIER_NODE_SHM_GATHER_TYPE : @Alias:# I_MPI_ADJUST_BARRIER_NODE_SHM_GATHER_TYPE @Default:# -1
20220327 015824.559 INFO             PET2 index= 306                     MPIR_CVAR_BARRIER_NODE_SHM_GATHER_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_SHM_GATHER_RADIX @Default:# -1
20220327 015824.559 INFO             PET2 index= 307                     MPIR_CVAR_BARRIER_NODE_SHM_RELEASE_TYPE : @Alias:# I_MPI_ADJUST_BARRIER_NODE_SHM_RELEASE_TYPE @Default:# -1
20220327 015824.559 INFO             PET2 index= 308                    MPIR_CVAR_BARRIER_NODE_SHM_RELEASE_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_SHM_RELEASE_RADIX @Default:# -1
20220327 015824.559 INFO             PET2 index= 309                                MPIR_CVAR_BARRIER_ZETA_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_ZETA_RADIX @Default:# -1
20220327 015824.559 INFO             PET2 index= 310                             MPIR_CVAR_BARRIER_ZETA_SHM_TYPE : @Alias:# I_MPI_ADJUST_BARRIER_ZETA_SHM_TYPE @Default:# -1
20220327 015824.559 INFO             PET2 index= 311                                         MPIR_CVAR_USE_BCAST : Control selection of MPI_Bcast algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-18 @Alias:# I_MPI_ADJUST_BCAST @Default:# -1
20220327 015824.559 INFO             PET2 index= 312                                    MPIR_CVAR_USE_BCAST_LIST : @Alias:# I_MPI_ADJUST_BCAST_LIST @Default:# -1
20220327 015824.559 INFO             PET2 index= 313                             MPIR_CVAR_USE_BCAST_COMPOSITION : @Alias:# I_MPI_ADJUST_BCAST_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET2 index= 314                                 MPIR_CVAR_USE_BCAST_NETWORK : range: 0-13 @Alias:# I_MPI_ADJUST_BCAST_NETWORK @Default:# -1
20220327 015824.559 INFO             PET2 index= 315                                    MPIR_CVAR_USE_BCAST_NODE : range: 0-9 @Alias:# I_MPI_ADJUST_BCAST_NODE @Default:# -1
20220327 015824.559 INFO             PET2 index= 316                               MPIR_CVAR_BCAST_EPSILON_NRAIL : @Alias:# I_MPI_ADJUST_BCAST_EPSILON_NRAIL @Default:# -1
20220327 015824.559 INFO             PET2 index= 317                          MPIR_CVAR_BCAST_NETWORK_KARY_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_KARY_RADIX @Default:# -1
20220327 015824.559 INFO             PET2 index= 318                       MPIR_CVAR_BCAST_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET2 index= 319                        MPIR_CVAR_BCAST_NETWORK_KARY_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_KARY_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET2 index= 320                     MPIR_CVAR_BCAST_NETWORK_KNOMIAL_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_KNOMIAL_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET2 index= 321                             MPIR_CVAR_BCAST_NODE_KARY_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NODE_KARY_RADIX @Default:# -1
20220327 015824.559 INFO             PET2 index= 322                          MPIR_CVAR_BCAST_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET2 index= 323                           MPIR_CVAR_BCAST_NODE_KARY_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_KARY_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET2 index= 324                        MPIR_CVAR_BCAST_NODE_KNOMIAL_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_KNOMIAL_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET2 index= 325                          MPIR_CVAR_BCAST_NETWORK_TREE_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_TREE_RADIX @Default:# -1
20220327 015824.559 INFO             PET2 index= 326                        MPIR_CVAR_BCAST_NETWORK_TREE_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_TREE_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET2 index= 327                           MPIR_CVAR_BCAST_NETWORK_TREE_TYPE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_TREE_TYPE @Default:# -1
20220327 015824.559 INFO             PET2 index= 328                       MPIR_CVAR_BCAST_NETWORK_TREE_THROTTLE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_TREE_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET2 index= 329                       MPIR_CVAR_BCAST_NODE_SHUMILIN_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHUMILIN_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET2 index= 330                        MPIR_CVAR_BCAST_NODE_SHM_GATHER_TYPE : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHM_GATHER_TYPE @Default:# -1
20220327 015824.559 INFO             PET2 index= 331                       MPIR_CVAR_BCAST_NODE_SHM_GATHER_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHM_GATHER_RADIX @Default:# -1
20220327 015824.559 INFO             PET2 index= 332                       MPIR_CVAR_BCAST_NODE_SHM_RELEASE_TYPE : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHM_RELEASE_TYPE @Default:# -1
20220327 015824.559 INFO             PET2 index= 333                      MPIR_CVAR_BCAST_NODE_SHM_RELEASE_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHM_RELEASE_RADIX @Default:# -1
20220327 015824.559 INFO             PET2 index= 334                    MPIR_CVAR_BCAST_NETWORK_SHUMILIN_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_SHUMILIN_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET2 index= 335                 MPIR_CVAR_BCAST_NODE_NUMA_AWARE_MEMCPY_ARCH : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_MEMCPY_ARCH @Default:# -1
20220327 015824.559 INFO             PET2 index= 336                   MPIR_CVAR_BCAST_NODE_NUMA_AWARE_FRAME_NUM : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_FRAME_NUM
20220327 015824.559 INFO             PET2 index= 337                  MPIR_CVAR_BCAST_NODE_NUMA_AWARE_FRAME_SIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_FRAME_SIZE
20220327 015824.559 INFO             PET2 index= 338  MPIR_CVAR_BCAST_NODE_NUMA_AWARE_RECV_NONTEMPORAL_THRESHOLD : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_NONTEMPORAL_THRESHOLD
20220327 015824.559 INFO             PET2 index= 339      MPIR_CVAR_BCAST_NODE_NUMA_AWARE_FRAME_NONTEMPORAL_SIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_FRAME_NONTEMPORAL_SIZE
20220327 015824.559 INFO             PET2 index= 340 MPIR_CVAR_BCAST_NODE_NUMA_AWARE_TINY_MESSAGE_SIZE_THRESHOLD : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_TINY_MESSAGE_SIZE_THRESHOLD
20220327 015824.559 INFO             PET2 index= 341MPIR_CVAR_BCAST_NODE_NUMA_AWARE_SHM_HEAP_MESSAGE_SIZE_THRESHOLD : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_SHM_HEAP_MESSAGE_SIZE_THRESHOLD
20220327 015824.559 INFO             PET2 index= 342                                        MPIR_CVAR_USE_EXSCAN : Control selection of MPI_Exscan algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_EXSCAN @Default:# -1
20220327 015824.559 INFO             PET2 index= 343                                   MPIR_CVAR_USE_EXSCAN_LIST : @Alias:# I_MPI_ADJUST_EXSCAN_LIST @Default:# -1
20220327 015824.559 INFO             PET2 index= 344                            MPIR_CVAR_USE_EXSCAN_COMPOSITION : @Alias:# I_MPI_ADJUST_EXSCAN_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET2 index= 345                                MPIR_CVAR_USE_EXSCAN_NETWORK : @Alias:# I_MPI_ADJUST_EXSCAN_NETWORK @Default:# -1
20220327 015824.559 INFO             PET2 index= 346                                   MPIR_CVAR_USE_EXSCAN_NODE : @Alias:# I_MPI_ADJUST_EXSCAN_NODE @Default:# -1
20220327 015824.559 INFO             PET2 index= 347                                        MPIR_CVAR_USE_GATHER : Control selection of MPI_Gather algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_GATHER @Default:# -1
20220327 015824.559 INFO             PET2 index= 348                                   MPIR_CVAR_USE_GATHER_LIST : @Alias:# I_MPI_ADJUST_GATHER_LIST @Default:# -1
20220327 015824.559 INFO             PET2 index= 349                            MPIR_CVAR_USE_GATHER_COMPOSITION : @Alias:# I_MPI_ADJUST_GATHER_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET2 index= 350                                MPIR_CVAR_USE_GATHER_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_GATHER_NETWORK @Default:# -1
20220327 015824.559 INFO             PET2 index= 351                                   MPIR_CVAR_USE_GATHER_NODE : range: 0-3 @Alias:# I_MPI_ADJUST_GATHER_NODE @Default:# -1
20220327 015824.559 INFO             PET2 index= 352            MPIR_CVAR_GATHER_NODE_BINOMIAL_SEGMENTED_SEGSIZE : @Alias:# I_MPI_ADJUST_GATHER_NODE_BINOMIAL_SEGMENTED_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET2 index= 353         MPIR_CVAR_GATHER_NETWORK_BINOMIAL_SEGMENTED_SEGSIZE : @Alias:# I_MPI_ADJUST_GATHER_NETWORK_BINOMIAL_SEGMENTED_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET2 index= 354                                       MPIR_CVAR_USE_GATHERV : Control selection of MPI_Gatherv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-4 @Alias:# I_MPI_ADJUST_GATHERV @Default:# -1
20220327 015824.559 INFO             PET2 index= 355                                  MPIR_CVAR_USE_GATHERV_LIST : @Alias:# I_MPI_ADJUST_GATHERV_LIST @Default:# -1
20220327 015824.559 INFO             PET2 index= 356                           MPIR_CVAR_USE_GATHERV_COMPOSITION : @Alias:# I_MPI_ADJUST_GATHERV_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET2 index= 357                               MPIR_CVAR_USE_GATHERV_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_GATHERV_NETWORK @Default:# -1
20220327 015824.559 INFO             PET2 index= 358                                  MPIR_CVAR_USE_GATHERV_NODE : range: 0-2 @Alias:# I_MPI_ADJUST_GATHERV_NODE @Default:# -1
20220327 015824.559 INFO             PET2 index= 359                     MPIR_CVAR_GATHERV_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_GATHERV_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET2 index= 360                        MPIR_CVAR_GATHERV_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_GATHERV_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET2 index= 361                                MPIR_CVAR_USE_REDUCE_SCATTER : Control selection of MPI_Reduce_scatter algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-6 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER @Default:# -1
20220327 015824.559 INFO             PET2 index= 362                           MPIR_CVAR_USE_REDUCE_SCATTER_LIST : @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_LIST @Default:# -1
20220327 015824.559 INFO             PET2 index= 363                    MPIR_CVAR_USE_REDUCE_SCATTER_COMPOSITION : @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET2 index= 364                        MPIR_CVAR_USE_REDUCE_SCATTER_NETWORK : range: 0-5 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_NETWORK @Default:# -1
20220327 015824.559 INFO             PET2 index= 365                           MPIR_CVAR_USE_REDUCE_SCATTER_NODE : range: 0-4 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_NODE @Default:# -1
20220327 015824.559 INFO             PET2 index= 366                          MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK : Control selection of MPI_Reduce_scatter_block algorithm presets. Arguments$ <algid> - Algorithm identifier$ range: 0-4 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK @Default:# -1
20220327 015824.559 INFO             PET2 index= 367                     MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK_LIST : @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK_LIST @Default:# -1
20220327 015824.559 INFO             PET2 index= 368              MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK_COMPOSITION : @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET2 index= 369                  MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK_NETWORK @Default:# -1
20220327 015824.559 INFO             PET2 index= 370                     MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK_NODE : range: 0-3 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK_NODE @Default:# -1
20220327 015824.559 INFO             PET2 index= 371                                        MPIR_CVAR_USE_REDUCE : Control selection of MPI_Reduce algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-13 @Alias:# I_MPI_ADJUST_REDUCE @Default:# -1
20220327 015824.559 INFO             PET2 index= 372                                   MPIR_CVAR_USE_REDUCE_LIST : @Alias:# I_MPI_ADJUST_REDUCE_LIST @Default:# -1
20220327 015824.559 INFO             PET2 index= 373                            MPIR_CVAR_USE_REDUCE_COMPOSITION : @Alias:# I_MPI_ADJUST_REDUCE_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET2 index= 374                                MPIR_CVAR_USE_REDUCE_NETWORK : range: 0-10 @Alias:# I_MPI_ADJUST_REDUCE_NETWORK @Default:# -1
20220327 015824.559 INFO             PET2 index= 375                                   MPIR_CVAR_USE_REDUCE_NODE : range: 0-7 @Alias:# I_MPI_ADJUST_REDUCE_NODE @Default:# -1
20220327 015824.559 INFO             PET2 index= 376                         MPIR_CVAR_REDUCE_NETWORK_KARY_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KARY_RADIX @Default:# -1
20220327 015824.559 INFO             PET2 index= 377                      MPIR_CVAR_REDUCE_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET2 index= 378                      MPIR_CVAR_REDUCE_NETWORK_KARY_NBUFFERS : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KARY_NBUFFERS @Default:# -1
20220327 015824.559 INFO             PET2 index= 379                   MPIR_CVAR_REDUCE_NETWORK_KNOMIAL_NBUFFERS : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KNOMIAL_NBUFFERS @Default:# -1
20220327 015824.559 INFO             PET2 index= 380                       MPIR_CVAR_REDUCE_NETWORK_KARY_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KARY_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET2 index= 381                    MPIR_CVAR_REDUCE_NETWORK_KNOMIAL_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KNOMIAL_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET2 index= 382                       MPIR_CVAR_REDUCE_NETWORK_RING_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_RING_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET2 index= 383                            MPIR_CVAR_REDUCE_NODE_KARY_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KARY_RADIX @Default:# -1
20220327 015824.559 INFO             PET2 index= 384                         MPIR_CVAR_REDUCE_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET2 index= 385                         MPIR_CVAR_REDUCE_NODE_KARY_NBUFFERS : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KARY_NBUFFERS @Default:# -1
20220327 015824.559 INFO             PET2 index= 386                      MPIR_CVAR_REDUCE_NODE_KNOMIAL_NBUFFERS : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KNOMIAL_NBUFFERS @Default:# -1
20220327 015824.559 INFO             PET2 index= 387                          MPIR_CVAR_REDUCE_NODE_KARY_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KARY_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET2 index= 388                       MPIR_CVAR_REDUCE_NODE_KNOMIAL_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KNOMIAL_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET2 index= 389                          MPIR_CVAR_REDUCE_NODE_RING_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_RING_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET2 index= 390                       MPIR_CVAR_REDUCE_NODE_SHM_GATHER_TYPE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_SHM_GATHER_TYPE @Default:# -1
20220327 015824.559 INFO             PET2 index= 391                      MPIR_CVAR_REDUCE_NODE_SHM_GATHER_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NODE_SHM_GATHER_RADIX @Default:# -1
20220327 015824.560 INFO             PET2 index= 392                      MPIR_CVAR_REDUCE_NODE_SHM_RELEASE_TYPE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_SHM_RELEASE_TYPE @Default:# -1
20220327 015824.560 INFO             PET2 index= 393                     MPIR_CVAR_REDUCE_NODE_SHM_RELEASE_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NODE_SHM_RELEASE_RADIX @Default:# -1
20220327 015824.560 INFO             PET2 index= 394                                          MPIR_CVAR_USE_SCAN : Control selection of MPI_Scan algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_SCAN @Default:# -1
20220327 015824.560 INFO             PET2 index= 395                                     MPIR_CVAR_USE_SCAN_LIST : @Alias:# I_MPI_ADJUST_SCAN_LIST @Default:# -1
20220327 015824.560 INFO             PET2 index= 396                              MPIR_CVAR_USE_SCAN_COMPOSITION : @Alias:# I_MPI_ADJUST_SCAN_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET2 index= 397                                  MPIR_CVAR_USE_SCAN_NETWORK : @Alias:# I_MPI_ADJUST_SCAN_NETWORK @Default:# -1
20220327 015824.560 INFO             PET2 index= 398                                     MPIR_CVAR_USE_SCAN_NODE : @Alias:# I_MPI_ADJUST_SCAN_NODE @Default:# -1
20220327 015824.560 INFO             PET2 index= 399                                       MPIR_CVAR_USE_SCATTER : Control selection of MPI_Scatter algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_SCATTER @Default:# -1
20220327 015824.560 INFO             PET2 index= 400                                  MPIR_CVAR_USE_SCATTER_LIST : @Alias:# I_MPI_ADJUST_SCATTER_LIST @Default:# -1
20220327 015824.560 INFO             PET2 index= 401                           MPIR_CVAR_USE_SCATTER_COMPOSITION : @Alias:# I_MPI_ADJUST_SCATTER_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET2 index= 402                               MPIR_CVAR_USE_SCATTER_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_SCATTER_NETWORK @Default:# -1
20220327 015824.560 INFO             PET2 index= 403                                  MPIR_CVAR_USE_SCATTER_NODE : range: 0-3 @Alias:# I_MPI_ADJUST_SCATTER_NODE @Default:# -1
20220327 015824.560 INFO             PET2 index= 404                                      MPIR_CVAR_USE_SCATTERV : Control selection of MPI_Scatterv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-4 @Alias:# I_MPI_ADJUST_SCATTERV @Default:# -1
20220327 015824.560 INFO             PET2 index= 405                                 MPIR_CVAR_USE_SCATTERV_LIST : @Alias:# I_MPI_ADJUST_SCATTERV_LIST @Default:# -1
20220327 015824.560 INFO             PET2 index= 406                          MPIR_CVAR_USE_SCATTERV_COMPOSITION : @Alias:# I_MPI_ADJUST_SCATTERV_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET2 index= 407                              MPIR_CVAR_USE_SCATTERV_NETWORK : range: 0-3 @Alias:# I_MPI_ADJUST_SCATTERV_NETWORK @Default:# -1
20220327 015824.560 INFO             PET2 index= 408                                 MPIR_CVAR_USE_SCATTERV_NODE : range: 0-2 @Alias:# I_MPI_ADJUST_SCATTERV_NODE @Default:# -1
20220327 015824.560 INFO             PET2 index= 409                                    MPIR_CVAR_USE_IALLREDUCE : Control selection of MPI_Iallreduce algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-9 @Alias:# I_MPI_ADJUST_IALLREDUCE @Default:# -1
20220327 015824.560 INFO             PET2 index= 410                               MPIR_CVAR_USE_IALLREDUCE_LIST : @Alias:# I_MPI_ADJUST_IALLREDUCE_LIST @Default:# -1
20220327 015824.560 INFO             PET2 index= 411                        MPIR_CVAR_USE_IALLREDUCE_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLREDUCE_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET2 index= 412                            MPIR_CVAR_USE_IALLREDUCE_NETWORK : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK @Default:# -1
20220327 015824.560 INFO             PET2 index= 413                               MPIR_CVAR_USE_IALLREDUCE_NODE : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE @Default:# -1
20220327 015824.560 INFO             PET2 index= 414                   MPIR_CVAR_IALLREDUCE_KNOMIAL_REDUCE_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_KNOMIAL_REDUCE_RADIX @Default:# -1
20220327 015824.560 INFO             PET2 index= 415                    MPIR_CVAR_IALLREDUCE_KNOMIAL_BCAST_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_KNOMIAL_BCAST_RADIX @Default:# -1
20220327 015824.560 INFO             PET2 index= 416           MPIR_CVAR_IALLREDUCE_NETWORK_KNOMIAL_REDUCE_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK_KNOMIAL_REDUCE_RADIX @Default:# -1
20220327 015824.560 INFO             PET2 index= 417              MPIR_CVAR_IALLREDUCE_NODE_KNOMIAL_REDUCE_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE_KNOMIAL_REDUCE_RADIX @Default:# -1
20220327 015824.560 INFO             PET2 index= 418            MPIR_CVAR_IALLREDUCE_NETWORK_KNOMIAL_BCAST_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK_KNOMIAL_BCAST_RADIX @Default:# -1
20220327 015824.560 INFO             PET2 index= 419               MPIR_CVAR_IALLREDUCE_NODE_KNOMIAL_BCAST_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE_KNOMIAL_BCAST_RADIX @Default:# -1
20220327 015824.560 INFO             PET2 index= 420                 MPIR_CVAR_IALLREDUCE_NODE_NREDUCE_DO_GATHER : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE_NREDUCE_DO_GATHER @Default:# -1
20220327 015824.560 INFO             PET2 index= 421              MPIR_CVAR_IALLREDUCE_NETWORK_NREDUCE_DO_GATHER : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK_NREDUCE_DO_GATHER @Default:# -1
20220327 015824.560 INFO             PET2 index= 422              MPIR_CVAR_IALLREDUCE_NODE_NREDUCE_NB_ALLGATHER : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE_NREDUCE_NB_ALLGATHER @Default:# -1
20220327 015824.560 INFO             PET2 index= 423           MPIR_CVAR_IALLREDUCE_NETWORK_NREDUCE_NB_ALLGATHER : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK_NREDUCE_NB_ALLGATHER @Default:# -1
20220327 015824.560 INFO             PET2 index= 424                                        MPIR_CVAR_USE_IBCAST : Control selection of MPI_Ibcast algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_IBCAST @Default:# -1
20220327 015824.560 INFO             PET2 index= 425                                   MPIR_CVAR_USE_IBCAST_LIST : @Alias:# I_MPI_ADJUST_IBCAST_LIST @Default:# -1
20220327 015824.560 INFO             PET2 index= 426                            MPIR_CVAR_USE_IBCAST_COMPOSITION : @Alias:# I_MPI_ADJUST_IBCAST_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET2 index= 427                                MPIR_CVAR_USE_IBCAST_NETWORK : @Alias:# I_MPI_ADJUST_IBCAST_NETWORK @Default:# -1
20220327 015824.560 INFO             PET2 index= 428                                   MPIR_CVAR_USE_IBCAST_NODE : @Alias:# I_MPI_ADJUST_IBCAST_NODE @Default:# -1
20220327 015824.560 INFO             PET2 index= 429                              MPIR_CVAR_IBCAST_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IBCAST_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET2 index= 430                      MPIR_CVAR_IBCAST_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IBCAST_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET2 index= 431                         MPIR_CVAR_IBCAST_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IBCAST_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET2 index= 432                                       MPIR_CVAR_USE_IREDUCE : Control selection of MPI_Ireduce algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-4 @Alias:# I_MPI_ADJUST_IREDUCE @Default:# -1
20220327 015824.560 INFO             PET2 index= 433                                  MPIR_CVAR_USE_IREDUCE_LIST : @Alias:# I_MPI_ADJUST_IREDUCE_LIST @Default:# -1
20220327 015824.560 INFO             PET2 index= 434                           MPIR_CVAR_USE_IREDUCE_COMPOSITION : @Alias:# I_MPI_ADJUST_IREDUCE_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET2 index= 435                               MPIR_CVAR_USE_IREDUCE_NETWORK : @Alias:# I_MPI_ADJUST_IREDUCE_NETWORK @Default:# -1
20220327 015824.560 INFO             PET2 index= 436                                  MPIR_CVAR_USE_IREDUCE_NODE : @Alias:# I_MPI_ADJUST_IREDUCE_NODE @Default:# -1
20220327 015824.560 INFO             PET2 index= 437                             MPIR_CVAR_IREDUCE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IREDUCE_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET2 index= 438                     MPIR_CVAR_IREDUCE_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IREDUCE_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET2 index= 439                        MPIR_CVAR_IREDUCE_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IREDUCE_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET2 index= 440                                       MPIR_CVAR_USE_IGATHER : Control selection of MPI_Igather algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-2 @Alias:# I_MPI_ADJUST_IGATHER @Default:# -1
20220327 015824.560 INFO             PET2 index= 441                                  MPIR_CVAR_USE_IGATHER_LIST : @Alias:# I_MPI_ADJUST_IGATHER_LIST @Default:# -1
20220327 015824.560 INFO             PET2 index= 442                           MPIR_CVAR_USE_IGATHER_COMPOSITION : @Alias:# I_MPI_ADJUST_IGATHER_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET2 index= 443                               MPIR_CVAR_USE_IGATHER_NETWORK : @Alias:# I_MPI_ADJUST_IGATHER_NETWORK @Default:# -1
20220327 015824.560 INFO             PET2 index= 444                                  MPIR_CVAR_USE_IGATHER_NODE : @Alias:# I_MPI_ADJUST_IGATHER_NODE @Default:# -1
20220327 015824.560 INFO             PET2 index= 445                             MPIR_CVAR_IGATHER_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IGATHER_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET2 index= 446                     MPIR_CVAR_IGATHER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IGATHER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET2 index= 447                        MPIR_CVAR_IGATHER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IGATHER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET2 index= 448                                    MPIR_CVAR_USE_IALLGATHER : Control selection of MPI_Iallgather algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_IALLGATHER @Default:# -1
20220327 015824.560 INFO             PET2 index= 449                               MPIR_CVAR_USE_IALLGATHER_LIST : @Alias:# I_MPI_ADJUST_IALLGATHER_LIST @Default:# -1
20220327 015824.560 INFO             PET2 index= 450                        MPIR_CVAR_USE_IALLGATHER_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLGATHER_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET2 index= 451                            MPIR_CVAR_USE_IALLGATHER_NETWORK : @Alias:# I_MPI_ADJUST_IALLGATHER_NETWORK @Default:# -1
20220327 015824.560 INFO             PET2 index= 452                               MPIR_CVAR_USE_IALLGATHER_NODE : @Alias:# I_MPI_ADJUST_IALLGATHER_NODE @Default:# -1
20220327 015824.560 INFO             PET2 index= 453                  MPIR_CVAR_IALLGATHER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IALLGATHER_NETWORK_KNOMIAL_RADIX
20220327 015824.560 INFO             PET2 index= 454                     MPIR_CVAR_IALLGATHER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IALLGATHER_NODE_KNOMIAL_RADIX
20220327 015824.560 INFO             PET2 index= 455                                     MPIR_CVAR_USE_IALLTOALL : Control selection of MPI_Ialltoall algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-7 @Alias:# I_MPI_ADJUST_IALLTOALL @Default:# -1
20220327 015824.560 INFO             PET2 index= 456                                MPIR_CVAR_USE_IALLTOALL_LIST : @Alias:# I_MPI_ADJUST_IALLTOALL_LIST @Default:# -1
20220327 015824.560 INFO             PET2 index= 457                         MPIR_CVAR_USE_IALLTOALL_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLTOALL_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET2 index= 458                             MPIR_CVAR_USE_IALLTOALL_NETWORK : @Alias:# I_MPI_ADJUST_IALLTOALL_NETWORK @Default:# -1
20220327 015824.560 INFO             PET2 index= 459                                MPIR_CVAR_USE_IALLTOALL_NODE : @Alias:# I_MPI_ADJUST_IALLTOALL_NODE @Default:# -1
20220327 015824.560 INFO             PET2 index= 460              MPIR_CVAR_IALLTOALL_PERMUTED_SENDRECV_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALL_PERMUTED_SENDRECV_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET2 index= 461      MPIR_CVAR_IALLTOALL_NETWORK_PERMUTED_SENDRECV_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALL_NETWORK_PERMUTED_SENDRECV_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET2 index= 462         MPIR_CVAR_IALLTOALL_NODE_PERMUTED_SENDRECV_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALL_NODE_PERMUTED_SENDRECV_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET2 index= 463                                    MPIR_CVAR_USE_IALLTOALLV : Control selection of MPI_Ialltoallv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-6 @Alias:# I_MPI_ADJUST_IALLTOALLV @Default:# -1
20220327 015824.560 INFO             PET2 index= 464                               MPIR_CVAR_USE_IALLTOALLV_LIST : @Alias:# I_MPI_ADJUST_IALLTOALLV_LIST @Default:# -1
20220327 015824.560 INFO             PET2 index= 465                        MPIR_CVAR_USE_IALLTOALLV_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLTOALLV_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET2 index= 466                            MPIR_CVAR_USE_IALLTOALLV_NETWORK : @Alias:# I_MPI_ADJUST_IALLTOALLV_NETWORK @Default:# -1
20220327 015824.560 INFO             PET2 index= 467                               MPIR_CVAR_USE_IALLTOALLV_NODE : @Alias:# I_MPI_ADJUST_IALLTOALLV_NODE @Default:# -1
20220327 015824.560 INFO             PET2 index= 468                       MPIR_CVAR_IALLTOALLV_BLOCKED_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALLV_BLOCKED_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET2 index= 469               MPIR_CVAR_IALLTOALLV_NETWORK_BLOCKED_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALLV_NETWORK_BLOCKED_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET2 index= 470                                    MPIR_CVAR_USE_IALLTOALLW : Control selection of MPI_Ialltoallw algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-1 @Alias:# I_MPI_ADJUST_IALLTOALLW @Default:# -1
20220327 015824.560 INFO             PET2 index= 471                               MPIR_CVAR_USE_IALLTOALLW_LIST : @Alias:# I_MPI_ADJUST_IALLTOALLW_LIST @Default:# -1
20220327 015824.560 INFO             PET2 index= 472                        MPIR_CVAR_USE_IALLTOALLW_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLTOALLW_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET2 index= 473                            MPIR_CVAR_USE_IALLTOALLW_NETWORK : @Alias:# I_MPI_ADJUST_IALLTOALLW_NETWORK @Default:# -1
20220327 015824.560 INFO             PET2 index= 474                               MPIR_CVAR_USE_IALLTOALLW_NODE : @Alias:# I_MPI_ADJUST_IALLTOALLW_NODE @Default:# -1
20220327 015824.560 INFO             PET2 index= 475                       MPIR_CVAR_IALLTOALLW_BLOCKED_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALLW_BLOCKED_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET2 index= 476               MPIR_CVAR_IALLTOALLW_NETWORK_BLOCKED_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALLW_NETWORK_BLOCKED_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET2 index= 477                                      MPIR_CVAR_USE_IGATHERV : Control selection of MPI_Igatherv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_IGATHERV @Default:# -1
20220327 015824.560 INFO             PET2 index= 478                                 MPIR_CVAR_USE_IGATHERV_LIST : @Alias:# I_MPI_ADJUST_IGATHERV_LIST @Default:# -1
20220327 015824.560 INFO             PET2 index= 479                          MPIR_CVAR_USE_IGATHERV_COMPOSITION : @Alias:# I_MPI_ADJUST_IGATHERV_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET2 index= 480                              MPIR_CVAR_USE_IGATHERV_NETWORK : @Alias:# I_MPI_ADJUST_IGATHERV_NETWORK @Default:# -1
20220327 015824.560 INFO             PET2 index= 481                                 MPIR_CVAR_USE_IGATHERV_NODE : @Alias:# I_MPI_ADJUST_IGATHERV_NODE @Default:# -1
20220327 015824.560 INFO             PET2 index= 482                                     MPIR_CVAR_USE_ISCATTERV : Control selection of MPI_Iscatterv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-1 @Alias:# I_MPI_ADJUST_ISCATTERV @Default:# -1
20220327 015824.560 INFO             PET2 index= 483                                MPIR_CVAR_USE_ISCATTERV_LIST : @Alias:# I_MPI_ADJUST_ISCATTERV_LIST @Default:# -1
20220327 015824.560 INFO             PET2 index= 484                         MPIR_CVAR_USE_ISCATTERV_COMPOSITION : @Alias:# I_MPI_ADJUST_ISCATTERV_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET2 index= 485                             MPIR_CVAR_USE_ISCATTERV_NETWORK : @Alias:# I_MPI_ADJUST_ISCATTERV_NETWORK @Default:# -1
20220327 015824.560 INFO             PET2 index= 486                                MPIR_CVAR_USE_ISCATTERV_NODE : @Alias:# I_MPI_ADJUST_ISCATTERV_NODE @Default:# -1
20220327 015824.560 INFO             PET2 index= 487                                      MPIR_CVAR_USE_IBARRIER : Control selection of MPI_Ibarrier algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-2 @Alias:# I_MPI_ADJUST_IBARRIER @Default:# -1
20220327 015824.560 INFO             PET2 index= 488                                 MPIR_CVAR_USE_IBARRIER_LIST : @Alias:# I_MPI_ADJUST_IBARRIER_LIST @Default:# -1
20220327 015824.560 INFO             PET2 index= 489                          MPIR_CVAR_USE_IBARRIER_COMPOSITION : @Alias:# I_MPI_ADJUST_IBARRIER_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET2 index= 490                              MPIR_CVAR_USE_IBARRIER_NETWORK : @Alias:# I_MPI_ADJUST_IBARRIER_NETWORK @Default:# -1
20220327 015824.560 INFO             PET2 index= 491                                 MPIR_CVAR_USE_IBARRIER_NODE : @Alias:# I_MPI_ADJUST_IBARRIER_NODE @Default:# -1
20220327 015824.561 INFO             PET2 index= 492                                      MPIR_CVAR_USE_ISCATTER : Control selection of MPI_Iscatter algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-2 @Alias:# I_MPI_ADJUST_ISCATTER @Default:# -1
20220327 015824.561 INFO             PET2 index= 493                                 MPIR_CVAR_USE_ISCATTER_LIST : @Alias:# I_MPI_ADJUST_ISCATTER_LIST @Default:# -1
20220327 015824.561 INFO             PET2 index= 494                          MPIR_CVAR_USE_ISCATTER_COMPOSITION : @Alias:# I_MPI_ADJUST_ISCATTER_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET2 index= 495                              MPIR_CVAR_USE_ISCATTER_NETWORK : @Alias:# I_MPI_ADJUST_ISCATTER_NETWORK @Default:# -1
20220327 015824.561 INFO             PET2 index= 496                                 MPIR_CVAR_USE_ISCATTER_NODE : @Alias:# I_MPI_ADJUST_ISCATTER_NODE @Default:# -1
20220327 015824.561 INFO             PET2 index= 497                            MPIR_CVAR_ISCATTER_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ISCATTER_KNOMIAL_RADIX @Default:# -1
20220327 015824.561 INFO             PET2 index= 498                    MPIR_CVAR_ISCATTER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ISCATTER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.561 INFO             PET2 index= 499                       MPIR_CVAR_ISCATTER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ISCATTER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.561 INFO             PET2 index= 500                                   MPIR_CVAR_USE_IALLGATHERV : Control selection of MPI_Iallgatherv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_IALLGATHERV @Default:# -1
20220327 015824.561 INFO             PET2 index= 501                              MPIR_CVAR_USE_IALLGATHERV_LIST : @Alias:# I_MPI_ADJUST_IALLGATHERV_LIST @Default:# -1
20220327 015824.561 INFO             PET2 index= 502                       MPIR_CVAR_USE_IALLGATHERV_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLGATHERV_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET2 index= 503                           MPIR_CVAR_USE_IALLGATHERV_NETWORK : @Alias:# I_MPI_ADJUST_IALLGATHERV_NETWORK @Default:# -1
20220327 015824.561 INFO             PET2 index= 504                              MPIR_CVAR_USE_IALLGATHERV_NODE : @Alias:# I_MPI_ADJUST_IALLGATHERV_NODE @Default:# -1
20220327 015824.561 INFO             PET2 index= 505                                       MPIR_CVAR_USE_IEXSCAN : Control selection of MPI_Iexscan algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-1 @Alias:# I_MPI_ADJUST_IEXSCAN @Default:# -1
20220327 015824.561 INFO             PET2 index= 506                                  MPIR_CVAR_USE_IEXSCAN_LIST : @Alias:# I_MPI_ADJUST_IEXSCAN_LIST @Default:# -1
20220327 015824.561 INFO             PET2 index= 507                           MPIR_CVAR_USE_IEXSCAN_COMPOSITION : @Alias:# I_MPI_ADJUST_IEXSCAN_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET2 index= 508                               MPIR_CVAR_USE_IEXSCAN_NETWORK : @Alias:# I_MPI_ADJUST_IEXSCAN_NETWORK @Default:# -1
20220327 015824.561 INFO             PET2 index= 509                                  MPIR_CVAR_USE_IEXSCAN_NODE : @Alias:# I_MPI_ADJUST_IEXSCAN_NODE @Default:# -1
20220327 015824.561 INFO             PET2 index= 510                                         MPIR_CVAR_USE_ISCAN : Control selection of MPI_Iscan algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-1 @Alias:# I_MPI_ADJUST_ISCAN @Default:# -1
20220327 015824.561 INFO             PET2 index= 511                                    MPIR_CVAR_USE_ISCAN_LIST : @Alias:# I_MPI_ADJUST_ISCAN_LIST @Default:# -1
20220327 015824.561 INFO             PET2 index= 512                             MPIR_CVAR_USE_ISCAN_COMPOSITION : @Alias:# I_MPI_ADJUST_ISCAN_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET2 index= 513                                 MPIR_CVAR_USE_ISCAN_NETWORK : @Alias:# I_MPI_ADJUST_ISCAN_NETWORK @Default:# -1
20220327 015824.561 INFO             PET2 index= 514                                    MPIR_CVAR_USE_ISCAN_NODE : @Alias:# I_MPI_ADJUST_ISCAN_NODE @Default:# -1
20220327 015824.561 INFO             PET2 index= 515                               MPIR_CVAR_USE_IREDUCE_SCATTER : Control selection of MPI_Ireduce_scatter algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER @Default:# -1
20220327 015824.561 INFO             PET2 index= 516                          MPIR_CVAR_USE_IREDUCE_SCATTER_LIST : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_LIST @Default:# -1
20220327 015824.561 INFO             PET2 index= 517                   MPIR_CVAR_USE_IREDUCE_SCATTER_COMPOSITION : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET2 index= 518                       MPIR_CVAR_USE_IREDUCE_SCATTER_NETWORK : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_NETWORK @Default:# -1
20220327 015824.561 INFO             PET2 index= 519                          MPIR_CVAR_USE_IREDUCE_SCATTER_NODE : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_NODE @Default:# -1
20220327 015824.561 INFO             PET2 index= 520                         MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK : Control selection of MPI_Ireduce_scatter_block algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK @Default:# -1
20220327 015824.561 INFO             PET2 index= 521                    MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK_LIST : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK_LIST @Default:# -1
20220327 015824.561 INFO             PET2 index= 522             MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK_COMPOSITION : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET2 index= 523                 MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK_NETWORK : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK_NETWORK @Default:# -1
20220327 015824.561 INFO             PET2 index= 524                    MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK_NODE : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK_NODE @Default:# -1
20220327 015824.561 INFO             PET2 index= 525                               MPIR_CVAR_IMPI_SHMGR_DATASIZE : Define the size of shared memory area available for each rank for data placement. Messages greater than this value will not be processed by SHM-based collective operation, but will be processed by point-to-point based collective operation. The value must be a multiple of 4096. @Alias:# I_MPI_COLL_SHM_THRESHOLD @Default:# 16384
20220327 015824.561 INFO             PET2 index= 526                              MPIR_CVAR_IMPI_SHMGR_SPINCOUNT : @Alias:# I_MPI_COLL_SHM_PROGRESS_SPIN_COUNT
20220327 015824.561 INFO             PET2 index= 527                              MPIR_CVAR_INTEL_COLL_INTRANODE : @Alias:# I_MPI_COLL_INTRANODE
20220327 015824.561 INFO             PET2 index= 528                         MPIR_CVAR_ENABLE_EXPERIMENTAL_ALGOS : @Alias:# I_MPI_COLL_EXPERIMENTAL
20220327 015824.561 INFO             PET2 index= 529                                    MPIR_CVAR_IMPI_WAIT_MODE : @Alias:# I_MPI_WAIT_MODE
20220327 015824.561 INFO             PET2 index= 530                                 MPIR_CVAR_IMPI_THREAD_SLEEP : @Alias:# I_MPI_THREAD_SLEEP
20220327 015824.561 INFO             PET2 index= 531                          MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the short message algorithm will be used if the send buffer size is < this value (in bytes). (See also: MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE)
20220327 015824.561 INFO             PET2 index= 532                           MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the long message algorithm will be used if the send buffer size is >= this value (in bytes) (See also: MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE)
20220327 015824.561 INFO             PET2 index= 533                      MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE : The smallest message size that will be used for the pipelined, large-message, ring algorithm in the MPI_Allgatherv implementation.
20220327 015824.561 INFO             PET2 index= 534                          MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20220327 015824.561 INFO             PET2 index= 535                            MPIR_CVAR_ENABLE_SMP_COLLECTIVES : Enable SMP aware collective communication.
20220327 015824.561 INFO             PET2 index= 536                              MPIR_CVAR_ENABLE_SMP_ALLREDUCE : Enable SMP aware allreduce.
20220327 015824.561 INFO             PET2 index= 537                        MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE : Maximum message size for which SMP-aware allreduce is used.  A value of '0' uses SMP-aware allreduce for all message sizes.
20220327 015824.561 INFO             PET2 index= 538                           MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE : the short message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value (See also: MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE)
20220327 015824.561 INFO             PET2 index= 539                          MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE : the medium message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value and larger than MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE (See also: MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE)
20220327 015824.561 INFO             PET2 index= 540                                MPIR_CVAR_ENABLE_SMP_BARRIER : Enable SMP aware barrier.
20220327 015824.561 INFO             PET2 index= 541                                   MPIR_CVAR_BCAST_MIN_PROCS : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_SHORT_MSG_SIZE, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20220327 015824.561 INFO             PET2 index= 542                              MPIR_CVAR_BCAST_SHORT_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20220327 015824.561 INFO             PET2 index= 543                               MPIR_CVAR_BCAST_LONG_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_SHORT_MSG_SIZE)
20220327 015824.561 INFO             PET2 index= 544                                  MPIR_CVAR_ENABLE_SMP_BCAST : Enable SMP aware broadcast (See also: MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE)
20220327 015824.561 INFO             PET2 index= 545                            MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE : Maximum message size for which SMP-aware broadcast is used.  A value of '0' uses SMP-aware broadcast for all message sizes. (See also: MPIR_CVAR_ENABLE_SMP_BCAST)
20220327 015824.561 INFO             PET2 index= 546                            MPIR_CVAR_GATHER_VSMALL_MSG_SIZE : use a temporary buffer for intracommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE)
20220327 015824.561 INFO             PET2 index= 547                       MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_VSMALL_MSG_SIZE)
20220327 015824.561 INFO             PET2 index= 548                     MPIR_CVAR_GATHERV_INTER_SSEND_MIN_PROCS : Use Ssend (synchronous send) for intercommunicator MPI_Gatherv if the "group B" size is >= this value.  Specifying "-1" always avoids using Ssend.  For backwards compatibility, specifying "0" uses the default value.
20220327 015824.561 INFO             PET2 index= 549                             MPIR_CVAR_REDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20220327 015824.561 INFO             PET2 index= 550                                 MPIR_CVAR_ENABLE_SMP_REDUCE : Enable SMP aware reduce.
20220327 015824.561 INFO             PET2 index= 551                           MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE : Maximum message size for which SMP-aware reduce is used.  A value of '0' uses SMP-aware reduce for all message sizes.
20220327 015824.561 INFO             PET2 index= 552                      MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Scatter if the send buffer size is < this value (in bytes)
20220327 015824.561 INFO             PET2 index= 553                                  MPIR_CVAR_USE_CPU_PLATFORM : @Alias:# I_MPI_PLATFORM
20220327 015824.561 INFO             PET2 index= 554                          MPIR_CVAR_FAILURE_ON_COLL_FALLBACK : @Alias:# I_MPI_ADJUST_FAILURE_ON_COLL_FALLBACK
20220327 015824.561 INFO             PET2 index= 555                         MPIR_CVAR_FAILURE_ON_MATCH_FALLBACK : @Alias:# I_MPI_ADJUST_FAILURE_ON_MATCH_FALLBACK
20220327 015824.561 INFO             PET2 index= 556                           MPIR_CVAR_ADJUST_SENDRECV_REPLACE : @Alias:# I_MPI_ADJUST_SENDRECV_REPLACE
20220327 015824.561 INFO             PET2 index= 557                MPIR_CVAR_ADJUST_SENDRECV_REPLACE_FRAME_SIZE : @Alias:# I_MPI_ADJUST_SENDRECV_REPLACE_FRAME_SIZE
20220327 015824.561 INFO             PET2 index= 558                         MPIR_CVAR_NUMERICAL_REPRODUCIBILITY : @Alias:# I_MPI_CBWR
20220327 015824.561 INFO             PET2 index= 559                                    MPIR_CVAR_USE_TUNING_CH4 : @Alias:# I_MPI_TUNING
20220327 015824.561 INFO             PET2 index= 560                                    MPIR_CVAR_USE_TUNING_NET : @Alias:# I_MPI_TUNING_NETWORK
20220327 015824.561 INFO             PET2 index= 561                                    MPIR_CVAR_USE_TUNING_SHM : @Alias:# I_MPI_TUNING_NODE
20220327 015824.561 INFO             PET2 index= 562                                   MPIR_CVAR_DUMP_TUNING_CH4 : @Alias:# I_MPI_TUNING_COMPOSITION_DUMP
20220327 015824.561 INFO             PET2 index= 563                                   MPIR_CVAR_DUMP_TUNING_NET : @Alias:# I_MPI_TUNING_NETWORK_DUMP
20220327 015824.561 INFO             PET2 index= 564                                   MPIR_CVAR_DUMP_TUNING_SHM : @Alias:# I_MPI_TUNING_NODE_DUMP
20220327 015824.561 INFO             PET2 index= 565                                        MPIR_CVAR_BIN_TUNING : @Alias:# I_MPI_TUNING_BIN
20220327 015824.561 INFO             PET2 index= 566                                   MPIR_CVAR_BIN_DUMP_TUNING : @Alias:# I_MPI_TUNING_BIN_DUMP
20220327 015824.561 INFO             PET2 index= 567                                   MPIR_CVAR_TUNING_BIN_PATH : @Alias:# I_MPI_TUNING_BIN_PATH
20220327 015824.561 INFO             PET2 index= 568                            MPIR_CVAR_TUNING_COMPOSITION_PPN : @Alias:# I_MPI_TUNING_COMPOSITION_PPN
20220327 015824.561 INFO             PET2 index= 569                                MPIR_CVAR_TUNING_NETWORK_PPN : @Alias:# I_MPI_TUNING_NETWORK_PPN
20220327 015824.561 INFO             PET2 index= 570                                   MPIR_CVAR_TUNING_NODE_PPN : @Alias:# I_MPI_TUNING_NODE_PPN
20220327 015824.561 INFO             PET2 index= 571                 MPIR_CVAR_TUNING_COMPOSITION_COMM_HIERARCHY : @Alias:# I_MPI_TUNING_COMPOSITION_COMM_HIERARCHY
20220327 015824.561 INFO             PET2 index= 572                     MPIR_CVAR_TUNING_NETWORK_COMM_HIERARCHY : @Alias:# I_MPI_TUNING_NETWORK_COMM_HIERARCHY
20220327 015824.561 INFO             PET2 index= 573                        MPIR_CVAR_TUNING_NODE_COMM_HIERARCHY : @Alias:# I_MPI_TUNING_NODE_COMM_HIERARCHY
20220327 015824.561 INFO             PET2 index= 574                                       MPIR_CVAR_TUNING_MODE : @Alias:# I_MPI_TUNING_MODE
20220327 015824.561 INFO             PET2 index= 575                                MPIR_CVAR_TUNING_AUTO_POLICY : @Alias:# I_MPI_TUNING_AUTO_POLICY
20220327 015824.561 INFO             PET2 index= 576                             MPIR_CVAR_TUNING_AUTO_COMM_LIST : @Alias:# I_MPI_TUNING_AUTO_COMM_LIST
20220327 015824.561 INFO             PET2 index= 577                             MPIR_CVAR_TUNING_AUTO_COMM_USER : @Alias:# I_MPI_TUNING_AUTO_COMM_USER
20220327 015824.561 INFO             PET2 index= 578                          MPIR_CVAR_TUNING_AUTO_COMM_DEFAULT : @Alias:# I_MPI_TUNING_AUTO_COMM_DEFAULT
20220327 015824.561 INFO             PET2 index= 579                              MPIR_CVAR_TUNING_AUTO_ITER_NUM : @Alias:# I_MPI_TUNING_AUTO_ITER_NUM
20220327 015824.561 INFO             PET2 index= 580                           MPIR_CVAR_TUNING_AUTO_ITER_POLICY : @Alias:# I_MPI_TUNING_AUTO_ITER_POLICY
20220327 015824.561 INFO             PET2 index= 581                 MPIR_CVAR_TUNING_AUTO_ITER_POLICY_THRESHOLD : @Alias:# I_MPI_TUNING_AUTO_ITER_POLICY_THRESHOLD
20220327 015824.561 INFO             PET2 index= 582                                  MPIR_CVAR_TUNING_AUTO_SYNC : @Alias:# I_MPI_TUNING_AUTO_SYNC
20220327 015824.561 INFO             PET2 index= 583                          MPIR_CVAR_TUNING_AUTO_STORAGE_SIZE : @Alias:# I_MPI_TUNING_AUTO_STORAGE_SIZE
20220327 015824.561 INFO             PET2 index= 584                       MPIR_CVAR_TUNING_AUTO_WARMUP_ITER_NUM : @Alias:# I_MPI_TUNING_AUTO_WARMUP_ITER_NUM
20220327 015824.561 INFO             PET2 index= 585                                 MPIR_CVAR_TUNING_AUTO_SMART : @Alias:# I_MPI_TUNING_AUTO_SMART
20220327 015824.561 INFO             PET2 index= 586                                  MPIR_CVAR_TUNING_COLL_LIST : @Alias:# I_MPI_TUNING_COLL_LIST
20220327 015824.561 INFO             PET2 index= 587                               MPIR_CVAR_TUNING_COLL_VEC_OPS : @Alias:# I_MPI_TUNING_COLL_VEC_OPS
20220327 015824.561 INFO             PET2 index= 588                              MPIR_CVAR_DEFAULT_THREAD_LEVEL : @Alias:# I_MPI_THREAD_LEVEL
20220327 015824.561 INFO             PET2 index= 589                                      MPIR_CVAR_THREAD_SPLIT : Control the MPI_THREAD_SPLIT model support. @Alias:# I_MPI_THREAD_SPLIT @Default:# false
20220327 015824.561 INFO             PET2 index= 590                                    MPIR_CVAR_THREAD_RUNTIME : Control threading runtimes support. @Alias:# I_MPI_THREAD_RUNTIME @Default:# generic
20220327 015824.561 INFO             PET2 index= 591                                     MPIR_CVAR_THREAD_ID_KEY : Set the MPI info object key that is used to explicitly define the application $thread_id for a communicator. @Alias:# I_MPI_THREAD_ID_KEY @Default:# -1
20220327 015824.561 INFO             PET2 index= 592                                        MPIR_CVAR_THREAD_MAX : Set the maximum number of application threads per rank. @Alias:# I_MPI_THREAD_MAX @Default:# -1
20220327 015824.561 INFO             PET2 index= 593                                    MPIR_CVAR_ASYNC_PROGRESS : Enables asynchronous progress threads. @Alias:# I_MPI_ASYNC_PROGRESS @Default:# false
20220327 015824.561 INFO             PET2 index= 594                          MPIR_CVAR_CH4_MAX_PROGRESS_THREADS : Specifies the maximum number of progress threads. @Alias:# I_MPI_ASYNC_PROGRESS_THREADS @Default:# 1
20220327 015824.562 INFO             PET2 index= 595                      MPIR_CVAR_CH4_PROGRESS_THREAD_AFFINITY : Specifies affinity for all progress threads of local processes. @Alias:# I_MPI_ASYNC_PROGRESS_PIN @Default:# not defined
20220327 015824.562 INFO             PET2 index= 596                                         MPIR_CVAR_EP_ID_KEY : Set the MPI info object key that is used to explicitly define the progress $thread_id for a communicator. @Alias:# I_MPI_ASYNC_PROGRESS_ID_KEY @Default:# thread_id
20220327 015824.562 INFO             PET2 index= 597                                       MPIR_CVAR_THREAD_MODE : @Alias:# I_MPI_THREAD_MODE
20220327 015824.562 INFO             PET2 index= 598                                 MPIR_CVAR_THREAD_LOCK_LEVEL : @Alias:# I_MPI_THREAD_LOCK_LEVEL
20220327 015824.562 INFO             PET2 index= 599                                  MPIR_CVAR_CH4_OFI_MAX_VCIS : @Alias:# I_MPI_THREAD_EP_MAX
20220327 015824.562 INFO             PET2 index= 600                                       MPIR_CVAR_INTEL_DEBUG : Print out debugging information when an MPI program starts running.$ Syntax$ I_MPI_DEBUG=<level>$ Arguments$ <level> - indicate level of debug information provided @Alias:# I_MPI_DEBUG @Default:# 0
20220327 015824.562 INFO             PET2 index= 601                                     MPIR_CVAR_DEBUG_VERSION : Print Intel MPI version. @Alias:# I_MPI_PRINT_VERSION @Default:# 0
20220327 015824.562 INFO             PET2 index= 602                                    MPIR_CVAR_ERROR_CHECKING : @Alias:# I_MPI_ERROR_CHECKING
20220327 015824.562 INFO             PET2 index= 603                                        MPIR_CVAR_MULTI_INIT : @Alias:# I_MPI_MULTI_INIT
20220327 015824.562 INFO             PET2 index= 604                               MPIR_CVAR_REMOVED_VAR_WARNING : Print out a warning if a removed environment variable is set. @Alias:# I_MPI_REMOVED_VAR_WARNING @Default:# 1
20220327 015824.562 INFO             PET2 index= 605                                MPIR_CVAR_VAR_CHECK_SPELLING : Print out a warning if an unknown environment variable is set. @Alias:# I_MPI_VAR_CHECK_SPELLING @Default:# 1
20220327 015824.562 INFO             PET2 index= 606                           MPIR_CVAR_INTEL_MPI_COMPATIBILITY : Select the runtime compatibility mode.$ Syntax$ I_MPI_COMPATIBILITY=<value>$ Arguments$ <value> - Define compatibility mode$ -----------------------------------------------------------------------$ <not defined> - The MPI-3.1 standard compatibility$ <3> - The Intel® MPI Library 3.x compatible mode$ <4> - The Intel® MPI Library 4.x compatible mode$ <5> - The Intel® MPI Library 5.x compatible mode$ ----------------------------------------------------------------------- @Alias:# I_MPI_COMPATIBILITY @Default:# 5
20220327 015824.562 INFO             PET2 index= 607                          MPIR_CVAR_IMPI_PROGRESS_SPIN_COUNT : @Alias:# I_MPI_SPIN_COUNT
20220327 015824.562 INFO             PET2 index= 608                         MPIR_CVAR_IMPI_PROGRESS_PAUSE_COUNT : @Alias:# I_MPI_PAUSE_COUNT
20220327 015824.562 INFO             PET2 index= 609                                 MPIR_CVAR_IMPI_THREAD_YIELD : @Alias:# I_MPI_THREAD_YIELD
20220327 015824.562 INFO             PET2 index= 610                                      MPIR_CVAR_SILENT_ABORT : Do not print abort warning message @Alias:# I_MPI_SILENT_ABORT
20220327 015824.562 INFO             PET2 index= 611                                  MPIR_CVAR_JOB_IDLE_TIMEOUT : Abort job if idle time is larger than the threshold in seconds. @Alias:# I_MPI_JOB_IDLE_TIMEOUT
20220327 015824.562 INFO             PET2 index= 612                              MPIR_CVAR_PMI_VALUE_LENGTH_MAX : Set PMI buffer length as minimum of variable value and PMI_KVS_Get_value_length_max(). @Alias:# I_MPI_PMI_VALUE_LENGTH_MAX
20220327 015824.562 INFO             PET2 index= 613                                       MPIR_CVAR_PMI_LIBRARY : Specify the name to third party implementation of the PMI library. @Alias:# I_MPI_PMI_LIBRARY
20220327 015824.562 INFO             PET2 index= 614                                               MPIR_CVAR_PMI : Select PMI version. Choices: auto, pmi1, pmi2, pmix.$ By default pmi version will be chosen automatically. @Alias:# I_MPI_PMI
20220327 015824.562 INFO             PET2 index= 615                                 MPIR_CVAR_NODEMAP_ALGORITHM : Select algorithm for nodemap creation. Choices: pmi_process_mapping, slurm, pmi_alltoall, auto. @Alias:# I_MPI_NODEMAP_ALGORITHM
20220327 015824.562 INFO             PET2 index= 616                                      MPIR_CVAR_ASYNC_REDUCE : @Alias:# I_MPI_ASYNC_REDUCE @Verbosity:# hidden
20220327 015824.562 INFO             PET2 index= 617                            MPIR_CVAR_CH4_MAX_REDUCE_THREADS : @Alias:# I_MPI_ASYNC_REDUCE_THREADS @Verbosity:# hidden
20220327 015824.562 INFO             PET2 index= 618                      MPIR_CVAR_ASYNC_REDUCE_COUNT_THRESHOLD : @Alias:# I_MPI_ASYNC_REDUCE_COUNT_THRESHOLD @Verbosity:# hidden
20220327 015824.562 INFO             PET2 index= 619                        MPIR_CVAR_CH4_REDUCE_THREAD_AFFINITY : @Alias:# I_MPI_ASYNC_REDUCE_PIN @Verbosity:# hidden
20220327 015824.562 INFO             PET2 --- VMK::logSystem() end ---------------------------------
20220327 015824.562 INFO             PET2 main: --- VMK::log() start -------------------------------------
20220327 015824.562 INFO             PET2 main: vm located at: 0x87ae60
20220327 015824.562 INFO             PET2 main: petCount=6 localPet=2 mypthid=140737352203136 currentSsiPe=8
20220327 015824.562 INFO             PET2 main: Current system level affinity pinning for local PET:
20220327 015824.562 INFO             PET2 main:  SSIPE=8
20220327 015824.562 INFO             PET2 main:  SSIPE=9
20220327 015824.562 INFO             PET2 main:  SSIPE=10
20220327 015824.562 INFO             PET2 main:  SSIPE=11
20220327 015824.562 INFO             PET2 main: Current system level OMP_NUM_THREADS setting for local PET: 1
20220327 015824.562 INFO             PET2 main: ssiCount=1 localSsi=0
20220327 015824.562 INFO             PET2 main: mpionly=1 threadsflag=0
20220327 015824.562 INFO             PET2 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220327 015824.562 INFO             PET2 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220327 015824.562 INFO             PET2 main:  PE=0 SSI=0 SSIPE=0
20220327 015824.562 INFO             PET2 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220327 015824.562 INFO             PET2 main:  PE=1 SSI=0 SSIPE=1
20220327 015824.562 INFO             PET2 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220327 015824.562 INFO             PET2 main:  PE=2 SSI=0 SSIPE=2
20220327 015824.562 INFO             PET2 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220327 015824.562 INFO             PET2 main:  PE=3 SSI=0 SSIPE=3
20220327 015824.562 INFO             PET2 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220327 015824.562 INFO             PET2 main:  PE=4 SSI=0 SSIPE=4
20220327 015824.562 INFO             PET2 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220327 015824.562 INFO             PET2 main:  PE=5 SSI=0 SSIPE=5
20220327 015824.562 INFO             PET2 main: --- VMK::log() end ---------------------------------------
20220327 015824.565 INFO             PET2 Executing 'userm1_setvm'
20220327 015824.566 INFO             PET2 Executing 'userm1_register'
20220327 015824.566 INFO             PET2 Executing 'userm2_setvm'
20220327 015824.566 DEBUG            PET2 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220327 015824.566 DEBUG            PET2 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220327 015824.649 INFO             PET2 Entering 'user1_run'
20220327 015824.649 INFO             PET2 model1: --- VMK::log() start -------------------------------------
20220327 015824.649 INFO             PET2 model1: vm located at: 0xa38690
20220327 015824.649 INFO             PET2 model1: petCount=6 localPet=2 mypthid=140737352203136 currentSsiPe=2
20220327 015824.649 INFO             PET2 model1: Current system level affinity pinning for local PET:
20220327 015824.649 INFO             PET2 model1:  SSIPE=2
20220327 015824.649 INFO             PET2 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220327 015824.649 INFO             PET2 model1: ssiCount=1 localSsi=0
20220327 015824.649 INFO             PET2 model1: mpionly=1 threadsflag=0
20220327 015824.649 INFO             PET2 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220327 015824.649 INFO             PET2 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220327 015824.649 INFO             PET2 model1:  PE=0 SSI=0 SSIPE=0
20220327 015824.649 INFO             PET2 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220327 015824.649 INFO             PET2 model1:  PE=1 SSI=0 SSIPE=1
20220327 015824.649 INFO             PET2 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220327 015824.649 INFO             PET2 model1:  PE=2 SSI=0 SSIPE=2
20220327 015824.649 INFO             PET2 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220327 015824.649 INFO             PET2 model1:  PE=3 SSI=0 SSIPE=3
20220327 015824.649 INFO             PET2 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220327 015824.649 INFO             PET2 model1:  PE=4 SSI=0 SSIPE=4
20220327 015824.649 INFO             PET2 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220327 015824.649 INFO             PET2 model1:  PE=5 SSI=0 SSIPE=5
20220327 015824.649 INFO             PET2 model1: --- VMK::log() end ---------------------------------------
20220327 015824.649 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220327 015824.775 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220327 015824.900 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220327 015825.025 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220327 015825.150 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220327 015825.275 INFO             PET2 Exiting 'user1_run'
20220327 015826.769 INFO             PET2 Entering 'user1_run'
20220327 015826.769 INFO             PET2 model1: --- VMK::log() start -------------------------------------
20220327 015826.769 INFO             PET2 model1: vm located at: 0xa38690
20220327 015826.769 INFO             PET2 model1: petCount=6 localPet=2 mypthid=140737352203136 currentSsiPe=2
20220327 015826.769 INFO             PET2 model1: Current system level affinity pinning for local PET:
20220327 015826.769 INFO             PET2 model1:  SSIPE=2
20220327 015826.769 INFO             PET2 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220327 015826.769 INFO             PET2 model1: ssiCount=1 localSsi=0
20220327 015826.769 INFO             PET2 model1: mpionly=1 threadsflag=0
20220327 015826.769 INFO             PET2 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220327 015826.769 INFO             PET2 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220327 015826.769 INFO             PET2 model1:  PE=0 SSI=0 SSIPE=0
20220327 015826.769 INFO             PET2 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220327 015826.769 INFO             PET2 model1:  PE=1 SSI=0 SSIPE=1
20220327 015826.769 INFO             PET2 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220327 015826.769 INFO             PET2 model1:  PE=2 SSI=0 SSIPE=2
20220327 015826.769 INFO             PET2 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220327 015826.769 INFO             PET2 model1:  PE=3 SSI=0 SSIPE=3
20220327 015826.769 INFO             PET2 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220327 015826.769 INFO             PET2 model1:  PE=4 SSI=0 SSIPE=4
20220327 015826.769 INFO             PET2 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220327 015826.769 INFO             PET2 model1:  PE=5 SSI=0 SSIPE=5
20220327 015826.769 INFO             PET2 model1: --- VMK::log() end ---------------------------------------
20220327 015826.769 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220327 015826.894 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220327 015827.020 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220327 015827.145 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220327 015827.270 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220327 015827.395 INFO             PET2 Exiting 'user1_run'
20220327 015828.897 INFO             PET2  NUMBER_OF_PROCESSORS           6
20220327 015828.898 INFO             PET2  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220327 015828.898 INFO             PET2 Finalizing ESMF
20220327 015824.554 INFO             PET3 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220327 015824.554 INFO             PET3 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220327 015824.554 INFO             PET3 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220327 015824.554 INFO             PET3 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220327 015824.554 INFO             PET3 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220327 015824.554 INFO             PET3 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220327 015824.554 INFO             PET3 Running with ESMF Version   : v8.3.0b10-105-ga5295e34ae
20220327 015824.554 INFO             PET3 ESMF library build date/time: "Mar 27 2022" "01:23:54"
20220327 015824.554 INFO             PET3 ESMF library build location : /gpfsm/dnb04/projects/p98/mpotts/esmf/intel_19.1.3_intelmpi_O_develop
20220327 015824.554 INFO             PET3 ESMF_COMM                   : intelmpi
20220327 015824.555 INFO             PET3 ESMF_MOAB                   : enabled
20220327 015824.555 INFO             PET3 ESMF_LAPACK                 : enabled
20220327 015824.555 INFO             PET3 ESMF_NETCDF                 : enabled
20220327 015824.555 INFO             PET3 ESMF_PNETCDF                : disabled
20220327 015824.555 INFO             PET3 ESMF_PIO                    : enabled
20220327 015824.555 INFO             PET3 ESMF_YAMLCPP                : enabled
20220327 015824.555 INFO             PET3 --- VMK::logSystem() start -------------------------------
20220327 015824.555 INFO             PET3 esmfComm=intelmpi
20220327 015824.555 INFO             PET3 isPthreadsEnabled=1
20220327 015824.555 INFO             PET3 isOpenMPEnabled=1
20220327 015824.555 INFO             PET3 isOpenACCEnabled=0
20220327 015824.556 INFO             PET3 isSsiSharedMemoryEnabled=1
20220327 015824.556 INFO             PET3 ssiCount=1 peCount=6
20220327 015824.556 INFO             PET3 PE=0 SSI=0 SSIPE=0
20220327 015824.556 INFO             PET3 PE=1 SSI=0 SSIPE=1
20220327 015824.556 INFO             PET3 PE=2 SSI=0 SSIPE=2
20220327 015824.556 INFO             PET3 PE=3 SSI=0 SSIPE=3
20220327 015824.556 INFO             PET3 PE=4 SSI=0 SSIPE=4
20220327 015824.556 INFO             PET3 PE=5 SSI=0 SSIPE=5
20220327 015824.556 INFO             PET3 --- VMK::logSystem() MPI Control Variables ---------------
20220327 015824.556 INFO             PET3 index=   0                                          I_MPI_DEBUG_OUTPUT : @Default:# not defined
20220327 015824.556 INFO             PET3 index=   1                                        I_MPI_DEBUG_COREDUMP : @Default:# 1
20220327 015824.556 INFO             PET3 index=   2                                          I_MPI_LIBRARY_KIND : @Default:# not defined
20220327 015824.556 INFO             PET3 index=   3                                  I_MPI_OFI_LIBRARY_INTERNAL : @Default:# not defined
20220327 015824.556 INFO             PET3 index=   4                                            I_MPI_CC_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET3 index=   5                                           I_MPI_CXX_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET3 index=   6                                            I_MPI_FC_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET3 index=   7                                           I_MPI_F77_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET3 index=   8                                           I_MPI_F90_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET3 index=   9                                         I_MPI_TRACE_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  10                                         I_MPI_CHECK_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  11                                        I_MPI_CHECK_COMPILER : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  12                                                    I_MPI_CC : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  13                                                   I_MPI_CXX : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  14                                                    I_MPI_FC : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  15                                                   I_MPI_F90 : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  16                                                   I_MPI_F77 : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  17                                                  I_MPI_ROOT : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  18                                           I_MPI_ONEAPI_ROOT : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  19                                           MPIR_CVAR_VT_ROOT : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  20                                   I_MPI_COMPILER_CONFIG_DIR : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  21                                                  I_MPI_LINK : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  22                                      I_MPI_DEBUG_INFO_STRIP : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  23                                                I_MPI_CFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  24                                              I_MPI_CXXFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  25                                               I_MPI_FCFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  26                                                I_MPI_FFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  27                                               I_MPI_LDFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  28                                             I_MPI_FORT_BIND : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  29                                           I_MPI_AUTH_METHOD : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  30                               I_MPI_HYDRA_COLLECTIVE_LAUNCH : @Default:# 1
20220327 015824.556 INFO             PET3 index=  31                                  I_MPI_HYDRA_UNIQUE_PROXIES : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  32                                        I_MPI_FAULT_CONTINUE : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  33                                   I_MPI_FAULT_NODE_CONTINUE : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  34                                                I_MPI_MPIRUN : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  35                                            I_MPI_BIND_ORDER : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  36                                             I_MPI_BIND_NUMA : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  37                                     I_MPI_BIND_WIN_ALLOCATE : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  38                                      I_MPI_HYDRA_NAMESERVER : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  39                                        I_MPI_JOB_CHECK_LIBS : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  40                                    I_MPI_HYDRA_SERVICE_PORT : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  41                                           I_MPI_HYDRA_DEBUG : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  42                                             I_MPI_HYDRA_ENV : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  43                                           I_MPI_JOB_TIMEOUT : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  44                                       I_MPI_MPIEXEC_TIMEOUT : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  45                                   I_MPI_JOB_STARTUP_TIMEOUT : Set this environment variable to make mpiexec.hydra terminate$ the job in <timeout> seconds if some processes are not launched. @Default:# -1
20220327 015824.556 INFO             PET3 index=  46                                    I_MPI_JOB_TIMEOUT_SIGNAL : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  47                                      I_MPI_JOB_ABORT_SIGNAL : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  48                                I_MPI_JOB_SIGNAL_PROPAGATION : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  49                                  I_MPI_HYDRA_BOOTSTRAP_EXEC : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  50                       I_MPI_HYDRA_BOOTSTRAP_EXEC_EXTRA_ARGS : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  51                              I_MPI_HYDRA_BOOTSTRAP_AUTOFORK : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  52                                             I_MPI_HYDRA_RMK : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  53                                     I_MPI_HYDRA_PMI_CONNECT : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  54                                         I_MPI_HYDRA_TOPOLIB : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  55                                            I_MPI_PORT_RANGE : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  56                         I_MPI_JOB_RESPECT_PROCESS_PLACEMENT : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  57                                                I_MPI_TMPDIR : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  58                                           I_MPI_HYDRA_DEMUX : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  59                                           I_MPI_HYDRA_IFACE : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  60                                I_MPI_HYDRA_GDB_REMOTE_SHELL : @Default:# not defined
20220327 015824.556 INFO             PET3 index=  61                                   I_MPI_HYDRA_PMI_AGGREGATE : @Default:# not defined
20220327 015824.557 INFO             PET3 index=  62                                        I_MPI_JOB_TRACE_LIBS : @Default:# not defined
20220327 015824.557 INFO             PET3 index=  63                                       I_MPI_HYDRA_HOST_FILE : Set the host file to run the application.$ Syntax$ I_MPI_HYDRA_HOST_FILE=<arg>$ Arguments$ <arg> - String parameter$ -----------------------------------------------------------------------$ <hostsfile> - The full or relative path to the host file$ ----------------------------------------------------------------------- @Default:# not defined
20220327 015824.557 INFO             PET3 index=  64                                     I_MPI_HYDRA_HOSTS_GROUP : This environment variable allows to set node ranges using brackets,$ commas, and dashes (like in Slurm* Workload Manager). @Default:# not defined
20220327 015824.557 INFO             PET3 index=  65                                               I_MPI_PERHOST : Define the default behavior for the -perhost option of the mpiexec.hydra$ command.$ Syntax$ I_MPI_PERHOST=<value>$ Arguments$ <value> - Define a value used for -perhost by default$ -----------------------------------------------------------------------$ <integer > 0> - Exact value for the option$ <all>         - All logical CPUs on the node$ <allcores>    - All cores (physical CPUs) on the node. This is the$  default value.$ ----------------------------------------------------------------------- @Default:# not defined
20220327 015824.557 INFO             PET3 index=  66                                                 I_MPI_GTOOL : Specify the tools to be launched for selected ranks. An alternative to$ this variable is the -gtool option$ Syntax$ I_MPI_GTOOL="<command line for a tool 1>:<ranks set 1>[=exclusive]$ [@arch 1];<command line for a tool 2>:<ranks set 2>[=exclusive]$ [@arch 2]; â€¦ ;<command line for a tool n>:<ranks set n>[=exclusive]$ [@arch n]"$ Arguments$ <arg> - Specify a tool launch command, including parameters.$ -----------------------------------------------------------------------$ <command line>     - Specify tool launch command, including parameters$ <rank set>         - Specify the range of ranks that are involved in the$  tool execution. Separate ranks with a comma or use the '-' symbol for$ a set ofcontiguous ranks. To run the tool forall ranks, use the all$  argument.$ [=exclusive]       - All cores (physical CPUs) on the node. This is$ the default value.$ [@arch]            - Specify the architecture on which the tool runs$  optional). For a given <rank set>, if you specify this argument,$ the tool is launched
20220327 015824.557 INFO             PET3 index=  67                                    I_MPI_HYDRA_BRANCH_COUNT : Set this environment variable to restrict the number of child management$ processes launched by the mpiexec.hydra operation or by each pmi_proxy$ anagement process.$ Syntax$ I_MPI_HYDRA_BRANCH_COUNT=<num>$ Arguments$ <value> - Number$ -----------------------------------------------------------------------$ <n> >= 0 - The default value is -1 if less than 128 nodes are used. $ This value also means that there is no hierarchical structure$ The default value is 32 if more than 127 nodes are used$ ----------------------------------------------------------------------- @Default:# not defined
20220327 015824.557 INFO             PET3 index=  68                                       I_MPI_HYDRA_BOOTSTRAP : Set this environment variable to specify the bootstrap server Syntax$ I_MPI_HYDRA_BOOTSTRAP=<arg>$ Arguments$ <arg> - String parameter$ -----------------------------------------------------------------------$ <ssh>     - Use secure shell. This is the default value$ <rsh>     - Use remote shell$ <pdsh>    - Use parallel distributed shell$ <pbsdsh>  - Use Torque* and PBS* pbsdsh command$ <fork>    - Use fork call$ <slurm>   - Use SLURM* srun command$ <ll>      - Use LoadLeveler* llspawn.stdio command$ <lsf>     - Use LSF* blaunch command$ <sge>     - Use Univa* Grid Engine* qrsh command$ ----------------------------------------------------------------------- @Default:# not defined
20220327 015824.557 INFO             PET3 index=  69                                              I_MPI_PIN_UNIT : @Default:# not defined
20220327 015824.557 INFO             PET3 index=  70                                                 I_MPI_STATS :
20220327 015824.557 INFO             PET3 index=  71                                             I_MPI_TIMER_ART : @Default:# 1
20220327 015824.557 INFO             PET3 index=  72                                   I_MPI_OFFLOAD_DOMAIN_SIZE : Control the number of devices/tiles per MPI rank
20220327 015824.557 INFO             PET3 index=  73                                          I_MPI_OFFLOAD_CELL : Variable to choose the base unit: tile or device
20220327 015824.557 INFO             PET3 index=  74                                       I_MPI_OFFLOAD_DEVICES : Variable to select available devices
20220327 015824.557 INFO             PET3 index=  75                                   I_MPI_OFFLOAD_DEVICE_LIST : A comma-separated list of tiles and/or ranges of tiles.$ The process with the i-th rank is pinned to the i-th tile in the list.$ If I_MPI_OFFLOAD_CELL=device then it is comma-separated list of devices.
20220327 015824.557 INFO             PET3 index=  76                                        I_MPI_OFFLOAD_DOMAIN : Define domains through the comma separated list of hexadecimal numbers (domain masks).
20220327 015824.557 INFO             PET3 index=  77                               I_MPI_OFFLOAD_INFO_SET_NTILES : Set the number of tiles
20220327 015824.557 INFO             PET3 index=  78                                I_MPI_OFFLOAD_INFO_SET_NGPUS : Set the number of gpus
20220327 015824.557 INFO             PET3 index=  79                           I_MPI_OFFLOAD_INFO_SET_NNUMANODES : Set the number of numanodes
20220327 015824.557 INFO             PET3 index=  80                               I_MPI_OFFLOAD_INFO_SET_GPU_ID : Set gpu id for each tile
20220327 015824.557 INFO             PET3 index=  81                 I_MPI_OFFLOAD_INFO_SET_NUMANODE_ID_FOR_GPUS : Set numanode id for each gpu
20220327 015824.557 INFO             PET3 index=  82                I_MPI_OFFLOAD_INFO_SET_NUMANODE_ID_FOR_RANKS : Set numanode id for each rank
20220327 015824.557 INFO             PET3 index=  83                            I_MPI_OFFLOAD_INFO_SET_VENDOR_ID : Set vendor id for each device
20220327 015824.557 INFO             PET3 index=  84                                         MPIR_CVAR_INTEL_PIN : Turn on/off process pinning.$ Syntax$ I_MPI_PIN=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 - Enable process pinning$ > disable | no | off | 0 - Disable processes pinning$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN @Default:# on
20220327 015824.557 INFO             PET3 index=  85                          MPIR_CVAR_INTEL_PIN_SHOW_REAL_MASK : Turn on/off real masks pinning print.$ Syntax$ I_MPI_PIN_SHOW_REAL_MASK=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 - Enable real pinning print$ > disable | no | off | 0 - Disable real pinning print$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_SHOW_REAL_MASK @Default:# on
20220327 015824.557 INFO             PET3 index=  86                          MPIR_CVAR_INTEL_PIN_PROCESSOR_LIST : Define a processor subset and the mapping rules for MPI processes within$ this subset.$ Syntax$ I_MPI_PIN_PROCESSOR_LIST=<value>$ The environment variable value has the following syntax forms:$ 1. <proclist>$ 2. [<procset>][:[grain=<grain>][,shift=<shift>]$ [,preoffset=<preoffset>][,postoffset=<postoffset>]$ 3. [<procset>][:map=<map>]$ @Alias:# I_MPI_PIN_PROCESSOR_LIST @Default:# not defined
20220327 015824.557 INFO             PET3 index=  87                  MPIR_CVAR_INTEL_PIN_PROCESSOR_EXCLUDE_LIST : Define a subset of logical processors to be excluded for the pinning$ capability on the intended hosts.$ Syntax$ I_MPI_PIN_PROCESSOR_EXCLUDE_LIST=<proclist>$ Arguments$ <proclist> - A comma-separated list of logical processor numbers$ and/or ranges of processors.$ -----------------------------------------------------------------------$ > <l> - Processor with logical number <l>.$ > <l>-<m> - Range of processors with logical numbers from <l> to <m>.$ > <k>,<l>-<m> - Processors <k>, as well as <l> through <m>.$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_PROCESSOR_EXCLUDE_LIST @Default:# not defined
20220327 015824.557 INFO             PET3 index=  88                                    MPIR_CVAR_INTEL_PIN_CELL : Set this environment variable to define the pinning resolution$ granularity. I_MPI_PIN_CELL specifies the minimal processor cell$ allocated when an MPI process is running.$ Syntax$ I_MPI_PIN_CELL=<cell>$ Arguments$ <cell> - Specify the resolution granularity$ -----------------------------------------------------------------------$ > unit - Basic processor unit (logical CPU)$ > core - Physical processor core$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_CELL @Default:# unit
20220327 015824.557 INFO             PET3 index=  89                          MPIR_CVAR_INTEL_PIN_RESPECT_CPUSET : Respect the process affinity mask.$ Syntax$ I_MPI_PIN_RESPECT_CPUSET=<value>$ Arguments$ <value> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 - Respect the process affinity mask$ > disable | no | off | 0 - Do not respect the process affinity mask$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_RESPECT_CPUSET @Default:# on
20220327 015824.557 INFO             PET3 index=  90                             MPIR_CVAR_INTEL_PIN_RESPECT_HCA : In the presence of Intel(R) Omni-Path Architecture (Intel(R) OPA) or$ Infiniband architecture* host channel adapter (IBA* HCA),$ adjust the pinning according to the location of adapter.$ Syntax$ I_MPI_PIN_RESPECT_HCA=<value>$ Arguments$ <value> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 Use the location of IBA HCA if available$ > disable | no | off | 0 Do not use the location of IBA HCA$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_RESPECT_HCA @Default:# on
20220327 015824.557 INFO             PET3 index=  91                                  MPIR_CVAR_INTEL_PIN_DOMAIN : Intel(R) MPI Library provides environment variable to control process pinning for hybrid MPI/OpenMP* applications. This environment variable is used to define a number of non-overlapping subsets (domains) of logical processors on a node, and a set of rules on how MPI processes are bound to these domains by the following formula: one MPI process per one domain. Multi-core Shape:$ I_MPI_PIN_DOMAIN=<mc-shape>$ <mc-shape> - Define domains through multi-core terms.$ -----------------------------------------------------------------------$ > core - Each domain consists of the logical processors that share a$ particular core. The number of domains on a node is equal to the number$ of cores on the node.$ > socket | sock - Each domain consists of the logical processors that$ share a particular socket. The number of domains on a node is equal to$ the number of sockets on the node.$ > numa - Each domain consists of the logical processors that share a$ particular NUMA node. The number of domains on a machine is equal to$
20220327 015824.557 INFO             PET3 index=  92                                   MPIR_CVAR_INTEL_PIN_ORDER : Set this environment variable to define the mapping order for MPI$ processes to domains as specified by the$ I_MPI_PIN_DOMAIN environment variable.$ Syntax$ I_MPI_PIN_ORDER=<order>$ <order> - Specify the ranking order$ -----------------------------------------------------------------------$ > range - The domains are ordered according to the processor's BIOS$ numbering. This is a platform dependent numbering$ > scatter - The domains are ordered so that adjacent domains have$ minimal sharing of common resources$ > compact - The domains are ordered so that adjacent domains share$ common resources as much as possible. This is the default value$ > spread - The domains are ordered consecutively with the possibility$ not to share common resources$ > bunch - The processes are mapped proportionally to sockets and the$ domains are ordered as close as possible on the sockets$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_ORDER @Default:# compact
20220327 015824.557 INFO             PET3 index=  93                                   MPIR_CVAR_IMPI_HBW_POLICY : @Alias:# I_MPI_HBW_POLICY
20220327 015824.557 INFO             PET3 index=  94                          MPIR_CVAR_IMPI_INTERNAL_MEM_POLICY : @Alias:# I_MPI_INTERNAL_MEM_POLICY
20220327 015824.557 INFO             PET3 index=  95                                 MPIR_CVAR_IMPI_STATIC_BUILD : @Alias:# I_MPI_STATIC_BUILD
20220327 015824.557 INFO             PET3 index=  96                     MPIR_CVAR_IMPI_RETURN_INTERNAL_MEM_NUMA : @Alias:# I_MPI_RETURN_INTERNAL_MEM_NUMA
20220327 015824.557 INFO             PET3 index=  97                                   MPIR_CVAR_OFFLOAD_TOPOLIB : @Alias:# I_MPI_OFFLOAD_TOPOLIB
20220327 015824.557 INFO             PET3 index=  98                                        MPIR_CVAR_ENABLE_GPU : Control support of buffers offloaded to GPU/accelerator in MPI calls.$ Syntax$ I_MPI_OFFLOAD=<value>$ Arguments$ <value> - choice$ -----------------------------------------------------------------------$ <0> - Disabled$ <1> - Enabled only if Level Zero library is loaded at MPI_Init() time$ <2> - Enabled, will fail if Level Zero library is not loadable$ ----------------------------------------------------------------------- @Alias:# I_MPI_OFFLOAD @Default:# 0
20220327 015824.557 INFO             PET3 index=  99                        MPIR_CVAR_ENABLE_GPU_BUFFER_CHECKING : Turn on/off bounds checking of the offloaded buffers.$ @Alias:# I_MPI_OFFLOAD_BUFFER_CHECKING @Default:# 1
20220327 015824.557 INFO             PET3 index= 100                        MPIR_CVAR_OFFLOAD_LEVEL_ZERO_LIBRARY : Specify name or full path to Level Zero ze_loader library.$ @Alias:# I_MPI_OFFLOAD_LEVEL_ZERO_LIBRARY
20220327 015824.557 INFO             PET3 index= 101                            MPIR_CVAR_INTEL_EXTRA_FILESYSTEM : Turn on/off native parallel file systems support.$ Syntax$ I_MPI_EXTRA_FILESYSTEM=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 - Enable native support for parallel file$ systems.$ > disable | no | off | 0 - Disable native support for parallel file$ systems.$ ----------------------------------------------------------------------- @Alias:# I_MPI_EXTRA_FILESYSTEM @Default:# off
20220327 015824.557 INFO             PET3 index= 102                      MPIR_CVAR_INTEL_EXTRA_FILESYSTEM_FORCE : Force filesystem recognition logic.$ Syntax$ I_MPI_EXTRA_FILESYSTEM_FORCE=<ufs|nfs|gpfs|panfs|lustre|daos>$ @Alias:# I_MPI_EXTRA_FILESYSTEM_FORCE @Default:# not defined
20220327 015824.557 INFO             PET3 index= 103                                     MPIR_CVAR_INTEL_FABRICS : Select the particular fabrics to be used.$ Syntax$ I_MPI_FABRICS=<ofi|shm:ofi>$ Arguments$ <fabric> -  Define a network fabric.$ -----------------------------------------------------------------------$ > shm - Shared memory transport (used for intra-node$ communication only).$ > ofi - OpenFabrics Interfaces* (OFI)-capable network fabrics, such as$ Intel(R) True Scale Fabric, Intel(R) Omni-Path Architecture, InfiniBand*$ and Ethernet (through OFI$ API).$ ----------------------------------------------------------------------- @Alias:# I_MPI_FABRICS @Default:# shm:ofi
20220327 015824.557 INFO             PET3 index= 104                                       MPIR_CVAR_IMPI_MALLOC : Enable or disable the Intel MPI private memory allocator.$ ----------------------------------------------------------------------- @Alias:# I_MPI_MALLOC @Default:# 1
20220327 015824.557 INFO             PET3 index= 105                                    MPIR_CVAR_INTEL_SHM_HEAP : Enable or disable the Intel MPI shared memory allocator.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP @Default:# -1
20220327 015824.557 INFO             PET3 index= 106                                MPIR_CVAR_INTEL_SHM_HEAP_OPT : Shared memory heap optimization: "rank", "numa".$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_OPT @Default:# -1
20220327 015824.557 INFO             PET3 index= 107                              MPIR_CVAR_INTEL_SHM_HEAP_VSIZE : Set shared memory heap virtual size (in MB).$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_VSIZE @Default:# -1
20220327 015824.557 INFO             PET3 index= 108                              MPIR_CVAR_INTEL_SHM_HEAP_CSIZE : Set shared memory heap cache size (in MB).$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_CSIZE @Default:# -1
20220327 015824.557 INFO             PET3 index= 109                  MPIR_CVAR_INTEL_SHM_HEAP_NCONTIG_THRESHOLD : Set non-contig object size threshold for use shared memory allocator.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_NCONTIG_THRESHOLD @Default:# -1
20220327 015824.557 INFO             PET3 index= 110                          MPIR_CVAR_INTEL_SHM_HEAP_THRESHOLD : Set object size threshold for use shared memory allocator.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_THRESHOLD @Default:# -1
20220327 015824.557 INFO             PET3 index= 111        MPIR_CVAR_INTEL_SHM_RECV_HEAP_SHORT_MEMCPY_THRESHOLD : Threshold for short size messages receive via SHM HEAP transport.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_HEAP_SHORT_MEMCPY_THRESHOLD @Default:# -1
20220327 015824.557 INFO             PET3 index= 112      MPIR_CVAR_INTEL_SHM_RECV_HEAP_REGULAR_MEMCPY_THRESHOLD : Threshold for regular size message receive via SHM HEAP transport.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_HEAP_REGULAR_MEMCPY_THRESHOLD @Default:# -1
20220327 015824.557 INFO             PET3 index= 113            MPIR_CVAR_INTEL_SHM_RECV_INTER_HEAP_SHORT_MEMCPY : Name of memory copy function for short message receive via SHM HEAP transport, inter socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTER_HEAP_SHORT_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET3 index= 114            MPIR_CVAR_INTEL_SHM_RECV_INTRA_HEAP_SHORT_MEMCPY : Name of memory copy function for short message receive via SHM HEAP transport, intra socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTRA_HEAP_SHORT_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET3 index= 115          MPIR_CVAR_INTEL_SHM_RECV_INTER_HEAP_REGULAR_MEMCPY : Name of memory copy function for regular receive messages via SHM HEAP transport, inter socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTER_HEAP_REGULAR_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET3 index= 116          MPIR_CVAR_INTEL_SHM_RECV_INTRA_HEAP_REGULAR_MEMCPY : Name of memory copy function for regular receive messages via SHM HEAP transport, intra socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTRA_HEAP_REGULAR_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET3 index= 117                  MPIR_CVAR_INTEL_SHM_RECV_INTER_HEAP_MEMCPY : Name of memory copy function for receive messages via SHM HEAP transport, inter socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTER_HEAP_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET3 index= 118                  MPIR_CVAR_INTEL_SHM_RECV_INTRA_HEAP_MEMCPY : Name of memory copy function for receive messages via SHM HEAP transport, intra socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTRA_HEAP_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET3 index= 119                               MPIR_CVAR_CH4_SHM_POSIX_EAGER : Select a shared memory transport to be used.$ Syntax$ I_MPI_SHM=<transport>$ Arguments$ <transport> - Define a shared memory transport solution.$ -----------------------------------------------------------------------$ > disable | no | off | 0 - Do not use shared memory transport.$ > auto - Select a shared memory transport solution automatically.$ > bdw_sse - The shared memory transport solution tuned for Intel(R)$ microarchitecture code name Broadwell. The SSE/SSE2/SSE3 instruction$ set is used.$ > bdw_avx2 - The shared memory transport solution tuned for Intel(R)$ microarchitecture code name Broadwell. The AVX2 instruction set is used.$ > skx_sse - The shared memory transport solution tuned for Intel(R)$ Xeon(R) processors based on Intel(R) microarchitecture code name Skylake.$ The SSE/SSE2/SSE3 instruction set is used.$ > skx_avx2 - The shared memory transport solution tuned for Intel(R)$ Xeon(R) processors based on Intel(R) microarchitecture code name Skylake.$ The AVX2 instruction set is used.$ > skx_av
20220327 015824.557 INFO             PET3 index= 120                                     MPIR_CVAR_INTEL_SHM_OPT : Select a shared memory transport optimization strategy to be used.$ Syntax$ I_MPI_SHM_OPT=<optimization_strategy>$ Arguments$ <optimization_strategy> - Define a shared memory transport optimization strategy.$ -----------------------------------------------------------------------$ > dynamic - Let shared memory transport make decision in runtime.$ > intra - Optimize intra socket message passing.$ > inter - Optimize inter socket message passing.$ -----------------------------------------------------------------------$ @Alias:# I_MPI_SHM_OPT @Default:# dynamic
20220327 015824.557 INFO             PET3 index= 121                           MPIR_CVAR_INTEL_SHM_CELL_FWD_SIZE : Change the size of a shared memory forward cell. @Alias:# I_MPI_SHM_CELL_FWD_SIZE @Default:# -1
20220327 015824.557 INFO             PET3 index= 122                           MPIR_CVAR_INTEL_SHM_CELL_BWD_SIZE : Change the size of a shared memory backward cell. @Alias:# I_MPI_SHM_CELL_BWD_SIZE @Default:# -1
20220327 015824.557 INFO             PET3 index= 123                           MPIR_CVAR_INTEL_SHM_CELL_EXT_SIZE : Change the size of a shared memory extended cell. @Alias:# I_MPI_SHM_CELL_EXT_SIZE @Default:# -1
20220327 015824.557 INFO             PET3 index= 124                            MPIR_CVAR_INTEL_SHM_CELL_FWD_NUM : Change the number of forward cells in the shared memory transport (per rank). @Alias:# I_MPI_SHM_CELL_FWD_NUM @Default:# -1
20220327 015824.557 INFO             PET3 index= 125                       MPIR_CVAR_INTEL_SHM_CELL_FWD_HOLD_NUM : Change the number of hold forward cells in the shared memory transport (per rank). @Alias:# I_MPI_SHM_CELL_FWD_HOLD_NUM @Default:# -1
20220327 015824.557 INFO             PET3 index= 126                            MPIR_CVAR_INTEL_SHM_CELL_BWD_NUM : Change the number of backward cells in the shared memory transport (per rank). @Alias:# I_MPI_SHM_CELL_BWD_NUM @Default:# -1
20220327 015824.558 INFO             PET3 index= 127                     MPIR_CVAR_INTEL_SHM_CELL_BWD_NUMA_AWARE : Use NUMA aware backward cells (1 : true, 0 : false, -1 : auto) (per rank). @Alias:# I_MPI_SHM_CELL_BWD_NUMA_AWARE @Default:# -1
20220327 015824.558 INFO             PET3 index= 128                      MPIR_CVAR_INTEL_SHM_CELL_EXT_NUM_TOTAL : Change the total number of extended cells in the shared memory$ transport. @Alias:# I_MPI_SHM_CELL_EXT_NUM_TOTAL @Default:# -1
20220327 015824.558 INFO             PET3 index= 129                            MPIR_CVAR_INTEL_SHM_MCDRAM_LIMIT : Change the size of the shared memory bound to the multi-channel DRAM (MCDRAM) (size per rank). @Alias:# I_MPI_SHM_MCDRAM_LIMIT @Default:# -1
20220327 015824.558 INFO             PET3 index= 130                         MPIR_CVAR_INTEL_SHM_SEND_SPIN_COUNT : Control the spin count value for the shared memory transport for sending messages. @Alias:# I_MPI_SHM_SEND_SPIN_COUNT @Default:# -1
20220327 015824.558 INFO             PET3 index= 131                         MPIR_CVAR_INTEL_SHM_RECV_SPIN_COUNT : Control the spin count value for the shared memory transport for receiving messages. @Alias:# I_MPI_SHM_RECV_SPIN_COUNT @Default:# -1
20220327 015824.558 INFO             PET3 index= 132                          MPIR_CVAR_INTEL_SHM_FILE_PREFIX_4K : Mount point of 4K page size shm files. @Alias:# I_MPI_SHM_FILE_PREFIX_4K @Default:# ""
20220327 015824.558 INFO             PET3 index= 133                          MPIR_CVAR_INTEL_SHM_FILE_PREFIX_2M : Mount point of 2M huge page size shm files. @Alias:# I_MPI_SHM_FILE_PREFIX_2M @Default:# ""
20220327 015824.558 INFO             PET3 index= 134                          MPIR_CVAR_INTEL_SHM_FILE_PREFIX_1G : Mount point of 1G huge page size shm files. @Alias:# I_MPI_SHM_FILE_PREFIX_1G @Default:# ""
20220327 015824.558 INFO             PET3 index= 135                                   MPIR_CVAR_INTEL_SHM_PAUSE : The number of pauses repeated. @Alias:# I_MPI_SHM_PAUSE
20220327 015824.558 INFO             PET3 index= 136                         MPIR_CVAR_INTEL_SHM_EAGER_THRESHOLD : Eager threshold. @Alias:# I_MPI_SHM_EAGER_THRESHOLD
20220327 015824.558 INFO             PET3 index= 137                               MPIR_CVAR_INTEL_SHM_RING_SIZE : @Alias:# I_MPI_SHM_RING_SIZE
20220327 015824.558 INFO             PET3 index= 138                      MPIR_CVAR_INTEL_SHM_RING_ACK_THRESHOLD : @Alias:# I_MPI_SHM_RING_ACK_THRESHOLD
20220327 015824.558 INFO             PET3 index= 139                          MPIR_CVAR_INTEL_SHM_CELL_FWD_FIRST : @Alias:# I_MPI_SHM_CELL_FWD_FIRST
20220327 015824.558 INFO             PET3 index= 140                      MPIR_CVAR_INTEL_SHM_PROFILER_DIRECTORY : @Alias:# I_MPI_SHM_PROFILER_DIRECTORY
20220327 015824.558 INFO             PET3 index= 141                         MPIR_CVAR_INTEL_SHM_TRACE_DIRECTORY : @Alias:# I_MPI_SHM_TRACE_DIRECTORY
20220327 015824.558 INFO             PET3 index= 142                              MPIR_CVAR_INTEL_SHM_FRAME_SIZE : @Alias:# I_MPI_SHM_FRAME_SIZE
20220327 015824.558 INFO             PET3 index= 143                         MPIR_CVAR_INTEL_SHM_FRAME_THRESHOLD : @Alias:# I_MPI_SHM_FRAME_THRESHOLD
20220327 015824.558 INFO             PET3 index= 144                        MPIR_CVAR_INTEL_SHM_SEND_TINY_MEMCPY : @Alias:# I_MPI_SHM_SEND_TINY_MEMCPY
20220327 015824.558 INFO             PET3 index= 145                  MPIR_CVAR_INTEL_SHM_SEND_INTRA_RING_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_RING_MEMCPY
20220327 015824.558 INFO             PET3 index= 146                  MPIR_CVAR_INTEL_SHM_SEND_INTER_RING_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_RING_MEMCPY
20220327 015824.558 INFO             PET3 index= 147                  MPIR_CVAR_INTEL_SHM_RECV_INTRA_RING_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_RING_MEMCPY
20220327 015824.558 INFO             PET3 index= 148                  MPIR_CVAR_INTEL_SHM_RECV_INTER_RING_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_RING_MEMCPY
20220327 015824.558 INFO             PET3 index= 149              MPIR_CVAR_INTEL_SHM_SEND_INTRA_CELL_FWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_CELL_FWD_MEMCPY
20220327 015824.558 INFO             PET3 index= 150              MPIR_CVAR_INTEL_SHM_SEND_INTER_CELL_FWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_CELL_FWD_MEMCPY
20220327 015824.558 INFO             PET3 index= 151              MPIR_CVAR_INTEL_SHM_SEND_INTRA_CELL_BWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_CELL_BWD_MEMCPY
20220327 015824.558 INFO             PET3 index= 152              MPIR_CVAR_INTEL_SHM_SEND_INTER_CELL_BWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_CELL_BWD_MEMCPY
20220327 015824.558 INFO             PET3 index= 153                  MPIR_CVAR_INTEL_SHM_RECV_INTRA_CELL_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_CELL_MEMCPY
20220327 015824.558 INFO             PET3 index= 154                  MPIR_CVAR_INTEL_SHM_RECV_INTER_CELL_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_CELL_MEMCPY
20220327 015824.558 INFO             PET3 index= 155          MPIR_CVAR_INTEL_SHM_RECV_INTRA_CELL_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_CELL_REGULAR_MEMCPY
20220327 015824.558 INFO             PET3 index= 156          MPIR_CVAR_INTEL_SHM_RECV_INTER_CELL_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_CELL_REGULAR_MEMCPY
20220327 015824.558 INFO             PET3 index= 157             MPIR_CVAR_INTEL_SHM_SEND_INTRA_FRAME_FWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_FRAME_FWD_MEMCPY
20220327 015824.558 INFO             PET3 index= 158             MPIR_CVAR_INTEL_SHM_SEND_INTER_FRAME_FWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_FRAME_FWD_MEMCPY
20220327 015824.558 INFO             PET3 index= 159             MPIR_CVAR_INTEL_SHM_SEND_INTRA_FRAME_BWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_FRAME_BWD_MEMCPY
20220327 015824.558 INFO             PET3 index= 160             MPIR_CVAR_INTEL_SHM_SEND_INTER_FRAME_BWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_FRAME_BWD_MEMCPY
20220327 015824.558 INFO             PET3 index= 161                 MPIR_CVAR_INTEL_SHM_RECV_INTRA_FRAME_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_FRAME_MEMCPY
20220327 015824.558 INFO             PET3 index= 162                 MPIR_CVAR_INTEL_SHM_RECV_INTER_FRAME_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_FRAME_MEMCPY
20220327 015824.558 INFO             PET3 index= 163     MPIR_CVAR_INTEL_SHM_SEND_INTRA_FRAME_FWD_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_FRAME_FWD_REGULAR_MEMCPY
20220327 015824.558 INFO             PET3 index= 164     MPIR_CVAR_INTEL_SHM_SEND_INTER_FRAME_FWD_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_FRAME_FWD_REGULAR_MEMCPY
20220327 015824.558 INFO             PET3 index= 165     MPIR_CVAR_INTEL_SHM_SEND_INTRA_FRAME_BWD_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_FRAME_BWD_REGULAR_MEMCPY
20220327 015824.558 INFO             PET3 index= 166     MPIR_CVAR_INTEL_SHM_SEND_INTER_FRAME_BWD_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_FRAME_BWD_REGULAR_MEMCPY
20220327 015824.558 INFO             PET3 index= 167         MPIR_CVAR_INTEL_SHM_RECV_INTRA_FRAME_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_FRAME_REGULAR_MEMCPY
20220327 015824.558 INFO             PET3 index= 168         MPIR_CVAR_INTEL_SHM_RECV_INTER_FRAME_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_FRAME_REGULAR_MEMCPY
20220327 015824.558 INFO             PET3 index= 169                       MPIR_CVAR_INTEL_SHM_INPLACE_THRESHOLD : @Alias:# I_MPI_SHM_INPLACE_THRESHOLD
20220327 015824.558 INFO             PET3 index= 170              MPIR_CVAR_INTEL_SHM_SEND_TINY_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_TINY_MEMCPY_THRESHOLD
20220327 015824.558 INFO             PET3 index= 171      MPIR_CVAR_INTEL_SHM_RECV_CELL_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_RECV_CELL_REGULAR_MEMCPY_THRESHOLD
20220327 015824.558 INFO             PET3 index= 172MPIR_CVAR_INTEL_SHM_SEND_INTER_UNIDIR_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_INTER_UNIDIR_REGULAR_MEMCPY_THRESHOLD
20220327 015824.558 INFO             PET3 index= 173MPIR_CVAR_INTEL_SHM_SEND_INTER_BIDIR_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_INTER_BIDIR_REGULAR_MEMCPY_THRESHOLD
20220327 015824.558 INFO             PET3 index= 174MPIR_CVAR_INTEL_SHM_SEND_INTRA_UNIDIR_REGULAR_MEMCPY_THRESHOLD_MIN : @Alias:# I_MPI_SHM_SEND_INTRA_UNIDIR_REGULAR_MEMCPY_THRESHOLD_MIN
20220327 015824.558 INFO             PET3 index= 175MPIR_CVAR_INTEL_SHM_SEND_INTRA_UNIDIR_REGULAR_MEMCPY_THRESHOLD_MAX : @Alias:# I_MPI_SHM_SEND_INTRA_UNIDIR_REGULAR_MEMCPY_THRESHOLD_MAX
20220327 015824.558 INFO             PET3 index= 176MPIR_CVAR_INTEL_SHM_SEND_INTRA_BIDIR_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_INTRA_BIDIR_REGULAR_MEMCPY_THRESHOLD
20220327 015824.558 INFO             PET3 index= 177     MPIR_CVAR_INTEL_SHM_RECV_FRAME_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_RECV_FRAME_REGULAR_MEMCPY_THRESHOLD
20220327 015824.558 INFO             PET3 index= 178          MPIR_CVAR_INTEL_SHM_CELL_FWD_UNIDIR_INTRA_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_UNIDIR_INTRA_CAPACITY
20220327 015824.558 INFO             PET3 index= 179  MPIR_CVAR_INTEL_SHM_CELL_FWD_UNIDIR_INTER_REGULAR_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_UNIDIR_INTER_REGULAR_CAPACITY
20220327 015824.558 INFO             PET3 index= 180MPIR_CVAR_INTEL_SHM_CELL_FWD_UNIDIR_INTER_NONTEMPORAL_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_UNIDIR_INTER_NONTEMPORAL_CAPACITY
20220327 015824.558 INFO             PET3 index= 181           MPIR_CVAR_INTEL_SHM_CELL_FWD_BIDIR_INTRA_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_BIDIR_INTRA_CAPACITY
20220327 015824.558 INFO             PET3 index= 182   MPIR_CVAR_INTEL_SHM_CELL_FWD_BIDIR_INTER_REGULAR_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_BIDIR_INTER_REGULAR_CAPACITY
20220327 015824.558 INFO             PET3 index= 183MPIR_CVAR_INTEL_SHM_CELL_FWD_BIDIR_INTER_NONTEMPORAL_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_BIDIR_INTER_NONTEMPORAL_CAPACITY
20220327 015824.558 INFO             PET3 index= 184MPIR_CVAR_INTEL_SHM_SEND_FWD_FRAME_NONTEMPORAL_MEMCPY_THRESHOLD_MIN : @Alias:# I_MPI_SHM_SEND_FWD_FRAME_NONTEMPORAL_MEMCPY_THRESHOLD_MIN
20220327 015824.558 INFO             PET3 index= 185MPIR_CVAR_INTEL_SHM_SEND_FWD_FRAME_NONTEMPORAL_MEMCPY_THRESHOLD_MAX : @Alias:# I_MPI_SHM_SEND_FWD_FRAME_NONTEMPORAL_MEMCPY_THRESHOLD_MAX
20220327 015824.558 INFO             PET3 index= 186MPIR_CVAR_INTEL_SHM_SEND_FWD_CELL_NONTEMPORAL_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_FWD_CELL_NONTEMPORAL_MEMCPY_THRESHOLD
20220327 015824.558 INFO             PET3 index= 187                                MPIR_CVAR_INTEL_SHM_HEAP_THP : @Alias:# I_MPI_SHM_HEAP_THP
20220327 015824.558 INFO             PET3 index= 188                                     MPIR_CVAR_INTEL_SHM_THP : @Alias:# I_MPI_SHM_THP
20220327 015824.558 INFO             PET3 index= 189                                  MPIR_CVAR_OFI_USE_PROVIDER : Define the name of the OFI provider to load.$ Syntax$ I_MPI_OFI_PROVIDER=<name>$ Arguments$ <name> - The name of the OFI provider to load @Alias:# I_MPI_OFI_PROVIDER @Default:# not defined
20220327 015824.558 INFO             PET3 index= 190                                MPIR_CVAR_OFI_DUMP_PROVIDERS : Control the capability of printing information about all OFI providers$ and their attributes from an OFI library.$ Syntax$ I_MPI_OFI_PROVIDER_DUMP=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > yes | on | 1 - Print the list of all OFI providers and their$ attributes from an OFI library$ > no | off | 0 - No action$ ----------------------------------------------------------------------- @Alias:# I_MPI_OFI_PROVIDER_DUMP @Default:# off
20220327 015824.558 INFO             PET3 index= 191                        MPIR_CVAR_CH4_OFI_ENABLE_DIRECT_RECV : Control the capability of the direct receive in the OFI fabric.$ Syntax$ I_MPI_OFI_DRECV=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > 1 - Enable direct receive$ > 0 - Disable direct receive$ ----------------------------------------------------------------------- @Alias:# I_MPI_OFI_DRECV @Default:# not defined
20220327 015824.558 INFO             PET3 index= 192                                  MPIR_CVAR_OFI_MAX_MSG_SIZE : @Alias:# I_MPI_OFI_MAX_MSG_SIZE
20220327 015824.558 INFO             PET3 index= 193                                  MPIR_CVAR_OFI_LMT_WIN_SIZE : @Alias:# I_MPI_OFI_LMT_WIN_SIZE
20220327 015824.558 INFO             PET3 index= 194                    MPIR_CVAR_CH4_OFI_ISEND_INJECT_THRESHOLD : @Alias:# I_MPI_OFI_ISEND_INJECT_THRESHOLD
20220327 015824.558 INFO             PET3 index= 195                     MPIR_CVAR_CH4_OFI_ADDRESS_EXCHANGE_MODE : @Alias:# I_MPI_STARTUP_MODE
20220327 015824.558 INFO             PET3 index= 196                     MPIR_CVAR_CH4_OFI_LARGE_SCALE_THRESHOLD : @Alias:# I_MPI_LARGE_SCALE_THRESHOLD
20220327 015824.558 INFO             PET3 index= 197                   MPIR_CVAR_CH4_OFI_EXTREME_SCALE_THRESHOLD : @Alias:# I_MPI_EXTREME_SCALE_THRESHOLD
20220327 015824.558 INFO             PET3 index= 198                        MPIR_CVAR_CH4_OFI_DYNAMIC_CONNECTION : @Alias:# I_MPI_DYNAMIC_CONNECTION
20220327 015824.558 INFO             PET3 index= 199                              MPIR_CVAR_CH4_OFI_EXPERIMENTAL : @Alias:# I_MPI_OFI_EXPERIMENTAL @Default:# 0
20220327 015824.558 INFO             PET3 index= 200                     MPIR_CVAR_CH4_OFI_CAPABILITY_SETS_DEBUG : Prints out the configuration of each capability selected via the capability sets interface.
20220327 015824.558 INFO             PET3 index= 201                               MPIR_CVAR_CH4_OFI_ENABLE_DATA : Enable immediate data fields in OFI to transmit source rank outside of the match bits
20220327 015824.558 INFO             PET3 index= 202                           MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE : If true, the OFI addressing information will be stored with an FI_AV_TABLE. If false, an FI_AV_MAP will be used.
20220327 015824.558 INFO             PET3 index= 203                 MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS : If true, use OFI scalable endpoints.
20220327 015824.559 INFO             PET3 index= 204                             MPIR_CVAR_CH4_OFI_MAX_ENDPOINTS : Specifies the maximum number of OFI endpoints that can be used by the OFI provider. The default value is -1, indicating that no value is set.
20220327 015824.559 INFO             PET3 index= 205                        MPIR_CVAR_CH4_OFI_ENABLE_MR_SCALABLE : If true, MR_SCALABLE for OFI memory regions. If false, MR_BASIC for OFI memory regions.
20220327 015824.559 INFO             PET3 index= 206                             MPIR_CVAR_CH4_OFI_ENABLE_TAGGED : If true, use tagged message transmission functions in OFI.
20220327 015824.559 INFO             PET3 index= 207                                 MPIR_CVAR_CH4_OFI_ENABLE_AM : If true, enable OFI active message support.
20220327 015824.559 INFO             PET3 index= 208                                MPIR_CVAR_CH4_OFI_ENABLE_RMA : If true, enable OFI RMA support for MPI RMA operations. OFI support for basic RMA is always required to implement large messgage transfers in the active message code path.
20220327 015824.559 INFO             PET3 index= 209                            MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS : If true, enable OFI Atomics support.
20220327 015824.559 INFO             PET3 index= 210                       MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS : Specifies the maximum number of iovecs that can be used by the OFI provider for fetch_atomic operations. The default value is -1, indicating that no value is set.
20220327 015824.559 INFO             PET3 index= 211                 MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS : If true, enable MPI data auto progress.
20220327 015824.559 INFO             PET3 index= 212              MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS : If true, enable MPI control auto progress.
20220327 015824.559 INFO             PET3 index= 213                       MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK : If true, enable iovec for pt2pt.
20220327 015824.559 INFO             PET3 index= 214                                 MPIR_CVAR_CH4_OFI_RANK_BITS : Specifies the number of bits that will be used for matching the MPI rank. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20220327 015824.559 INFO             PET3 index= 215                                  MPIR_CVAR_CH4_OFI_TAG_BITS : Specifies the number of bits that will be used for matching the user tag. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20220327 015824.559 INFO             PET3 index= 216                             MPIR_CVAR_CH4_OFI_MAJOR_VERSION : Specifies the major version of the OFI library. The default is the major version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20220327 015824.559 INFO             PET3 index= 217                             MPIR_CVAR_CH4_OFI_MINOR_VERSION : Specifies the major version of the OFI library. The default is the minor version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20220327 015824.559 INFO             PET3 index= 218                             MPIR_CVAR_CH4_OFI_ZERO_OP_FLAGS : Zeroes rx_attr.op_flags and tx_attr.op_flags, disables use of FI_SELECTIVE_COMPLETION. Can give more optimized behavior of underlying provider.
20220327 015824.559 INFO             PET3 index= 219                            MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS : Specifies the number of buffers for receiving active messages.
20220327 015824.559 INFO             PET3 index= 220                        MPIR_CVAR_INTEL_COLL_DIRECT_PROGRESS : @Alias:# I_MPI_COLL_DIRECT_PROGRESS
20220327 015824.559 INFO             PET3 index= 221                                      MPIR_CVAR_ENABLE_HCOLL : @Alias:# I_MPI_COLL_EXTERNAL
20220327 015824.559 INFO             PET3 index= 222                                     MPIR_CVAR_USE_ALLGATHER : Control selection of MPI_Allgather algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-6 @Alias:# I_MPI_ADJUST_ALLGATHER @Default:# -1
20220327 015824.559 INFO             PET3 index= 223                                MPIR_CVAR_USE_ALLGATHER_LIST : @Alias:# I_MPI_ADJUST_ALLGATHER_LIST @Default:# -1
20220327 015824.559 INFO             PET3 index= 224                         MPIR_CVAR_USE_ALLGATHER_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLGATHER_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET3 index= 225               MPIR_CVAR_ALLGATHER_COMPOSITION_DELTA_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLGATHER_COMPOSITION_DELTA_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET3 index= 226                             MPIR_CVAR_USE_ALLGATHER_NETWORK : range: 0-5 @Alias:# I_MPI_ADJUST_ALLGATHER_NETWORK @Default:# -1
20220327 015824.559 INFO             PET3 index= 227                                MPIR_CVAR_USE_ALLGATHER_NODE : range: 0-4 @Alias:# I_MPI_ADJUST_ALLGATHER_NODE @Default:# -1
20220327 015824.559 INFO             PET3 index= 228                                    MPIR_CVAR_USE_ALLGATHERV : Control selection of MPI_Allgatherv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_ALLGATHERV @Default:# -1
20220327 015824.559 INFO             PET3 index= 229                               MPIR_CVAR_USE_ALLGATHERV_LIST : @Alias:# I_MPI_ADJUST_ALLGATHERV_LIST @Default:# -1
20220327 015824.559 INFO             PET3 index= 230                        MPIR_CVAR_USE_ALLGATHERV_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLGATHERV_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET3 index= 231                            MPIR_CVAR_USE_ALLGATHERV_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_ALLGATHERV_NETWORK @Default:# -1
20220327 015824.559 INFO             PET3 index= 232                               MPIR_CVAR_USE_ALLGATHERV_NODE : range: 0-3 @Alias:# I_MPI_ADJUST_ALLGATHERV_NODE @Default:# -1
20220327 015824.559 INFO             PET3 index= 233                   MPIR_CVAR_ALLGATHER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ALLGATHER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET3 index= 234                      MPIR_CVAR_ALLGATHER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ALLGATHER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET3 index= 235                    MPIR_CVAR_SCATTERV_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_SCATTERV_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET3 index= 236                       MPIR_CVAR_SCATTERV_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_SCATTERV_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET3 index= 237                     MPIR_CVAR_SCATTER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_SCATTER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET3 index= 238                        MPIR_CVAR_SCATTER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_SCATTER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET3 index= 239                                     MPIR_CVAR_USE_ALLREDUCE : Control selection of MPI_Allreduce algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-25 @Alias:# I_MPI_ADJUST_ALLREDUCE @Default:# -1
20220327 015824.559 INFO             PET3 index= 240                                MPIR_CVAR_USE_ALLREDUCE_LIST : @Alias:# I_MPI_ADJUST_ALLREDUCE_LIST @Default:# -1
20220327 015824.559 INFO             PET3 index= 241                         MPIR_CVAR_USE_ALLREDUCE_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLREDUCE_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET3 index= 242                             MPIR_CVAR_USE_ALLREDUCE_NETWORK : range: 0-16 @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK @Default:# -1
20220327 015824.559 INFO             PET3 index= 243                                MPIR_CVAR_USE_ALLREDUCE_NODE : range: 0-9 @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE @Default:# -1
20220327 015824.559 INFO             PET3 index= 244               MPIR_CVAR_ALLREDUCE_NETWORK_MULTIPLYING_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_MULTIPLYING_RADIX @Default:# -1
20220327 015824.559 INFO             PET3 index= 245                  MPIR_CVAR_ALLREDUCE_NODE_MULTIPLYING_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_MULTIPLYING_RADIX @Default:# -1
20220327 015824.559 INFO             PET3 index= 246                   MPIR_CVAR_ALLREDUCE_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET3 index= 247                      MPIR_CVAR_ALLREDUCE_NETWORK_KARY_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_KARY_RADIX @Default:# -1
20220327 015824.559 INFO             PET3 index= 248                      MPIR_CVAR_ALLREDUCE_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET3 index= 249                         MPIR_CVAR_ALLREDUCE_NODE_KARY_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_KARY_RADIX @Default:# -1
20220327 015824.559 INFO             PET3 index= 250                    MPIR_CVAR_ALLREDUCE_NODE_SHM_GATHER_TYPE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHM_GATHER_TYPE @Default:# -1
20220327 015824.559 INFO             PET3 index= 251                   MPIR_CVAR_ALLREDUCE_NODE_SHM_GATHER_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHM_GATHER_RADIX @Default:# -1
20220327 015824.559 INFO             PET3 index= 252                   MPIR_CVAR_ALLREDUCE_NODE_SHM_RELEASE_TYPE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHM_RELEASE_TYPE @Default:# -1
20220327 015824.559 INFO             PET3 index= 253                  MPIR_CVAR_ALLREDUCE_NODE_SHM_RELEASE_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHM_RELEASE_RADIX @Default:# -1
20220327 015824.559 INFO             PET3 index= 254                MPIR_CVAR_ALLREDUCE_NETWORK_SHUMILIN_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_SHUMILIN_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET3 index= 255                   MPIR_CVAR_ALLREDUCE_NODE_SHUMILIN_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHUMILIN_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET3 index= 256                              MPIR_CVAR_ALLREDUCE_ZETA_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_ZETA_RADIX @Default:# -1
20220327 015824.559 INFO             PET3 index= 257                           MPIR_CVAR_ALLREDUCE_ZETA_SHM_TYPE : @Alias:# I_MPI_ADJUST_ALLREDUCE_ZETA_SHM_TYPE @Default:# -1
20220327 015824.559 INFO             PET3 index= 258                              MPIR_CVAR_ALLREDUCE_IOTA_NLEAD : @Alias:# I_MPI_ADJUST_ALLREDUCE_IOTA_NLEAD @Default:# -1
20220327 015824.559 INFO             PET3 index= 259               MPIR_CVAR_ALLREDUCE_NODE_NREDUCE_NB_ALLGATHER : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_NREDUCE_NB_ALLGATHER @Default:# -1
20220327 015824.559 INFO             PET3 index= 260            MPIR_CVAR_ALLREDUCE_NETWORK_NREDUCE_NB_ALLGATHER : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_NREDUCE_NB_ALLGATHER @Default:# -1
20220327 015824.559 INFO             PET3 index= 261                MPIR_CVAR_ALLREDUCE_NODE_NREDUCE_NB_ALLTOALL : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_NREDUCE_NB_ALLTOALL @Default:# -1
20220327 015824.559 INFO             PET3 index= 262             MPIR_CVAR_ALLREDUCE_NETWORK_NREDUCE_NB_ALLTOALL : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_NREDUCE_NB_ALLTOALL @Default:# -1
20220327 015824.559 INFO             PET3 index= 263                    MPIR_CVAR_ALLREDUCE_NODE_NREDUCE_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_NREDUCE_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET3 index= 264                 MPIR_CVAR_ALLREDUCE_NETWORK_NREDUCE_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_NREDUCE_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET3 index= 265                                      MPIR_CVAR_USE_ALLTOALL : Control selection of MPI_Alltoall algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-13 @Alias:# I_MPI_ADJUST_ALLTOALL @Default:# -1
20220327 015824.559 INFO             PET3 index= 266                                 MPIR_CVAR_USE_ALLTOALL_LIST : @Alias:# I_MPI_ADJUST_ALLTOALL_LIST @Default:# -1
20220327 015824.559 INFO             PET3 index= 267                          MPIR_CVAR_USE_ALLTOALL_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLTOALL_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET3 index= 268                              MPIR_CVAR_USE_ALLTOALL_NETWORK : range: 0-7 @Alias:# I_MPI_ADJUST_ALLTOALL_NETWORK @Default:# -1
20220327 015824.559 INFO             PET3 index= 269                                 MPIR_CVAR_USE_ALLTOALL_NODE : range: 0-6 @Alias:# I_MPI_ADJUST_ALLTOALL_NODE @Default:# -1
20220327 015824.559 INFO             PET3 index= 270                MPIR_CVAR_ALLTOALL_COMPOSITION_GAMMA_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLTOALL_COMPOSITION_GAMMA_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET3 index= 271                                 MPIR_CVAR_ALLTOALL_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALL_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET3 index= 272               MPIR_CVAR_ALLTOALL_NETWORK_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALL_NETWORK_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET3 index= 273                  MPIR_CVAR_ALLTOALL_NODE_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALL_NODE_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET3 index= 274                             MPIR_CVAR_ALLTOALL_BRUCKS_RADIX : @Alias:# I_MPI_ADJUST_ALLTOALL_BRUCKS_RADIX @Default:# -1
20220327 015824.559 INFO             PET3 index= 275                     MPIR_CVAR_ALLTOALL_NETWORK_BRUCKS_RADIX : @Alias:# I_MPI_ADJUST_ALLTOALL_NETWORK_BRUCKS_RADIX @Default:# -1
20220327 015824.559 INFO             PET3 index= 276                        MPIR_CVAR_ALLTOALL_NODE_BRUCKS_RADIX : @Alias:# I_MPI_ADJUST_ALLTOALL_NODE_BRUCKS_RADIX @Default:# -1
20220327 015824.559 INFO             PET3 index= 277                                     MPIR_CVAR_USE_ALLTOALLV : Control selection of MPI_Alltoallv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-9 @Alias:# I_MPI_ADJUST_ALLTOALLV @Default:# -1
20220327 015824.559 INFO             PET3 index= 278                                MPIR_CVAR_USE_ALLTOALLV_LIST : @Alias:# I_MPI_ADJUST_ALLTOALLV_LIST @Default:# -1
20220327 015824.559 INFO             PET3 index= 279                         MPIR_CVAR_USE_ALLTOALLV_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLTOALLV_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET3 index= 280                             MPIR_CVAR_USE_ALLTOALLV_NETWORK : range: 0-6 @Alias:# I_MPI_ADJUST_ALLTOALLV_NETWORK @Default:# -1
20220327 015824.560 INFO             PET3 index= 281                                MPIR_CVAR_USE_ALLTOALLV_NODE : range: 0-5 @Alias:# I_MPI_ADJUST_ALLTOALLV_NODE @Default:# -1
20220327 015824.560 INFO             PET3 index= 282                                MPIR_CVAR_ALLTOALLV_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLV_SCATTERED_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET3 index= 283              MPIR_CVAR_ALLTOALLV_NETWORK_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLV_NETWORK_SCATTERED_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET3 index= 284                 MPIR_CVAR_ALLTOALLV_NODE_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLV_NODE_SCATTERED_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET3 index= 285                                     MPIR_CVAR_USE_ALLTOALLW : Control selection of MPI_Alltoallw algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-6 @Alias:# I_MPI_ADJUST_ALLTOALLW @Default:# -1
20220327 015824.560 INFO             PET3 index= 286                                MPIR_CVAR_USE_ALLTOALLW_LIST : @Alias:# I_MPI_ADJUST_ALLTOALLW_LIST @Default:# -1
20220327 015824.560 INFO             PET3 index= 287                         MPIR_CVAR_USE_ALLTOALLW_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLTOALLW_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET3 index= 288                             MPIR_CVAR_USE_ALLTOALLW_NETWORK : @Alias:# I_MPI_ADJUST_ALLTOALLW_NETWORK @Default:# -1
20220327 015824.560 INFO             PET3 index= 289                                MPIR_CVAR_USE_ALLTOALLW_NODE : @Alias:# I_MPI_ADJUST_ALLTOALLW_NODE @Default:# -1
20220327 015824.560 INFO             PET3 index= 290                                MPIR_CVAR_ALLTOALLW_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLW_SCATTERED_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET3 index= 291              MPIR_CVAR_ALLTOALLW_NETWORK_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLW_NETWORK_SCATTERED_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET3 index= 292                 MPIR_CVAR_ALLTOALLW_NODE_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLW_NODE_SCATTERED_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET3 index= 293                                       MPIR_CVAR_USE_BARRIER : Control selection of MPI_Barrier algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-11 @Alias:# I_MPI_ADJUST_BARRIER @Default:# -1
20220327 015824.560 INFO             PET3 index= 294                                  MPIR_CVAR_USE_BARRIER_LIST : @Alias:# I_MPI_ADJUST_BARRIER_LIST @Default:# -1
20220327 015824.560 INFO             PET3 index= 295                           MPIR_CVAR_USE_BARRIER_COMPOSITION : @Alias:# I_MPI_ADJUST_BARRIER_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET3 index= 296                               MPIR_CVAR_USE_BARRIER_NETWORK : range: 0-6 @Alias:# I_MPI_ADJUST_BARRIER_NETWORK @Default:# -1
20220327 015824.560 INFO             PET3 index= 297                                  MPIR_CVAR_USE_BARRIER_NODE : range: 0-4 @Alias:# I_MPI_ADJUST_BARRIER_NODE @Default:# -1
20220327 015824.560 INFO             PET3 index= 298                 MPIR_CVAR_BARRIER_NETWORK_MULTIPLYING_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NETWORK_MULTIPLYING_RADIX @Default:# -1
20220327 015824.560 INFO             PET3 index= 299                     MPIR_CVAR_BARRIER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET3 index= 300                        MPIR_CVAR_BARRIER_NETWORK_KARY_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NETWORK_KARY_RADIX @Default:# -1
20220327 015824.560 INFO             PET3 index= 301          MPIR_CVAR_BARRIER_NETWORK_RECURSIVE_EXCHANGE_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NETWORK_RECURSIVE_EXCHANGE_RADIX @Default:# -1
20220327 015824.560 INFO             PET3 index= 302                        MPIR_CVAR_BARRIER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET3 index= 303                           MPIR_CVAR_BARRIER_NODE_KARY_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_KARY_RADIX @Default:# -1
20220327 015824.560 INFO             PET3 index= 304             MPIR_CVAR_BARRIER_NODE_RECURSIVE_EXCHANGE_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_RECURSIVE_EXCHANGE_RADIX @Default:# -1
20220327 015824.560 INFO             PET3 index= 305                      MPIR_CVAR_BARRIER_NODE_SHM_GATHER_TYPE : @Alias:# I_MPI_ADJUST_BARRIER_NODE_SHM_GATHER_TYPE @Default:# -1
20220327 015824.560 INFO             PET3 index= 306                     MPIR_CVAR_BARRIER_NODE_SHM_GATHER_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_SHM_GATHER_RADIX @Default:# -1
20220327 015824.560 INFO             PET3 index= 307                     MPIR_CVAR_BARRIER_NODE_SHM_RELEASE_TYPE : @Alias:# I_MPI_ADJUST_BARRIER_NODE_SHM_RELEASE_TYPE @Default:# -1
20220327 015824.560 INFO             PET3 index= 308                    MPIR_CVAR_BARRIER_NODE_SHM_RELEASE_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_SHM_RELEASE_RADIX @Default:# -1
20220327 015824.560 INFO             PET3 index= 309                                MPIR_CVAR_BARRIER_ZETA_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_ZETA_RADIX @Default:# -1
20220327 015824.560 INFO             PET3 index= 310                             MPIR_CVAR_BARRIER_ZETA_SHM_TYPE : @Alias:# I_MPI_ADJUST_BARRIER_ZETA_SHM_TYPE @Default:# -1
20220327 015824.560 INFO             PET3 index= 311                                         MPIR_CVAR_USE_BCAST : Control selection of MPI_Bcast algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-18 @Alias:# I_MPI_ADJUST_BCAST @Default:# -1
20220327 015824.560 INFO             PET3 index= 312                                    MPIR_CVAR_USE_BCAST_LIST : @Alias:# I_MPI_ADJUST_BCAST_LIST @Default:# -1
20220327 015824.560 INFO             PET3 index= 313                             MPIR_CVAR_USE_BCAST_COMPOSITION : @Alias:# I_MPI_ADJUST_BCAST_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET3 index= 314                                 MPIR_CVAR_USE_BCAST_NETWORK : range: 0-13 @Alias:# I_MPI_ADJUST_BCAST_NETWORK @Default:# -1
20220327 015824.560 INFO             PET3 index= 315                                    MPIR_CVAR_USE_BCAST_NODE : range: 0-9 @Alias:# I_MPI_ADJUST_BCAST_NODE @Default:# -1
20220327 015824.560 INFO             PET3 index= 316                               MPIR_CVAR_BCAST_EPSILON_NRAIL : @Alias:# I_MPI_ADJUST_BCAST_EPSILON_NRAIL @Default:# -1
20220327 015824.560 INFO             PET3 index= 317                          MPIR_CVAR_BCAST_NETWORK_KARY_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_KARY_RADIX @Default:# -1
20220327 015824.560 INFO             PET3 index= 318                       MPIR_CVAR_BCAST_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET3 index= 319                        MPIR_CVAR_BCAST_NETWORK_KARY_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_KARY_SEGSIZE @Default:# -1
20220327 015824.560 INFO             PET3 index= 320                     MPIR_CVAR_BCAST_NETWORK_KNOMIAL_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_KNOMIAL_SEGSIZE @Default:# -1
20220327 015824.560 INFO             PET3 index= 321                             MPIR_CVAR_BCAST_NODE_KARY_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NODE_KARY_RADIX @Default:# -1
20220327 015824.560 INFO             PET3 index= 322                          MPIR_CVAR_BCAST_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET3 index= 323                           MPIR_CVAR_BCAST_NODE_KARY_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_KARY_SEGSIZE @Default:# -1
20220327 015824.560 INFO             PET3 index= 324                        MPIR_CVAR_BCAST_NODE_KNOMIAL_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_KNOMIAL_SEGSIZE @Default:# -1
20220327 015824.560 INFO             PET3 index= 325                          MPIR_CVAR_BCAST_NETWORK_TREE_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_TREE_RADIX @Default:# -1
20220327 015824.560 INFO             PET3 index= 326                        MPIR_CVAR_BCAST_NETWORK_TREE_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_TREE_SEGSIZE @Default:# -1
20220327 015824.560 INFO             PET3 index= 327                           MPIR_CVAR_BCAST_NETWORK_TREE_TYPE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_TREE_TYPE @Default:# -1
20220327 015824.560 INFO             PET3 index= 328                       MPIR_CVAR_BCAST_NETWORK_TREE_THROTTLE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_TREE_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET3 index= 329                       MPIR_CVAR_BCAST_NODE_SHUMILIN_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHUMILIN_SEGSIZE @Default:# -1
20220327 015824.560 INFO             PET3 index= 330                        MPIR_CVAR_BCAST_NODE_SHM_GATHER_TYPE : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHM_GATHER_TYPE @Default:# -1
20220327 015824.560 INFO             PET3 index= 331                       MPIR_CVAR_BCAST_NODE_SHM_GATHER_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHM_GATHER_RADIX @Default:# -1
20220327 015824.560 INFO             PET3 index= 332                       MPIR_CVAR_BCAST_NODE_SHM_RELEASE_TYPE : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHM_RELEASE_TYPE @Default:# -1
20220327 015824.560 INFO             PET3 index= 333                      MPIR_CVAR_BCAST_NODE_SHM_RELEASE_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHM_RELEASE_RADIX @Default:# -1
20220327 015824.560 INFO             PET3 index= 334                    MPIR_CVAR_BCAST_NETWORK_SHUMILIN_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_SHUMILIN_SEGSIZE @Default:# -1
20220327 015824.560 INFO             PET3 index= 335                 MPIR_CVAR_BCAST_NODE_NUMA_AWARE_MEMCPY_ARCH : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_MEMCPY_ARCH @Default:# -1
20220327 015824.560 INFO             PET3 index= 336                   MPIR_CVAR_BCAST_NODE_NUMA_AWARE_FRAME_NUM : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_FRAME_NUM
20220327 015824.560 INFO             PET3 index= 337                  MPIR_CVAR_BCAST_NODE_NUMA_AWARE_FRAME_SIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_FRAME_SIZE
20220327 015824.560 INFO             PET3 index= 338  MPIR_CVAR_BCAST_NODE_NUMA_AWARE_RECV_NONTEMPORAL_THRESHOLD : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_NONTEMPORAL_THRESHOLD
20220327 015824.560 INFO             PET3 index= 339      MPIR_CVAR_BCAST_NODE_NUMA_AWARE_FRAME_NONTEMPORAL_SIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_FRAME_NONTEMPORAL_SIZE
20220327 015824.560 INFO             PET3 index= 340 MPIR_CVAR_BCAST_NODE_NUMA_AWARE_TINY_MESSAGE_SIZE_THRESHOLD : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_TINY_MESSAGE_SIZE_THRESHOLD
20220327 015824.560 INFO             PET3 index= 341MPIR_CVAR_BCAST_NODE_NUMA_AWARE_SHM_HEAP_MESSAGE_SIZE_THRESHOLD : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_SHM_HEAP_MESSAGE_SIZE_THRESHOLD
20220327 015824.560 INFO             PET3 index= 342                                        MPIR_CVAR_USE_EXSCAN : Control selection of MPI_Exscan algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_EXSCAN @Default:# -1
20220327 015824.560 INFO             PET3 index= 343                                   MPIR_CVAR_USE_EXSCAN_LIST : @Alias:# I_MPI_ADJUST_EXSCAN_LIST @Default:# -1
20220327 015824.560 INFO             PET3 index= 344                            MPIR_CVAR_USE_EXSCAN_COMPOSITION : @Alias:# I_MPI_ADJUST_EXSCAN_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET3 index= 345                                MPIR_CVAR_USE_EXSCAN_NETWORK : @Alias:# I_MPI_ADJUST_EXSCAN_NETWORK @Default:# -1
20220327 015824.560 INFO             PET3 index= 346                                   MPIR_CVAR_USE_EXSCAN_NODE : @Alias:# I_MPI_ADJUST_EXSCAN_NODE @Default:# -1
20220327 015824.560 INFO             PET3 index= 347                                        MPIR_CVAR_USE_GATHER : Control selection of MPI_Gather algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_GATHER @Default:# -1
20220327 015824.560 INFO             PET3 index= 348                                   MPIR_CVAR_USE_GATHER_LIST : @Alias:# I_MPI_ADJUST_GATHER_LIST @Default:# -1
20220327 015824.560 INFO             PET3 index= 349                            MPIR_CVAR_USE_GATHER_COMPOSITION : @Alias:# I_MPI_ADJUST_GATHER_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET3 index= 350                                MPIR_CVAR_USE_GATHER_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_GATHER_NETWORK @Default:# -1
20220327 015824.560 INFO             PET3 index= 351                                   MPIR_CVAR_USE_GATHER_NODE : range: 0-3 @Alias:# I_MPI_ADJUST_GATHER_NODE @Default:# -1
20220327 015824.561 INFO             PET3 index= 352            MPIR_CVAR_GATHER_NODE_BINOMIAL_SEGMENTED_SEGSIZE : @Alias:# I_MPI_ADJUST_GATHER_NODE_BINOMIAL_SEGMENTED_SEGSIZE @Default:# -1
20220327 015824.561 INFO             PET3 index= 353         MPIR_CVAR_GATHER_NETWORK_BINOMIAL_SEGMENTED_SEGSIZE : @Alias:# I_MPI_ADJUST_GATHER_NETWORK_BINOMIAL_SEGMENTED_SEGSIZE @Default:# -1
20220327 015824.561 INFO             PET3 index= 354                                       MPIR_CVAR_USE_GATHERV : Control selection of MPI_Gatherv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-4 @Alias:# I_MPI_ADJUST_GATHERV @Default:# -1
20220327 015824.561 INFO             PET3 index= 355                                  MPIR_CVAR_USE_GATHERV_LIST : @Alias:# I_MPI_ADJUST_GATHERV_LIST @Default:# -1
20220327 015824.561 INFO             PET3 index= 356                           MPIR_CVAR_USE_GATHERV_COMPOSITION : @Alias:# I_MPI_ADJUST_GATHERV_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET3 index= 357                               MPIR_CVAR_USE_GATHERV_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_GATHERV_NETWORK @Default:# -1
20220327 015824.561 INFO             PET3 index= 358                                  MPIR_CVAR_USE_GATHERV_NODE : range: 0-2 @Alias:# I_MPI_ADJUST_GATHERV_NODE @Default:# -1
20220327 015824.561 INFO             PET3 index= 359                     MPIR_CVAR_GATHERV_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_GATHERV_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.561 INFO             PET3 index= 360                        MPIR_CVAR_GATHERV_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_GATHERV_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.561 INFO             PET3 index= 361                                MPIR_CVAR_USE_REDUCE_SCATTER : Control selection of MPI_Reduce_scatter algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-6 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER @Default:# -1
20220327 015824.561 INFO             PET3 index= 362                           MPIR_CVAR_USE_REDUCE_SCATTER_LIST : @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_LIST @Default:# -1
20220327 015824.561 INFO             PET3 index= 363                    MPIR_CVAR_USE_REDUCE_SCATTER_COMPOSITION : @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET3 index= 364                        MPIR_CVAR_USE_REDUCE_SCATTER_NETWORK : range: 0-5 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_NETWORK @Default:# -1
20220327 015824.561 INFO             PET3 index= 365                           MPIR_CVAR_USE_REDUCE_SCATTER_NODE : range: 0-4 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_NODE @Default:# -1
20220327 015824.561 INFO             PET3 index= 366                          MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK : Control selection of MPI_Reduce_scatter_block algorithm presets. Arguments$ <algid> - Algorithm identifier$ range: 0-4 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK @Default:# -1
20220327 015824.561 INFO             PET3 index= 367                     MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK_LIST : @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK_LIST @Default:# -1
20220327 015824.561 INFO             PET3 index= 368              MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK_COMPOSITION : @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET3 index= 369                  MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK_NETWORK @Default:# -1
20220327 015824.561 INFO             PET3 index= 370                     MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK_NODE : range: 0-3 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK_NODE @Default:# -1
20220327 015824.561 INFO             PET3 index= 371                                        MPIR_CVAR_USE_REDUCE : Control selection of MPI_Reduce algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-13 @Alias:# I_MPI_ADJUST_REDUCE @Default:# -1
20220327 015824.561 INFO             PET3 index= 372                                   MPIR_CVAR_USE_REDUCE_LIST : @Alias:# I_MPI_ADJUST_REDUCE_LIST @Default:# -1
20220327 015824.561 INFO             PET3 index= 373                            MPIR_CVAR_USE_REDUCE_COMPOSITION : @Alias:# I_MPI_ADJUST_REDUCE_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET3 index= 374                                MPIR_CVAR_USE_REDUCE_NETWORK : range: 0-10 @Alias:# I_MPI_ADJUST_REDUCE_NETWORK @Default:# -1
20220327 015824.561 INFO             PET3 index= 375                                   MPIR_CVAR_USE_REDUCE_NODE : range: 0-7 @Alias:# I_MPI_ADJUST_REDUCE_NODE @Default:# -1
20220327 015824.561 INFO             PET3 index= 376                         MPIR_CVAR_REDUCE_NETWORK_KARY_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KARY_RADIX @Default:# -1
20220327 015824.561 INFO             PET3 index= 377                      MPIR_CVAR_REDUCE_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.561 INFO             PET3 index= 378                      MPIR_CVAR_REDUCE_NETWORK_KARY_NBUFFERS : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KARY_NBUFFERS @Default:# -1
20220327 015824.561 INFO             PET3 index= 379                   MPIR_CVAR_REDUCE_NETWORK_KNOMIAL_NBUFFERS : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KNOMIAL_NBUFFERS @Default:# -1
20220327 015824.561 INFO             PET3 index= 380                       MPIR_CVAR_REDUCE_NETWORK_KARY_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KARY_SEGSIZE @Default:# -1
20220327 015824.561 INFO             PET3 index= 381                    MPIR_CVAR_REDUCE_NETWORK_KNOMIAL_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KNOMIAL_SEGSIZE @Default:# -1
20220327 015824.561 INFO             PET3 index= 382                       MPIR_CVAR_REDUCE_NETWORK_RING_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_RING_SEGSIZE @Default:# -1
20220327 015824.561 INFO             PET3 index= 383                            MPIR_CVAR_REDUCE_NODE_KARY_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KARY_RADIX @Default:# -1
20220327 015824.561 INFO             PET3 index= 384                         MPIR_CVAR_REDUCE_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.561 INFO             PET3 index= 385                         MPIR_CVAR_REDUCE_NODE_KARY_NBUFFERS : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KARY_NBUFFERS @Default:# -1
20220327 015824.561 INFO             PET3 index= 386                      MPIR_CVAR_REDUCE_NODE_KNOMIAL_NBUFFERS : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KNOMIAL_NBUFFERS @Default:# -1
20220327 015824.561 INFO             PET3 index= 387                          MPIR_CVAR_REDUCE_NODE_KARY_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KARY_SEGSIZE @Default:# -1
20220327 015824.561 INFO             PET3 index= 388                       MPIR_CVAR_REDUCE_NODE_KNOMIAL_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KNOMIAL_SEGSIZE @Default:# -1
20220327 015824.561 INFO             PET3 index= 389                          MPIR_CVAR_REDUCE_NODE_RING_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_RING_SEGSIZE @Default:# -1
20220327 015824.561 INFO             PET3 index= 390                       MPIR_CVAR_REDUCE_NODE_SHM_GATHER_TYPE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_SHM_GATHER_TYPE @Default:# -1
20220327 015824.561 INFO             PET3 index= 391                      MPIR_CVAR_REDUCE_NODE_SHM_GATHER_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NODE_SHM_GATHER_RADIX @Default:# -1
20220327 015824.561 INFO             PET3 index= 392                      MPIR_CVAR_REDUCE_NODE_SHM_RELEASE_TYPE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_SHM_RELEASE_TYPE @Default:# -1
20220327 015824.561 INFO             PET3 index= 393                     MPIR_CVAR_REDUCE_NODE_SHM_RELEASE_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NODE_SHM_RELEASE_RADIX @Default:# -1
20220327 015824.561 INFO             PET3 index= 394                                          MPIR_CVAR_USE_SCAN : Control selection of MPI_Scan algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_SCAN @Default:# -1
20220327 015824.561 INFO             PET3 index= 395                                     MPIR_CVAR_USE_SCAN_LIST : @Alias:# I_MPI_ADJUST_SCAN_LIST @Default:# -1
20220327 015824.561 INFO             PET3 index= 396                              MPIR_CVAR_USE_SCAN_COMPOSITION : @Alias:# I_MPI_ADJUST_SCAN_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET3 index= 397                                  MPIR_CVAR_USE_SCAN_NETWORK : @Alias:# I_MPI_ADJUST_SCAN_NETWORK @Default:# -1
20220327 015824.561 INFO             PET3 index= 398                                     MPIR_CVAR_USE_SCAN_NODE : @Alias:# I_MPI_ADJUST_SCAN_NODE @Default:# -1
20220327 015824.561 INFO             PET3 index= 399                                       MPIR_CVAR_USE_SCATTER : Control selection of MPI_Scatter algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_SCATTER @Default:# -1
20220327 015824.561 INFO             PET3 index= 400                                  MPIR_CVAR_USE_SCATTER_LIST : @Alias:# I_MPI_ADJUST_SCATTER_LIST @Default:# -1
20220327 015824.561 INFO             PET3 index= 401                           MPIR_CVAR_USE_SCATTER_COMPOSITION : @Alias:# I_MPI_ADJUST_SCATTER_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET3 index= 402                               MPIR_CVAR_USE_SCATTER_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_SCATTER_NETWORK @Default:# -1
20220327 015824.561 INFO             PET3 index= 403                                  MPIR_CVAR_USE_SCATTER_NODE : range: 0-3 @Alias:# I_MPI_ADJUST_SCATTER_NODE @Default:# -1
20220327 015824.561 INFO             PET3 index= 404                                      MPIR_CVAR_USE_SCATTERV : Control selection of MPI_Scatterv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-4 @Alias:# I_MPI_ADJUST_SCATTERV @Default:# -1
20220327 015824.561 INFO             PET3 index= 405                                 MPIR_CVAR_USE_SCATTERV_LIST : @Alias:# I_MPI_ADJUST_SCATTERV_LIST @Default:# -1
20220327 015824.561 INFO             PET3 index= 406                          MPIR_CVAR_USE_SCATTERV_COMPOSITION : @Alias:# I_MPI_ADJUST_SCATTERV_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET3 index= 407                              MPIR_CVAR_USE_SCATTERV_NETWORK : range: 0-3 @Alias:# I_MPI_ADJUST_SCATTERV_NETWORK @Default:# -1
20220327 015824.561 INFO             PET3 index= 408                                 MPIR_CVAR_USE_SCATTERV_NODE : range: 0-2 @Alias:# I_MPI_ADJUST_SCATTERV_NODE @Default:# -1
20220327 015824.561 INFO             PET3 index= 409                                    MPIR_CVAR_USE_IALLREDUCE : Control selection of MPI_Iallreduce algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-9 @Alias:# I_MPI_ADJUST_IALLREDUCE @Default:# -1
20220327 015824.561 INFO             PET3 index= 410                               MPIR_CVAR_USE_IALLREDUCE_LIST : @Alias:# I_MPI_ADJUST_IALLREDUCE_LIST @Default:# -1
20220327 015824.561 INFO             PET3 index= 411                        MPIR_CVAR_USE_IALLREDUCE_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLREDUCE_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET3 index= 412                            MPIR_CVAR_USE_IALLREDUCE_NETWORK : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK @Default:# -1
20220327 015824.561 INFO             PET3 index= 413                               MPIR_CVAR_USE_IALLREDUCE_NODE : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE @Default:# -1
20220327 015824.561 INFO             PET3 index= 414                   MPIR_CVAR_IALLREDUCE_KNOMIAL_REDUCE_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_KNOMIAL_REDUCE_RADIX @Default:# -1
20220327 015824.561 INFO             PET3 index= 415                    MPIR_CVAR_IALLREDUCE_KNOMIAL_BCAST_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_KNOMIAL_BCAST_RADIX @Default:# -1
20220327 015824.561 INFO             PET3 index= 416           MPIR_CVAR_IALLREDUCE_NETWORK_KNOMIAL_REDUCE_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK_KNOMIAL_REDUCE_RADIX @Default:# -1
20220327 015824.561 INFO             PET3 index= 417              MPIR_CVAR_IALLREDUCE_NODE_KNOMIAL_REDUCE_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE_KNOMIAL_REDUCE_RADIX @Default:# -1
20220327 015824.561 INFO             PET3 index= 418            MPIR_CVAR_IALLREDUCE_NETWORK_KNOMIAL_BCAST_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK_KNOMIAL_BCAST_RADIX @Default:# -1
20220327 015824.561 INFO             PET3 index= 419               MPIR_CVAR_IALLREDUCE_NODE_KNOMIAL_BCAST_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE_KNOMIAL_BCAST_RADIX @Default:# -1
20220327 015824.561 INFO             PET3 index= 420                 MPIR_CVAR_IALLREDUCE_NODE_NREDUCE_DO_GATHER : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE_NREDUCE_DO_GATHER @Default:# -1
20220327 015824.561 INFO             PET3 index= 421              MPIR_CVAR_IALLREDUCE_NETWORK_NREDUCE_DO_GATHER : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK_NREDUCE_DO_GATHER @Default:# -1
20220327 015824.561 INFO             PET3 index= 422              MPIR_CVAR_IALLREDUCE_NODE_NREDUCE_NB_ALLGATHER : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE_NREDUCE_NB_ALLGATHER @Default:# -1
20220327 015824.561 INFO             PET3 index= 423           MPIR_CVAR_IALLREDUCE_NETWORK_NREDUCE_NB_ALLGATHER : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK_NREDUCE_NB_ALLGATHER @Default:# -1
20220327 015824.561 INFO             PET3 index= 424                                        MPIR_CVAR_USE_IBCAST : Control selection of MPI_Ibcast algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_IBCAST @Default:# -1
20220327 015824.561 INFO             PET3 index= 425                                   MPIR_CVAR_USE_IBCAST_LIST : @Alias:# I_MPI_ADJUST_IBCAST_LIST @Default:# -1
20220327 015824.561 INFO             PET3 index= 426                            MPIR_CVAR_USE_IBCAST_COMPOSITION : @Alias:# I_MPI_ADJUST_IBCAST_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET3 index= 427                                MPIR_CVAR_USE_IBCAST_NETWORK : @Alias:# I_MPI_ADJUST_IBCAST_NETWORK @Default:# -1
20220327 015824.562 INFO             PET3 index= 428                                   MPIR_CVAR_USE_IBCAST_NODE : @Alias:# I_MPI_ADJUST_IBCAST_NODE @Default:# -1
20220327 015824.562 INFO             PET3 index= 429                              MPIR_CVAR_IBCAST_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IBCAST_KNOMIAL_RADIX @Default:# -1
20220327 015824.562 INFO             PET3 index= 430                      MPIR_CVAR_IBCAST_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IBCAST_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.562 INFO             PET3 index= 431                         MPIR_CVAR_IBCAST_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IBCAST_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.562 INFO             PET3 index= 432                                       MPIR_CVAR_USE_IREDUCE : Control selection of MPI_Ireduce algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-4 @Alias:# I_MPI_ADJUST_IREDUCE @Default:# -1
20220327 015824.562 INFO             PET3 index= 433                                  MPIR_CVAR_USE_IREDUCE_LIST : @Alias:# I_MPI_ADJUST_IREDUCE_LIST @Default:# -1
20220327 015824.562 INFO             PET3 index= 434                           MPIR_CVAR_USE_IREDUCE_COMPOSITION : @Alias:# I_MPI_ADJUST_IREDUCE_COMPOSITION @Default:# -1
20220327 015824.562 INFO             PET3 index= 435                               MPIR_CVAR_USE_IREDUCE_NETWORK : @Alias:# I_MPI_ADJUST_IREDUCE_NETWORK @Default:# -1
20220327 015824.562 INFO             PET3 index= 436                                  MPIR_CVAR_USE_IREDUCE_NODE : @Alias:# I_MPI_ADJUST_IREDUCE_NODE @Default:# -1
20220327 015824.562 INFO             PET3 index= 437                             MPIR_CVAR_IREDUCE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IREDUCE_KNOMIAL_RADIX @Default:# -1
20220327 015824.562 INFO             PET3 index= 438                     MPIR_CVAR_IREDUCE_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IREDUCE_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.562 INFO             PET3 index= 439                        MPIR_CVAR_IREDUCE_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IREDUCE_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.562 INFO             PET3 index= 440                                       MPIR_CVAR_USE_IGATHER : Control selection of MPI_Igather algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-2 @Alias:# I_MPI_ADJUST_IGATHER @Default:# -1
20220327 015824.562 INFO             PET3 index= 441                                  MPIR_CVAR_USE_IGATHER_LIST : @Alias:# I_MPI_ADJUST_IGATHER_LIST @Default:# -1
20220327 015824.562 INFO             PET3 index= 442                           MPIR_CVAR_USE_IGATHER_COMPOSITION : @Alias:# I_MPI_ADJUST_IGATHER_COMPOSITION @Default:# -1
20220327 015824.562 INFO             PET3 index= 443                               MPIR_CVAR_USE_IGATHER_NETWORK : @Alias:# I_MPI_ADJUST_IGATHER_NETWORK @Default:# -1
20220327 015824.562 INFO             PET3 index= 444                                  MPIR_CVAR_USE_IGATHER_NODE : @Alias:# I_MPI_ADJUST_IGATHER_NODE @Default:# -1
20220327 015824.562 INFO             PET3 index= 445                             MPIR_CVAR_IGATHER_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IGATHER_KNOMIAL_RADIX @Default:# -1
20220327 015824.562 INFO             PET3 index= 446                     MPIR_CVAR_IGATHER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IGATHER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.562 INFO             PET3 index= 447                        MPIR_CVAR_IGATHER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IGATHER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.562 INFO             PET3 index= 448                                    MPIR_CVAR_USE_IALLGATHER : Control selection of MPI_Iallgather algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_IALLGATHER @Default:# -1
20220327 015824.562 INFO             PET3 index= 449                               MPIR_CVAR_USE_IALLGATHER_LIST : @Alias:# I_MPI_ADJUST_IALLGATHER_LIST @Default:# -1
20220327 015824.562 INFO             PET3 index= 450                        MPIR_CVAR_USE_IALLGATHER_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLGATHER_COMPOSITION @Default:# -1
20220327 015824.562 INFO             PET3 index= 451                            MPIR_CVAR_USE_IALLGATHER_NETWORK : @Alias:# I_MPI_ADJUST_IALLGATHER_NETWORK @Default:# -1
20220327 015824.562 INFO             PET3 index= 452                               MPIR_CVAR_USE_IALLGATHER_NODE : @Alias:# I_MPI_ADJUST_IALLGATHER_NODE @Default:# -1
20220327 015824.562 INFO             PET3 index= 453                  MPIR_CVAR_IALLGATHER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IALLGATHER_NETWORK_KNOMIAL_RADIX
20220327 015824.562 INFO             PET3 index= 454                     MPIR_CVAR_IALLGATHER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IALLGATHER_NODE_KNOMIAL_RADIX
20220327 015824.562 INFO             PET3 index= 455                                     MPIR_CVAR_USE_IALLTOALL : Control selection of MPI_Ialltoall algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-7 @Alias:# I_MPI_ADJUST_IALLTOALL @Default:# -1
20220327 015824.562 INFO             PET3 index= 456                                MPIR_CVAR_USE_IALLTOALL_LIST : @Alias:# I_MPI_ADJUST_IALLTOALL_LIST @Default:# -1
20220327 015824.562 INFO             PET3 index= 457                         MPIR_CVAR_USE_IALLTOALL_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLTOALL_COMPOSITION @Default:# -1
20220327 015824.562 INFO             PET3 index= 458                             MPIR_CVAR_USE_IALLTOALL_NETWORK : @Alias:# I_MPI_ADJUST_IALLTOALL_NETWORK @Default:# -1
20220327 015824.562 INFO             PET3 index= 459                                MPIR_CVAR_USE_IALLTOALL_NODE : @Alias:# I_MPI_ADJUST_IALLTOALL_NODE @Default:# -1
20220327 015824.562 INFO             PET3 index= 460              MPIR_CVAR_IALLTOALL_PERMUTED_SENDRECV_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALL_PERMUTED_SENDRECV_THROTTLE @Default:# -1
20220327 015824.562 INFO             PET3 index= 461      MPIR_CVAR_IALLTOALL_NETWORK_PERMUTED_SENDRECV_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALL_NETWORK_PERMUTED_SENDRECV_THROTTLE @Default:# -1
20220327 015824.562 INFO             PET3 index= 462         MPIR_CVAR_IALLTOALL_NODE_PERMUTED_SENDRECV_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALL_NODE_PERMUTED_SENDRECV_THROTTLE @Default:# -1
20220327 015824.562 INFO             PET3 index= 463                                    MPIR_CVAR_USE_IALLTOALLV : Control selection of MPI_Ialltoallv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-6 @Alias:# I_MPI_ADJUST_IALLTOALLV @Default:# -1
20220327 015824.562 INFO             PET3 index= 464                               MPIR_CVAR_USE_IALLTOALLV_LIST : @Alias:# I_MPI_ADJUST_IALLTOALLV_LIST @Default:# -1
20220327 015824.562 INFO             PET3 index= 465                        MPIR_CVAR_USE_IALLTOALLV_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLTOALLV_COMPOSITION @Default:# -1
20220327 015824.562 INFO             PET3 index= 466                            MPIR_CVAR_USE_IALLTOALLV_NETWORK : @Alias:# I_MPI_ADJUST_IALLTOALLV_NETWORK @Default:# -1
20220327 015824.562 INFO             PET3 index= 467                               MPIR_CVAR_USE_IALLTOALLV_NODE : @Alias:# I_MPI_ADJUST_IALLTOALLV_NODE @Default:# -1
20220327 015824.562 INFO             PET3 index= 468                       MPIR_CVAR_IALLTOALLV_BLOCKED_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALLV_BLOCKED_THROTTLE @Default:# -1
20220327 015824.562 INFO             PET3 index= 469               MPIR_CVAR_IALLTOALLV_NETWORK_BLOCKED_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALLV_NETWORK_BLOCKED_THROTTLE @Default:# -1
20220327 015824.562 INFO             PET3 index= 470                                    MPIR_CVAR_USE_IALLTOALLW : Control selection of MPI_Ialltoallw algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-1 @Alias:# I_MPI_ADJUST_IALLTOALLW @Default:# -1
20220327 015824.562 INFO             PET3 index= 471                               MPIR_CVAR_USE_IALLTOALLW_LIST : @Alias:# I_MPI_ADJUST_IALLTOALLW_LIST @Default:# -1
20220327 015824.562 INFO             PET3 index= 472                        MPIR_CVAR_USE_IALLTOALLW_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLTOALLW_COMPOSITION @Default:# -1
20220327 015824.562 INFO             PET3 index= 473                            MPIR_CVAR_USE_IALLTOALLW_NETWORK : @Alias:# I_MPI_ADJUST_IALLTOALLW_NETWORK @Default:# -1
20220327 015824.562 INFO             PET3 index= 474                               MPIR_CVAR_USE_IALLTOALLW_NODE : @Alias:# I_MPI_ADJUST_IALLTOALLW_NODE @Default:# -1
20220327 015824.562 INFO             PET3 index= 475                       MPIR_CVAR_IALLTOALLW_BLOCKED_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALLW_BLOCKED_THROTTLE @Default:# -1
20220327 015824.562 INFO             PET3 index= 476               MPIR_CVAR_IALLTOALLW_NETWORK_BLOCKED_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALLW_NETWORK_BLOCKED_THROTTLE @Default:# -1
20220327 015824.562 INFO             PET3 index= 477                                      MPIR_CVAR_USE_IGATHERV : Control selection of MPI_Igatherv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_IGATHERV @Default:# -1
20220327 015824.562 INFO             PET3 index= 478                                 MPIR_CVAR_USE_IGATHERV_LIST : @Alias:# I_MPI_ADJUST_IGATHERV_LIST @Default:# -1
20220327 015824.562 INFO             PET3 index= 479                          MPIR_CVAR_USE_IGATHERV_COMPOSITION : @Alias:# I_MPI_ADJUST_IGATHERV_COMPOSITION @Default:# -1
20220327 015824.562 INFO             PET3 index= 480                              MPIR_CVAR_USE_IGATHERV_NETWORK : @Alias:# I_MPI_ADJUST_IGATHERV_NETWORK @Default:# -1
20220327 015824.562 INFO             PET3 index= 481                                 MPIR_CVAR_USE_IGATHERV_NODE : @Alias:# I_MPI_ADJUST_IGATHERV_NODE @Default:# -1
20220327 015824.562 INFO             PET3 index= 482                                     MPIR_CVAR_USE_ISCATTERV : Control selection of MPI_Iscatterv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-1 @Alias:# I_MPI_ADJUST_ISCATTERV @Default:# -1
20220327 015824.562 INFO             PET3 index= 483                                MPIR_CVAR_USE_ISCATTERV_LIST : @Alias:# I_MPI_ADJUST_ISCATTERV_LIST @Default:# -1
20220327 015824.562 INFO             PET3 index= 484                         MPIR_CVAR_USE_ISCATTERV_COMPOSITION : @Alias:# I_MPI_ADJUST_ISCATTERV_COMPOSITION @Default:# -1
20220327 015824.562 INFO             PET3 index= 485                             MPIR_CVAR_USE_ISCATTERV_NETWORK : @Alias:# I_MPI_ADJUST_ISCATTERV_NETWORK @Default:# -1
20220327 015824.562 INFO             PET3 index= 486                                MPIR_CVAR_USE_ISCATTERV_NODE : @Alias:# I_MPI_ADJUST_ISCATTERV_NODE @Default:# -1
20220327 015824.562 INFO             PET3 index= 487                                      MPIR_CVAR_USE_IBARRIER : Control selection of MPI_Ibarrier algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-2 @Alias:# I_MPI_ADJUST_IBARRIER @Default:# -1
20220327 015824.562 INFO             PET3 index= 488                                 MPIR_CVAR_USE_IBARRIER_LIST : @Alias:# I_MPI_ADJUST_IBARRIER_LIST @Default:# -1
20220327 015824.562 INFO             PET3 index= 489                          MPIR_CVAR_USE_IBARRIER_COMPOSITION : @Alias:# I_MPI_ADJUST_IBARRIER_COMPOSITION @Default:# -1
20220327 015824.562 INFO             PET3 index= 490                              MPIR_CVAR_USE_IBARRIER_NETWORK : @Alias:# I_MPI_ADJUST_IBARRIER_NETWORK @Default:# -1
20220327 015824.562 INFO             PET3 index= 491                                 MPIR_CVAR_USE_IBARRIER_NODE : @Alias:# I_MPI_ADJUST_IBARRIER_NODE @Default:# -1
20220327 015824.562 INFO             PET3 index= 492                                      MPIR_CVAR_USE_ISCATTER : Control selection of MPI_Iscatter algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-2 @Alias:# I_MPI_ADJUST_ISCATTER @Default:# -1
20220327 015824.562 INFO             PET3 index= 493                                 MPIR_CVAR_USE_ISCATTER_LIST : @Alias:# I_MPI_ADJUST_ISCATTER_LIST @Default:# -1
20220327 015824.563 INFO             PET3 index= 494                          MPIR_CVAR_USE_ISCATTER_COMPOSITION : @Alias:# I_MPI_ADJUST_ISCATTER_COMPOSITION @Default:# -1
20220327 015824.563 INFO             PET3 index= 495                              MPIR_CVAR_USE_ISCATTER_NETWORK : @Alias:# I_MPI_ADJUST_ISCATTER_NETWORK @Default:# -1
20220327 015824.563 INFO             PET3 index= 496                                 MPIR_CVAR_USE_ISCATTER_NODE : @Alias:# I_MPI_ADJUST_ISCATTER_NODE @Default:# -1
20220327 015824.563 INFO             PET3 index= 497                            MPIR_CVAR_ISCATTER_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ISCATTER_KNOMIAL_RADIX @Default:# -1
20220327 015824.563 INFO             PET3 index= 498                    MPIR_CVAR_ISCATTER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ISCATTER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.563 INFO             PET3 index= 499                       MPIR_CVAR_ISCATTER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ISCATTER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.563 INFO             PET3 index= 500                                   MPIR_CVAR_USE_IALLGATHERV : Control selection of MPI_Iallgatherv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_IALLGATHERV @Default:# -1
20220327 015824.563 INFO             PET3 index= 501                              MPIR_CVAR_USE_IALLGATHERV_LIST : @Alias:# I_MPI_ADJUST_IALLGATHERV_LIST @Default:# -1
20220327 015824.563 INFO             PET3 index= 502                       MPIR_CVAR_USE_IALLGATHERV_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLGATHERV_COMPOSITION @Default:# -1
20220327 015824.563 INFO             PET3 index= 503                           MPIR_CVAR_USE_IALLGATHERV_NETWORK : @Alias:# I_MPI_ADJUST_IALLGATHERV_NETWORK @Default:# -1
20220327 015824.563 INFO             PET3 index= 504                              MPIR_CVAR_USE_IALLGATHERV_NODE : @Alias:# I_MPI_ADJUST_IALLGATHERV_NODE @Default:# -1
20220327 015824.563 INFO             PET3 index= 505                                       MPIR_CVAR_USE_IEXSCAN : Control selection of MPI_Iexscan algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-1 @Alias:# I_MPI_ADJUST_IEXSCAN @Default:# -1
20220327 015824.563 INFO             PET3 index= 506                                  MPIR_CVAR_USE_IEXSCAN_LIST : @Alias:# I_MPI_ADJUST_IEXSCAN_LIST @Default:# -1
20220327 015824.563 INFO             PET3 index= 507                           MPIR_CVAR_USE_IEXSCAN_COMPOSITION : @Alias:# I_MPI_ADJUST_IEXSCAN_COMPOSITION @Default:# -1
20220327 015824.563 INFO             PET3 index= 508                               MPIR_CVAR_USE_IEXSCAN_NETWORK : @Alias:# I_MPI_ADJUST_IEXSCAN_NETWORK @Default:# -1
20220327 015824.563 INFO             PET3 index= 509                                  MPIR_CVAR_USE_IEXSCAN_NODE : @Alias:# I_MPI_ADJUST_IEXSCAN_NODE @Default:# -1
20220327 015824.563 INFO             PET3 index= 510                                         MPIR_CVAR_USE_ISCAN : Control selection of MPI_Iscan algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-1 @Alias:# I_MPI_ADJUST_ISCAN @Default:# -1
20220327 015824.563 INFO             PET3 index= 511                                    MPIR_CVAR_USE_ISCAN_LIST : @Alias:# I_MPI_ADJUST_ISCAN_LIST @Default:# -1
20220327 015824.563 INFO             PET3 index= 512                             MPIR_CVAR_USE_ISCAN_COMPOSITION : @Alias:# I_MPI_ADJUST_ISCAN_COMPOSITION @Default:# -1
20220327 015824.563 INFO             PET3 index= 513                                 MPIR_CVAR_USE_ISCAN_NETWORK : @Alias:# I_MPI_ADJUST_ISCAN_NETWORK @Default:# -1
20220327 015824.563 INFO             PET3 index= 514                                    MPIR_CVAR_USE_ISCAN_NODE : @Alias:# I_MPI_ADJUST_ISCAN_NODE @Default:# -1
20220327 015824.563 INFO             PET3 index= 515                               MPIR_CVAR_USE_IREDUCE_SCATTER : Control selection of MPI_Ireduce_scatter algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER @Default:# -1
20220327 015824.563 INFO             PET3 index= 516                          MPIR_CVAR_USE_IREDUCE_SCATTER_LIST : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_LIST @Default:# -1
20220327 015824.563 INFO             PET3 index= 517                   MPIR_CVAR_USE_IREDUCE_SCATTER_COMPOSITION : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_COMPOSITION @Default:# -1
20220327 015824.563 INFO             PET3 index= 518                       MPIR_CVAR_USE_IREDUCE_SCATTER_NETWORK : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_NETWORK @Default:# -1
20220327 015824.563 INFO             PET3 index= 519                          MPIR_CVAR_USE_IREDUCE_SCATTER_NODE : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_NODE @Default:# -1
20220327 015824.563 INFO             PET3 index= 520                         MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK : Control selection of MPI_Ireduce_scatter_block algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK @Default:# -1
20220327 015824.563 INFO             PET3 index= 521                    MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK_LIST : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK_LIST @Default:# -1
20220327 015824.563 INFO             PET3 index= 522             MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK_COMPOSITION : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK_COMPOSITION @Default:# -1
20220327 015824.563 INFO             PET3 index= 523                 MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK_NETWORK : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK_NETWORK @Default:# -1
20220327 015824.563 INFO             PET3 index= 524                    MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK_NODE : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK_NODE @Default:# -1
20220327 015824.563 INFO             PET3 index= 525                               MPIR_CVAR_IMPI_SHMGR_DATASIZE : Define the size of shared memory area available for each rank for data placement. Messages greater than this value will not be processed by SHM-based collective operation, but will be processed by point-to-point based collective operation. The value must be a multiple of 4096. @Alias:# I_MPI_COLL_SHM_THRESHOLD @Default:# 16384
20220327 015824.563 INFO             PET3 index= 526                              MPIR_CVAR_IMPI_SHMGR_SPINCOUNT : @Alias:# I_MPI_COLL_SHM_PROGRESS_SPIN_COUNT
20220327 015824.563 INFO             PET3 index= 527                              MPIR_CVAR_INTEL_COLL_INTRANODE : @Alias:# I_MPI_COLL_INTRANODE
20220327 015824.563 INFO             PET3 index= 528                         MPIR_CVAR_ENABLE_EXPERIMENTAL_ALGOS : @Alias:# I_MPI_COLL_EXPERIMENTAL
20220327 015824.563 INFO             PET3 index= 529                                    MPIR_CVAR_IMPI_WAIT_MODE : @Alias:# I_MPI_WAIT_MODE
20220327 015824.563 INFO             PET3 index= 530                                 MPIR_CVAR_IMPI_THREAD_SLEEP : @Alias:# I_MPI_THREAD_SLEEP
20220327 015824.563 INFO             PET3 index= 531                          MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the short message algorithm will be used if the send buffer size is < this value (in bytes). (See also: MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE)
20220327 015824.563 INFO             PET3 index= 532                           MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the long message algorithm will be used if the send buffer size is >= this value (in bytes) (See also: MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE)
20220327 015824.563 INFO             PET3 index= 533                      MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE : The smallest message size that will be used for the pipelined, large-message, ring algorithm in the MPI_Allgatherv implementation.
20220327 015824.563 INFO             PET3 index= 534                          MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20220327 015824.563 INFO             PET3 index= 535                            MPIR_CVAR_ENABLE_SMP_COLLECTIVES : Enable SMP aware collective communication.
20220327 015824.563 INFO             PET3 index= 536                              MPIR_CVAR_ENABLE_SMP_ALLREDUCE : Enable SMP aware allreduce.
20220327 015824.563 INFO             PET3 index= 537                        MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE : Maximum message size for which SMP-aware allreduce is used.  A value of '0' uses SMP-aware allreduce for all message sizes.
20220327 015824.563 INFO             PET3 index= 538                           MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE : the short message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value (See also: MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE)
20220327 015824.563 INFO             PET3 index= 539                          MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE : the medium message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value and larger than MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE (See also: MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE)
20220327 015824.563 INFO             PET3 index= 540                                MPIR_CVAR_ENABLE_SMP_BARRIER : Enable SMP aware barrier.
20220327 015824.563 INFO             PET3 index= 541                                   MPIR_CVAR_BCAST_MIN_PROCS : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_SHORT_MSG_SIZE, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20220327 015824.563 INFO             PET3 index= 542                              MPIR_CVAR_BCAST_SHORT_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20220327 015824.563 INFO             PET3 index= 543                               MPIR_CVAR_BCAST_LONG_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_SHORT_MSG_SIZE)
20220327 015824.563 INFO             PET3 index= 544                                  MPIR_CVAR_ENABLE_SMP_BCAST : Enable SMP aware broadcast (See also: MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE)
20220327 015824.563 INFO             PET3 index= 545                            MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE : Maximum message size for which SMP-aware broadcast is used.  A value of '0' uses SMP-aware broadcast for all message sizes. (See also: MPIR_CVAR_ENABLE_SMP_BCAST)
20220327 015824.563 INFO             PET3 index= 546                            MPIR_CVAR_GATHER_VSMALL_MSG_SIZE : use a temporary buffer for intracommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE)
20220327 015824.563 INFO             PET3 index= 547                       MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_VSMALL_MSG_SIZE)
20220327 015824.563 INFO             PET3 index= 548                     MPIR_CVAR_GATHERV_INTER_SSEND_MIN_PROCS : Use Ssend (synchronous send) for intercommunicator MPI_Gatherv if the "group B" size is >= this value.  Specifying "-1" always avoids using Ssend.  For backwards compatibility, specifying "0" uses the default value.
20220327 015824.563 INFO             PET3 index= 549                             MPIR_CVAR_REDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20220327 015824.563 INFO             PET3 index= 550                                 MPIR_CVAR_ENABLE_SMP_REDUCE : Enable SMP aware reduce.
20220327 015824.563 INFO             PET3 index= 551                           MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE : Maximum message size for which SMP-aware reduce is used.  A value of '0' uses SMP-aware reduce for all message sizes.
20220327 015824.563 INFO             PET3 index= 552                      MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Scatter if the send buffer size is < this value (in bytes)
20220327 015824.563 INFO             PET3 index= 553                                  MPIR_CVAR_USE_CPU_PLATFORM : @Alias:# I_MPI_PLATFORM
20220327 015824.563 INFO             PET3 index= 554                          MPIR_CVAR_FAILURE_ON_COLL_FALLBACK : @Alias:# I_MPI_ADJUST_FAILURE_ON_COLL_FALLBACK
20220327 015824.563 INFO             PET3 index= 555                         MPIR_CVAR_FAILURE_ON_MATCH_FALLBACK : @Alias:# I_MPI_ADJUST_FAILURE_ON_MATCH_FALLBACK
20220327 015824.563 INFO             PET3 index= 556                           MPIR_CVAR_ADJUST_SENDRECV_REPLACE : @Alias:# I_MPI_ADJUST_SENDRECV_REPLACE
20220327 015824.563 INFO             PET3 index= 557                MPIR_CVAR_ADJUST_SENDRECV_REPLACE_FRAME_SIZE : @Alias:# I_MPI_ADJUST_SENDRECV_REPLACE_FRAME_SIZE
20220327 015824.563 INFO             PET3 index= 558                         MPIR_CVAR_NUMERICAL_REPRODUCIBILITY : @Alias:# I_MPI_CBWR
20220327 015824.563 INFO             PET3 index= 559                                    MPIR_CVAR_USE_TUNING_CH4 : @Alias:# I_MPI_TUNING
20220327 015824.563 INFO             PET3 index= 560                                    MPIR_CVAR_USE_TUNING_NET : @Alias:# I_MPI_TUNING_NETWORK
20220327 015824.563 INFO             PET3 index= 561                                    MPIR_CVAR_USE_TUNING_SHM : @Alias:# I_MPI_TUNING_NODE
20220327 015824.563 INFO             PET3 index= 562                                   MPIR_CVAR_DUMP_TUNING_CH4 : @Alias:# I_MPI_TUNING_COMPOSITION_DUMP
20220327 015824.563 INFO             PET3 index= 563                                   MPIR_CVAR_DUMP_TUNING_NET : @Alias:# I_MPI_TUNING_NETWORK_DUMP
20220327 015824.563 INFO             PET3 index= 564                                   MPIR_CVAR_DUMP_TUNING_SHM : @Alias:# I_MPI_TUNING_NODE_DUMP
20220327 015824.563 INFO             PET3 index= 565                                        MPIR_CVAR_BIN_TUNING : @Alias:# I_MPI_TUNING_BIN
20220327 015824.563 INFO             PET3 index= 566                                   MPIR_CVAR_BIN_DUMP_TUNING : @Alias:# I_MPI_TUNING_BIN_DUMP
20220327 015824.564 INFO             PET3 index= 567                                   MPIR_CVAR_TUNING_BIN_PATH : @Alias:# I_MPI_TUNING_BIN_PATH
20220327 015824.564 INFO             PET3 index= 568                            MPIR_CVAR_TUNING_COMPOSITION_PPN : @Alias:# I_MPI_TUNING_COMPOSITION_PPN
20220327 015824.564 INFO             PET3 index= 569                                MPIR_CVAR_TUNING_NETWORK_PPN : @Alias:# I_MPI_TUNING_NETWORK_PPN
20220327 015824.564 INFO             PET3 index= 570                                   MPIR_CVAR_TUNING_NODE_PPN : @Alias:# I_MPI_TUNING_NODE_PPN
20220327 015824.564 INFO             PET3 index= 571                 MPIR_CVAR_TUNING_COMPOSITION_COMM_HIERARCHY : @Alias:# I_MPI_TUNING_COMPOSITION_COMM_HIERARCHY
20220327 015824.564 INFO             PET3 index= 572                     MPIR_CVAR_TUNING_NETWORK_COMM_HIERARCHY : @Alias:# I_MPI_TUNING_NETWORK_COMM_HIERARCHY
20220327 015824.564 INFO             PET3 index= 573                        MPIR_CVAR_TUNING_NODE_COMM_HIERARCHY : @Alias:# I_MPI_TUNING_NODE_COMM_HIERARCHY
20220327 015824.564 INFO             PET3 index= 574                                       MPIR_CVAR_TUNING_MODE : @Alias:# I_MPI_TUNING_MODE
20220327 015824.564 INFO             PET3 index= 575                                MPIR_CVAR_TUNING_AUTO_POLICY : @Alias:# I_MPI_TUNING_AUTO_POLICY
20220327 015824.564 INFO             PET3 index= 576                             MPIR_CVAR_TUNING_AUTO_COMM_LIST : @Alias:# I_MPI_TUNING_AUTO_COMM_LIST
20220327 015824.564 INFO             PET3 index= 577                             MPIR_CVAR_TUNING_AUTO_COMM_USER : @Alias:# I_MPI_TUNING_AUTO_COMM_USER
20220327 015824.564 INFO             PET3 index= 578                          MPIR_CVAR_TUNING_AUTO_COMM_DEFAULT : @Alias:# I_MPI_TUNING_AUTO_COMM_DEFAULT
20220327 015824.564 INFO             PET3 index= 579                              MPIR_CVAR_TUNING_AUTO_ITER_NUM : @Alias:# I_MPI_TUNING_AUTO_ITER_NUM
20220327 015824.564 INFO             PET3 index= 580                           MPIR_CVAR_TUNING_AUTO_ITER_POLICY : @Alias:# I_MPI_TUNING_AUTO_ITER_POLICY
20220327 015824.564 INFO             PET3 index= 581                 MPIR_CVAR_TUNING_AUTO_ITER_POLICY_THRESHOLD : @Alias:# I_MPI_TUNING_AUTO_ITER_POLICY_THRESHOLD
20220327 015824.564 INFO             PET3 index= 582                                  MPIR_CVAR_TUNING_AUTO_SYNC : @Alias:# I_MPI_TUNING_AUTO_SYNC
20220327 015824.564 INFO             PET3 index= 583                          MPIR_CVAR_TUNING_AUTO_STORAGE_SIZE : @Alias:# I_MPI_TUNING_AUTO_STORAGE_SIZE
20220327 015824.564 INFO             PET3 index= 584                       MPIR_CVAR_TUNING_AUTO_WARMUP_ITER_NUM : @Alias:# I_MPI_TUNING_AUTO_WARMUP_ITER_NUM
20220327 015824.564 INFO             PET3 index= 585                                 MPIR_CVAR_TUNING_AUTO_SMART : @Alias:# I_MPI_TUNING_AUTO_SMART
20220327 015824.564 INFO             PET3 index= 586                                  MPIR_CVAR_TUNING_COLL_LIST : @Alias:# I_MPI_TUNING_COLL_LIST
20220327 015824.564 INFO             PET3 index= 587                               MPIR_CVAR_TUNING_COLL_VEC_OPS : @Alias:# I_MPI_TUNING_COLL_VEC_OPS
20220327 015824.564 INFO             PET3 index= 588                              MPIR_CVAR_DEFAULT_THREAD_LEVEL : @Alias:# I_MPI_THREAD_LEVEL
20220327 015824.564 INFO             PET3 index= 589                                      MPIR_CVAR_THREAD_SPLIT : Control the MPI_THREAD_SPLIT model support. @Alias:# I_MPI_THREAD_SPLIT @Default:# false
20220327 015824.564 INFO             PET3 index= 590                                    MPIR_CVAR_THREAD_RUNTIME : Control threading runtimes support. @Alias:# I_MPI_THREAD_RUNTIME @Default:# generic
20220327 015824.564 INFO             PET3 index= 591                                     MPIR_CVAR_THREAD_ID_KEY : Set the MPI info object key that is used to explicitly define the application $thread_id for a communicator. @Alias:# I_MPI_THREAD_ID_KEY @Default:# -1
20220327 015824.564 INFO             PET3 index= 592                                        MPIR_CVAR_THREAD_MAX : Set the maximum number of application threads per rank. @Alias:# I_MPI_THREAD_MAX @Default:# -1
20220327 015824.564 INFO             PET3 index= 593                                    MPIR_CVAR_ASYNC_PROGRESS : Enables asynchronous progress threads. @Alias:# I_MPI_ASYNC_PROGRESS @Default:# false
20220327 015824.564 INFO             PET3 index= 594                          MPIR_CVAR_CH4_MAX_PROGRESS_THREADS : Specifies the maximum number of progress threads. @Alias:# I_MPI_ASYNC_PROGRESS_THREADS @Default:# 1
20220327 015824.564 INFO             PET3 index= 595                      MPIR_CVAR_CH4_PROGRESS_THREAD_AFFINITY : Specifies affinity for all progress threads of local processes. @Alias:# I_MPI_ASYNC_PROGRESS_PIN @Default:# not defined
20220327 015824.564 INFO             PET3 index= 596                                         MPIR_CVAR_EP_ID_KEY : Set the MPI info object key that is used to explicitly define the progress $thread_id for a communicator. @Alias:# I_MPI_ASYNC_PROGRESS_ID_KEY @Default:# thread_id
20220327 015824.564 INFO             PET3 index= 597                                       MPIR_CVAR_THREAD_MODE : @Alias:# I_MPI_THREAD_MODE
20220327 015824.564 INFO             PET3 index= 598                                 MPIR_CVAR_THREAD_LOCK_LEVEL : @Alias:# I_MPI_THREAD_LOCK_LEVEL
20220327 015824.564 INFO             PET3 index= 599                                  MPIR_CVAR_CH4_OFI_MAX_VCIS : @Alias:# I_MPI_THREAD_EP_MAX
20220327 015824.564 INFO             PET3 index= 600                                       MPIR_CVAR_INTEL_DEBUG : Print out debugging information when an MPI program starts running.$ Syntax$ I_MPI_DEBUG=<level>$ Arguments$ <level> - indicate level of debug information provided @Alias:# I_MPI_DEBUG @Default:# 0
20220327 015824.564 INFO             PET3 index= 601                                     MPIR_CVAR_DEBUG_VERSION : Print Intel MPI version. @Alias:# I_MPI_PRINT_VERSION @Default:# 0
20220327 015824.564 INFO             PET3 index= 602                                    MPIR_CVAR_ERROR_CHECKING : @Alias:# I_MPI_ERROR_CHECKING
20220327 015824.564 INFO             PET3 index= 603                                        MPIR_CVAR_MULTI_INIT : @Alias:# I_MPI_MULTI_INIT
20220327 015824.564 INFO             PET3 index= 604                               MPIR_CVAR_REMOVED_VAR_WARNING : Print out a warning if a removed environment variable is set. @Alias:# I_MPI_REMOVED_VAR_WARNING @Default:# 1
20220327 015824.564 INFO             PET3 index= 605                                MPIR_CVAR_VAR_CHECK_SPELLING : Print out a warning if an unknown environment variable is set. @Alias:# I_MPI_VAR_CHECK_SPELLING @Default:# 1
20220327 015824.564 INFO             PET3 index= 606                           MPIR_CVAR_INTEL_MPI_COMPATIBILITY : Select the runtime compatibility mode.$ Syntax$ I_MPI_COMPATIBILITY=<value>$ Arguments$ <value> - Define compatibility mode$ -----------------------------------------------------------------------$ <not defined> - The MPI-3.1 standard compatibility$ <3> - The Intel® MPI Library 3.x compatible mode$ <4> - The Intel® MPI Library 4.x compatible mode$ <5> - The Intel® MPI Library 5.x compatible mode$ ----------------------------------------------------------------------- @Alias:# I_MPI_COMPATIBILITY @Default:# 5
20220327 015824.564 INFO             PET3 index= 607                          MPIR_CVAR_IMPI_PROGRESS_SPIN_COUNT : @Alias:# I_MPI_SPIN_COUNT
20220327 015824.564 INFO             PET3 index= 608                         MPIR_CVAR_IMPI_PROGRESS_PAUSE_COUNT : @Alias:# I_MPI_PAUSE_COUNT
20220327 015824.564 INFO             PET3 index= 609                                 MPIR_CVAR_IMPI_THREAD_YIELD : @Alias:# I_MPI_THREAD_YIELD
20220327 015824.564 INFO             PET3 index= 610                                      MPIR_CVAR_SILENT_ABORT : Do not print abort warning message @Alias:# I_MPI_SILENT_ABORT
20220327 015824.564 INFO             PET3 index= 611                                  MPIR_CVAR_JOB_IDLE_TIMEOUT : Abort job if idle time is larger than the threshold in seconds. @Alias:# I_MPI_JOB_IDLE_TIMEOUT
20220327 015824.564 INFO             PET3 index= 612                              MPIR_CVAR_PMI_VALUE_LENGTH_MAX : Set PMI buffer length as minimum of variable value and PMI_KVS_Get_value_length_max(). @Alias:# I_MPI_PMI_VALUE_LENGTH_MAX
20220327 015824.564 INFO             PET3 index= 613                                       MPIR_CVAR_PMI_LIBRARY : Specify the name to third party implementation of the PMI library. @Alias:# I_MPI_PMI_LIBRARY
20220327 015824.564 INFO             PET3 index= 614                                               MPIR_CVAR_PMI : Select PMI version. Choices: auto, pmi1, pmi2, pmix.$ By default pmi version will be chosen automatically. @Alias:# I_MPI_PMI
20220327 015824.564 INFO             PET3 index= 615                                 MPIR_CVAR_NODEMAP_ALGORITHM : Select algorithm for nodemap creation. Choices: pmi_process_mapping, slurm, pmi_alltoall, auto. @Alias:# I_MPI_NODEMAP_ALGORITHM
20220327 015824.564 INFO             PET3 index= 616                                      MPIR_CVAR_ASYNC_REDUCE : @Alias:# I_MPI_ASYNC_REDUCE @Verbosity:# hidden
20220327 015824.564 INFO             PET3 index= 617                            MPIR_CVAR_CH4_MAX_REDUCE_THREADS : @Alias:# I_MPI_ASYNC_REDUCE_THREADS @Verbosity:# hidden
20220327 015824.564 INFO             PET3 index= 618                      MPIR_CVAR_ASYNC_REDUCE_COUNT_THRESHOLD : @Alias:# I_MPI_ASYNC_REDUCE_COUNT_THRESHOLD @Verbosity:# hidden
20220327 015824.564 INFO             PET3 index= 619                        MPIR_CVAR_CH4_REDUCE_THREAD_AFFINITY : @Alias:# I_MPI_ASYNC_REDUCE_PIN @Verbosity:# hidden
20220327 015824.564 INFO             PET3 --- VMK::logSystem() end ---------------------------------
20220327 015824.564 INFO             PET3 main: --- VMK::log() start -------------------------------------
20220327 015824.564 INFO             PET3 main: vm located at: 0x87ae60
20220327 015824.564 INFO             PET3 main: petCount=6 localPet=3 mypthid=140737352203136 currentSsiPe=14
20220327 015824.564 INFO             PET3 main: Current system level affinity pinning for local PET:
20220327 015824.564 INFO             PET3 main:  SSIPE=12
20220327 015824.564 INFO             PET3 main:  SSIPE=13
20220327 015824.564 INFO             PET3 main:  SSIPE=14
20220327 015824.564 INFO             PET3 main:  SSIPE=15
20220327 015824.565 INFO             PET3 main: Current system level OMP_NUM_THREADS setting for local PET: 1
20220327 015824.565 INFO             PET3 main: ssiCount=1 localSsi=0
20220327 015824.565 INFO             PET3 main: mpionly=1 threadsflag=0
20220327 015824.565 INFO             PET3 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220327 015824.565 INFO             PET3 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220327 015824.565 INFO             PET3 main:  PE=0 SSI=0 SSIPE=0
20220327 015824.565 INFO             PET3 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220327 015824.565 INFO             PET3 main:  PE=1 SSI=0 SSIPE=1
20220327 015824.565 INFO             PET3 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220327 015824.565 INFO             PET3 main:  PE=2 SSI=0 SSIPE=2
20220327 015824.565 INFO             PET3 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220327 015824.565 INFO             PET3 main:  PE=3 SSI=0 SSIPE=3
20220327 015824.565 INFO             PET3 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220327 015824.565 INFO             PET3 main:  PE=4 SSI=0 SSIPE=4
20220327 015824.565 INFO             PET3 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220327 015824.565 INFO             PET3 main:  PE=5 SSI=0 SSIPE=5
20220327 015824.565 INFO             PET3 main: --- VMK::log() end ---------------------------------------
20220327 015824.565 INFO             PET3 Executing 'userm1_setvm'
20220327 015824.566 INFO             PET3 Executing 'userm1_register'
20220327 015824.566 INFO             PET3 Executing 'userm2_setvm'
20220327 015824.567 INFO             PET3 Executing 'userm2_register'
20220327 015824.649 INFO             PET3 Entering 'user1_run'
20220327 015824.649 INFO             PET3 model1: --- VMK::log() start -------------------------------------
20220327 015824.649 INFO             PET3 model1: vm located at: 0x9fb9f0
20220327 015824.649 INFO             PET3 model1: petCount=6 localPet=3 mypthid=140737352203136 currentSsiPe=3
20220327 015824.649 INFO             PET3 model1: Current system level affinity pinning for local PET:
20220327 015824.649 INFO             PET3 model1:  SSIPE=3
20220327 015824.649 INFO             PET3 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220327 015824.649 INFO             PET3 model1: ssiCount=1 localSsi=0
20220327 015824.649 INFO             PET3 model1: mpionly=1 threadsflag=0
20220327 015824.649 INFO             PET3 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220327 015824.649 INFO             PET3 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220327 015824.649 INFO             PET3 model1:  PE=0 SSI=0 SSIPE=0
20220327 015824.649 INFO             PET3 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220327 015824.649 INFO             PET3 model1:  PE=1 SSI=0 SSIPE=1
20220327 015824.649 INFO             PET3 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220327 015824.649 INFO             PET3 model1:  PE=2 SSI=0 SSIPE=2
20220327 015824.649 INFO             PET3 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220327 015824.649 INFO             PET3 model1:  PE=3 SSI=0 SSIPE=3
20220327 015824.649 INFO             PET3 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220327 015824.649 INFO             PET3 model1:  PE=4 SSI=0 SSIPE=4
20220327 015824.649 INFO             PET3 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220327 015824.649 INFO             PET3 model1:  PE=5 SSI=0 SSIPE=5
20220327 015824.649 INFO             PET3 model1: --- VMK::log() end ---------------------------------------
20220327 015824.649 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220327 015824.775 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220327 015824.900 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220327 015825.026 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220327 015825.151 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220327 015825.276 INFO             PET3 Exiting 'user1_run'
20220327 015825.278 INFO             PET3 Entering 'user2_run'
20220327 015825.278 INFO             PET3 model2: --- VMK::log() start -------------------------------------
20220327 015825.278 INFO             PET3 model2: vm located at: 0x8bf290
20220327 015825.278 INFO             PET3 model2: petCount=2 localPet=1 mypthid=140737352203136 currentSsiPe=3
20220327 015825.278 INFO             PET3 model2: Current system level affinity pinning for local PET:
20220327 015825.278 INFO             PET3 model2:  SSIPE=3
20220327 015825.278 INFO             PET3 model2: Current system level OMP_NUM_THREADS setting for local PET: 3
20220327 015825.278 INFO             PET3 model2: ssiCount=1 localSsi=0
20220327 015825.278 INFO             PET3 model2: mpionly=1 threadsflag=0
20220327 015825.278 INFO             PET3 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220327 015825.278 INFO             PET3 model2: PET=0 lpid=0 tid=0 pid=0 peCount=3 accCount=0
20220327 015825.278 INFO             PET3 model2:  PE=0 SSI=0 SSIPE=0
20220327 015825.278 INFO             PET3 model2:  PE=1 SSI=0 SSIPE=1
20220327 015825.278 INFO             PET3 model2:  PE=2 SSI=0 SSIPE=2
20220327 015825.278 INFO             PET3 model2: PET=1 lpid=1 tid=0 pid=3 peCount=3 accCount=0
20220327 015825.278 INFO             PET3 model2:  PE=3 SSI=0 SSIPE=3
20220327 015825.278 INFO             PET3 model2:  PE=4 SSI=0 SSIPE=4
20220327 015825.278 INFO             PET3 model2:  PE=5 SSI=0 SSIPE=5
20220327 015825.278 INFO             PET3 model2: --- VMK::log() end ---------------------------------------
20220327 015825.279 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220327 015825.279 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220327 015825.279 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220327 015825.581 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220327 015825.581 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220327 015825.581 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220327 015825.878 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220327 015825.878 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220327 015825.878 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220327 015826.175 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220327 015826.175 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220327 015826.175 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220327 015826.472 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220327 015826.472 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220327 015826.472 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220327 015826.769 INFO             PET3  user2_run: All data correct.
20220327 015826.769 INFO             PET3 Exiting 'user2_run'
20220327 015826.769 INFO             PET3 Entering 'user1_run'
20220327 015826.769 INFO             PET3 model1: --- VMK::log() start -------------------------------------
20220327 015826.769 INFO             PET3 model1: vm located at: 0x9fb9f0
20220327 015826.769 INFO             PET3 model1: petCount=6 localPet=3 mypthid=140737352203136 currentSsiPe=3
20220327 015826.769 INFO             PET3 model1: Current system level affinity pinning for local PET:
20220327 015826.769 INFO             PET3 model1:  SSIPE=3
20220327 015826.769 INFO             PET3 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220327 015826.769 INFO             PET3 model1: ssiCount=1 localSsi=0
20220327 015826.769 INFO             PET3 model1: mpionly=1 threadsflag=0
20220327 015826.769 INFO             PET3 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220327 015826.769 INFO             PET3 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220327 015826.769 INFO             PET3 model1:  PE=0 SSI=0 SSIPE=0
20220327 015826.769 INFO             PET3 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220327 015826.769 INFO             PET3 model1:  PE=1 SSI=0 SSIPE=1
20220327 015826.769 INFO             PET3 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220327 015826.769 INFO             PET3 model1:  PE=2 SSI=0 SSIPE=2
20220327 015826.770 INFO             PET3 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220327 015826.770 INFO             PET3 model1:  PE=3 SSI=0 SSIPE=3
20220327 015826.770 INFO             PET3 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220327 015826.770 INFO             PET3 model1:  PE=4 SSI=0 SSIPE=4
20220327 015826.770 INFO             PET3 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220327 015826.770 INFO             PET3 model1:  PE=5 SSI=0 SSIPE=5
20220327 015826.770 INFO             PET3 model1: --- VMK::log() end ---------------------------------------
20220327 015826.770 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220327 015826.895 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220327 015827.020 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220327 015827.146 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220327 015827.271 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220327 015827.396 INFO             PET3 Exiting 'user1_run'
20220327 015827.397 INFO             PET3 Entering 'user2_run'
20220327 015827.397 INFO             PET3 model2: --- VMK::log() start -------------------------------------
20220327 015827.397 INFO             PET3 model2: vm located at: 0x8bf290
20220327 015827.397 INFO             PET3 model2: petCount=2 localPet=1 mypthid=140737352203136 currentSsiPe=3
20220327 015827.397 INFO             PET3 model2: Current system level affinity pinning for local PET:
20220327 015827.397 INFO             PET3 model2:  SSIPE=3
20220327 015827.397 INFO             PET3 model2: Current system level OMP_NUM_THREADS setting for local PET: 3
20220327 015827.397 INFO             PET3 model2: ssiCount=1 localSsi=0
20220327 015827.397 INFO             PET3 model2: mpionly=1 threadsflag=0
20220327 015827.397 INFO             PET3 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220327 015827.397 INFO             PET3 model2: PET=0 lpid=0 tid=0 pid=0 peCount=3 accCount=0
20220327 015827.397 INFO             PET3 model2:  PE=0 SSI=0 SSIPE=0
20220327 015827.397 INFO             PET3 model2:  PE=1 SSI=0 SSIPE=1
20220327 015827.397 INFO             PET3 model2:  PE=2 SSI=0 SSIPE=2
20220327 015827.397 INFO             PET3 model2: PET=1 lpid=1 tid=0 pid=3 peCount=3 accCount=0
20220327 015827.397 INFO             PET3 model2:  PE=3 SSI=0 SSIPE=3
20220327 015827.397 INFO             PET3 model2:  PE=4 SSI=0 SSIPE=4
20220327 015827.397 INFO             PET3 model2:  PE=5 SSI=0 SSIPE=5
20220327 015827.397 INFO             PET3 model2: --- VMK::log() end ---------------------------------------
20220327 015827.397 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220327 015827.397 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220327 015827.398 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220327 015827.694 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220327 015827.694 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220327 015827.694 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220327 015827.991 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220327 015827.992 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220327 015827.992 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220327 015828.289 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220327 015828.289 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220327 015828.289 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220327 015828.586 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220327 015828.586 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220327 015828.586 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220327 015828.883 INFO             PET3  user2_run: All data correct.
20220327 015828.883 INFO             PET3 Exiting 'user2_run'
20220327 015828.883 INFO             PET3  NUMBER_OF_PROCESSORS           6
20220327 015828.883 INFO             PET3  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220327 015828.883 INFO             PET3 Finalizing ESMF
20220327 015824.555 INFO             PET4 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220327 015824.555 INFO             PET4 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220327 015824.555 INFO             PET4 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220327 015824.555 INFO             PET4 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220327 015824.555 INFO             PET4 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220327 015824.555 INFO             PET4 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220327 015824.555 INFO             PET4 Running with ESMF Version   : v8.3.0b10-105-ga5295e34ae
20220327 015824.555 INFO             PET4 ESMF library build date/time: "Mar 27 2022" "01:23:54"
20220327 015824.555 INFO             PET4 ESMF library build location : /gpfsm/dnb04/projects/p98/mpotts/esmf/intel_19.1.3_intelmpi_O_develop
20220327 015824.555 INFO             PET4 ESMF_COMM                   : intelmpi
20220327 015824.555 INFO             PET4 ESMF_MOAB                   : enabled
20220327 015824.555 INFO             PET4 ESMF_LAPACK                 : enabled
20220327 015824.555 INFO             PET4 ESMF_NETCDF                 : enabled
20220327 015824.555 INFO             PET4 ESMF_PNETCDF                : disabled
20220327 015824.555 INFO             PET4 ESMF_PIO                    : enabled
20220327 015824.555 INFO             PET4 ESMF_YAMLCPP                : enabled
20220327 015824.555 INFO             PET4 --- VMK::logSystem() start -------------------------------
20220327 015824.555 INFO             PET4 esmfComm=intelmpi
20220327 015824.555 INFO             PET4 isPthreadsEnabled=1
20220327 015824.555 INFO             PET4 isOpenMPEnabled=1
20220327 015824.555 INFO             PET4 isOpenACCEnabled=0
20220327 015824.555 INFO             PET4 isSsiSharedMemoryEnabled=1
20220327 015824.556 INFO             PET4 ssiCount=1 peCount=6
20220327 015824.556 INFO             PET4 PE=0 SSI=0 SSIPE=0
20220327 015824.556 INFO             PET4 PE=1 SSI=0 SSIPE=1
20220327 015824.556 INFO             PET4 PE=2 SSI=0 SSIPE=2
20220327 015824.556 INFO             PET4 PE=3 SSI=0 SSIPE=3
20220327 015824.556 INFO             PET4 PE=4 SSI=0 SSIPE=4
20220327 015824.556 INFO             PET4 PE=5 SSI=0 SSIPE=5
20220327 015824.556 INFO             PET4 --- VMK::logSystem() MPI Control Variables ---------------
20220327 015824.556 INFO             PET4 index=   0                                          I_MPI_DEBUG_OUTPUT : @Default:# not defined
20220327 015824.556 INFO             PET4 index=   1                                        I_MPI_DEBUG_COREDUMP : @Default:# 1
20220327 015824.556 INFO             PET4 index=   2                                          I_MPI_LIBRARY_KIND : @Default:# not defined
20220327 015824.556 INFO             PET4 index=   3                                  I_MPI_OFI_LIBRARY_INTERNAL : @Default:# not defined
20220327 015824.556 INFO             PET4 index=   4                                            I_MPI_CC_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET4 index=   5                                           I_MPI_CXX_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET4 index=   6                                            I_MPI_FC_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET4 index=   7                                           I_MPI_F77_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET4 index=   8                                           I_MPI_F90_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET4 index=   9                                         I_MPI_TRACE_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  10                                         I_MPI_CHECK_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  11                                        I_MPI_CHECK_COMPILER : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  12                                                    I_MPI_CC : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  13                                                   I_MPI_CXX : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  14                                                    I_MPI_FC : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  15                                                   I_MPI_F90 : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  16                                                   I_MPI_F77 : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  17                                                  I_MPI_ROOT : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  18                                           I_MPI_ONEAPI_ROOT : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  19                                           MPIR_CVAR_VT_ROOT : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  20                                   I_MPI_COMPILER_CONFIG_DIR : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  21                                                  I_MPI_LINK : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  22                                      I_MPI_DEBUG_INFO_STRIP : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  23                                                I_MPI_CFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  24                                              I_MPI_CXXFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  25                                               I_MPI_FCFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  26                                                I_MPI_FFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  27                                               I_MPI_LDFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  28                                             I_MPI_FORT_BIND : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  29                                           I_MPI_AUTH_METHOD : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  30                               I_MPI_HYDRA_COLLECTIVE_LAUNCH : @Default:# 1
20220327 015824.556 INFO             PET4 index=  31                                  I_MPI_HYDRA_UNIQUE_PROXIES : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  32                                        I_MPI_FAULT_CONTINUE : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  33                                   I_MPI_FAULT_NODE_CONTINUE : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  34                                                I_MPI_MPIRUN : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  35                                            I_MPI_BIND_ORDER : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  36                                             I_MPI_BIND_NUMA : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  37                                     I_MPI_BIND_WIN_ALLOCATE : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  38                                      I_MPI_HYDRA_NAMESERVER : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  39                                        I_MPI_JOB_CHECK_LIBS : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  40                                    I_MPI_HYDRA_SERVICE_PORT : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  41                                           I_MPI_HYDRA_DEBUG : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  42                                             I_MPI_HYDRA_ENV : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  43                                           I_MPI_JOB_TIMEOUT : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  44                                       I_MPI_MPIEXEC_TIMEOUT : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  45                                   I_MPI_JOB_STARTUP_TIMEOUT : Set this environment variable to make mpiexec.hydra terminate$ the job in <timeout> seconds if some processes are not launched. @Default:# -1
20220327 015824.556 INFO             PET4 index=  46                                    I_MPI_JOB_TIMEOUT_SIGNAL : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  47                                      I_MPI_JOB_ABORT_SIGNAL : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  48                                I_MPI_JOB_SIGNAL_PROPAGATION : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  49                                  I_MPI_HYDRA_BOOTSTRAP_EXEC : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  50                       I_MPI_HYDRA_BOOTSTRAP_EXEC_EXTRA_ARGS : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  51                              I_MPI_HYDRA_BOOTSTRAP_AUTOFORK : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  52                                             I_MPI_HYDRA_RMK : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  53                                     I_MPI_HYDRA_PMI_CONNECT : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  54                                         I_MPI_HYDRA_TOPOLIB : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  55                                            I_MPI_PORT_RANGE : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  56                         I_MPI_JOB_RESPECT_PROCESS_PLACEMENT : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  57                                                I_MPI_TMPDIR : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  58                                           I_MPI_HYDRA_DEMUX : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  59                                           I_MPI_HYDRA_IFACE : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  60                                I_MPI_HYDRA_GDB_REMOTE_SHELL : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  61                                   I_MPI_HYDRA_PMI_AGGREGATE : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  62                                        I_MPI_JOB_TRACE_LIBS : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  63                                       I_MPI_HYDRA_HOST_FILE : Set the host file to run the application.$ Syntax$ I_MPI_HYDRA_HOST_FILE=<arg>$ Arguments$ <arg> - String parameter$ -----------------------------------------------------------------------$ <hostsfile> - The full or relative path to the host file$ ----------------------------------------------------------------------- @Default:# not defined
20220327 015824.556 INFO             PET4 index=  64                                     I_MPI_HYDRA_HOSTS_GROUP : This environment variable allows to set node ranges using brackets,$ commas, and dashes (like in Slurm* Workload Manager). @Default:# not defined
20220327 015824.556 INFO             PET4 index=  65                                               I_MPI_PERHOST : Define the default behavior for the -perhost option of the mpiexec.hydra$ command.$ Syntax$ I_MPI_PERHOST=<value>$ Arguments$ <value> - Define a value used for -perhost by default$ -----------------------------------------------------------------------$ <integer > 0> - Exact value for the option$ <all>         - All logical CPUs on the node$ <allcores>    - All cores (physical CPUs) on the node. This is the$  default value.$ ----------------------------------------------------------------------- @Default:# not defined
20220327 015824.556 INFO             PET4 index=  66                                                 I_MPI_GTOOL : Specify the tools to be launched for selected ranks. An alternative to$ this variable is the -gtool option$ Syntax$ I_MPI_GTOOL="<command line for a tool 1>:<ranks set 1>[=exclusive]$ [@arch 1];<command line for a tool 2>:<ranks set 2>[=exclusive]$ [@arch 2]; â€¦ ;<command line for a tool n>:<ranks set n>[=exclusive]$ [@arch n]"$ Arguments$ <arg> - Specify a tool launch command, including parameters.$ -----------------------------------------------------------------------$ <command line>     - Specify tool launch command, including parameters$ <rank set>         - Specify the range of ranks that are involved in the$  tool execution. Separate ranks with a comma or use the '-' symbol for$ a set ofcontiguous ranks. To run the tool forall ranks, use the all$  argument.$ [=exclusive]       - All cores (physical CPUs) on the node. This is$ the default value.$ [@arch]            - Specify the architecture on which the tool runs$  optional). For a given <rank set>, if you specify this argument,$ the tool is launched
20220327 015824.556 INFO             PET4 index=  67                                    I_MPI_HYDRA_BRANCH_COUNT : Set this environment variable to restrict the number of child management$ processes launched by the mpiexec.hydra operation or by each pmi_proxy$ anagement process.$ Syntax$ I_MPI_HYDRA_BRANCH_COUNT=<num>$ Arguments$ <value> - Number$ -----------------------------------------------------------------------$ <n> >= 0 - The default value is -1 if less than 128 nodes are used. $ This value also means that there is no hierarchical structure$ The default value is 32 if more than 127 nodes are used$ ----------------------------------------------------------------------- @Default:# not defined
20220327 015824.556 INFO             PET4 index=  68                                       I_MPI_HYDRA_BOOTSTRAP : Set this environment variable to specify the bootstrap server Syntax$ I_MPI_HYDRA_BOOTSTRAP=<arg>$ Arguments$ <arg> - String parameter$ -----------------------------------------------------------------------$ <ssh>     - Use secure shell. This is the default value$ <rsh>     - Use remote shell$ <pdsh>    - Use parallel distributed shell$ <pbsdsh>  - Use Torque* and PBS* pbsdsh command$ <fork>    - Use fork call$ <slurm>   - Use SLURM* srun command$ <ll>      - Use LoadLeveler* llspawn.stdio command$ <lsf>     - Use LSF* blaunch command$ <sge>     - Use Univa* Grid Engine* qrsh command$ ----------------------------------------------------------------------- @Default:# not defined
20220327 015824.556 INFO             PET4 index=  69                                              I_MPI_PIN_UNIT : @Default:# not defined
20220327 015824.556 INFO             PET4 index=  70                                                 I_MPI_STATS :
20220327 015824.556 INFO             PET4 index=  71                                             I_MPI_TIMER_ART : @Default:# 1
20220327 015824.556 INFO             PET4 index=  72                                   I_MPI_OFFLOAD_DOMAIN_SIZE : Control the number of devices/tiles per MPI rank
20220327 015824.556 INFO             PET4 index=  73                                          I_MPI_OFFLOAD_CELL : Variable to choose the base unit: tile or device
20220327 015824.556 INFO             PET4 index=  74                                       I_MPI_OFFLOAD_DEVICES : Variable to select available devices
20220327 015824.556 INFO             PET4 index=  75                                   I_MPI_OFFLOAD_DEVICE_LIST : A comma-separated list of tiles and/or ranges of tiles.$ The process with the i-th rank is pinned to the i-th tile in the list.$ If I_MPI_OFFLOAD_CELL=device then it is comma-separated list of devices.
20220327 015824.556 INFO             PET4 index=  76                                        I_MPI_OFFLOAD_DOMAIN : Define domains through the comma separated list of hexadecimal numbers (domain masks).
20220327 015824.557 INFO             PET4 index=  77                               I_MPI_OFFLOAD_INFO_SET_NTILES : Set the number of tiles
20220327 015824.557 INFO             PET4 index=  78                                I_MPI_OFFLOAD_INFO_SET_NGPUS : Set the number of gpus
20220327 015824.557 INFO             PET4 index=  79                           I_MPI_OFFLOAD_INFO_SET_NNUMANODES : Set the number of numanodes
20220327 015824.557 INFO             PET4 index=  80                               I_MPI_OFFLOAD_INFO_SET_GPU_ID : Set gpu id for each tile
20220327 015824.557 INFO             PET4 index=  81                 I_MPI_OFFLOAD_INFO_SET_NUMANODE_ID_FOR_GPUS : Set numanode id for each gpu
20220327 015824.557 INFO             PET4 index=  82                I_MPI_OFFLOAD_INFO_SET_NUMANODE_ID_FOR_RANKS : Set numanode id for each rank
20220327 015824.557 INFO             PET4 index=  83                            I_MPI_OFFLOAD_INFO_SET_VENDOR_ID : Set vendor id for each device
20220327 015824.557 INFO             PET4 index=  84                                         MPIR_CVAR_INTEL_PIN : Turn on/off process pinning.$ Syntax$ I_MPI_PIN=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 - Enable process pinning$ > disable | no | off | 0 - Disable processes pinning$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN @Default:# on
20220327 015824.557 INFO             PET4 index=  85                          MPIR_CVAR_INTEL_PIN_SHOW_REAL_MASK : Turn on/off real masks pinning print.$ Syntax$ I_MPI_PIN_SHOW_REAL_MASK=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 - Enable real pinning print$ > disable | no | off | 0 - Disable real pinning print$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_SHOW_REAL_MASK @Default:# on
20220327 015824.557 INFO             PET4 index=  86                          MPIR_CVAR_INTEL_PIN_PROCESSOR_LIST : Define a processor subset and the mapping rules for MPI processes within$ this subset.$ Syntax$ I_MPI_PIN_PROCESSOR_LIST=<value>$ The environment variable value has the following syntax forms:$ 1. <proclist>$ 2. [<procset>][:[grain=<grain>][,shift=<shift>]$ [,preoffset=<preoffset>][,postoffset=<postoffset>]$ 3. [<procset>][:map=<map>]$ @Alias:# I_MPI_PIN_PROCESSOR_LIST @Default:# not defined
20220327 015824.557 INFO             PET4 index=  87                  MPIR_CVAR_INTEL_PIN_PROCESSOR_EXCLUDE_LIST : Define a subset of logical processors to be excluded for the pinning$ capability on the intended hosts.$ Syntax$ I_MPI_PIN_PROCESSOR_EXCLUDE_LIST=<proclist>$ Arguments$ <proclist> - A comma-separated list of logical processor numbers$ and/or ranges of processors.$ -----------------------------------------------------------------------$ > <l> - Processor with logical number <l>.$ > <l>-<m> - Range of processors with logical numbers from <l> to <m>.$ > <k>,<l>-<m> - Processors <k>, as well as <l> through <m>.$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_PROCESSOR_EXCLUDE_LIST @Default:# not defined
20220327 015824.557 INFO             PET4 index=  88                                    MPIR_CVAR_INTEL_PIN_CELL : Set this environment variable to define the pinning resolution$ granularity. I_MPI_PIN_CELL specifies the minimal processor cell$ allocated when an MPI process is running.$ Syntax$ I_MPI_PIN_CELL=<cell>$ Arguments$ <cell> - Specify the resolution granularity$ -----------------------------------------------------------------------$ > unit - Basic processor unit (logical CPU)$ > core - Physical processor core$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_CELL @Default:# unit
20220327 015824.557 INFO             PET4 index=  89                          MPIR_CVAR_INTEL_PIN_RESPECT_CPUSET : Respect the process affinity mask.$ Syntax$ I_MPI_PIN_RESPECT_CPUSET=<value>$ Arguments$ <value> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 - Respect the process affinity mask$ > disable | no | off | 0 - Do not respect the process affinity mask$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_RESPECT_CPUSET @Default:# on
20220327 015824.557 INFO             PET4 index=  90                             MPIR_CVAR_INTEL_PIN_RESPECT_HCA : In the presence of Intel(R) Omni-Path Architecture (Intel(R) OPA) or$ Infiniband architecture* host channel adapter (IBA* HCA),$ adjust the pinning according to the location of adapter.$ Syntax$ I_MPI_PIN_RESPECT_HCA=<value>$ Arguments$ <value> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 Use the location of IBA HCA if available$ > disable | no | off | 0 Do not use the location of IBA HCA$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_RESPECT_HCA @Default:# on
20220327 015824.557 INFO             PET4 index=  91                                  MPIR_CVAR_INTEL_PIN_DOMAIN : Intel(R) MPI Library provides environment variable to control process pinning for hybrid MPI/OpenMP* applications. This environment variable is used to define a number of non-overlapping subsets (domains) of logical processors on a node, and a set of rules on how MPI processes are bound to these domains by the following formula: one MPI process per one domain. Multi-core Shape:$ I_MPI_PIN_DOMAIN=<mc-shape>$ <mc-shape> - Define domains through multi-core terms.$ -----------------------------------------------------------------------$ > core - Each domain consists of the logical processors that share a$ particular core. The number of domains on a node is equal to the number$ of cores on the node.$ > socket | sock - Each domain consists of the logical processors that$ share a particular socket. The number of domains on a node is equal to$ the number of sockets on the node.$ > numa - Each domain consists of the logical processors that share a$ particular NUMA node. The number of domains on a machine is equal to$
20220327 015824.557 INFO             PET4 index=  92                                   MPIR_CVAR_INTEL_PIN_ORDER : Set this environment variable to define the mapping order for MPI$ processes to domains as specified by the$ I_MPI_PIN_DOMAIN environment variable.$ Syntax$ I_MPI_PIN_ORDER=<order>$ <order> - Specify the ranking order$ -----------------------------------------------------------------------$ > range - The domains are ordered according to the processor's BIOS$ numbering. This is a platform dependent numbering$ > scatter - The domains are ordered so that adjacent domains have$ minimal sharing of common resources$ > compact - The domains are ordered so that adjacent domains share$ common resources as much as possible. This is the default value$ > spread - The domains are ordered consecutively with the possibility$ not to share common resources$ > bunch - The processes are mapped proportionally to sockets and the$ domains are ordered as close as possible on the sockets$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_ORDER @Default:# compact
20220327 015824.557 INFO             PET4 index=  93                                   MPIR_CVAR_IMPI_HBW_POLICY : @Alias:# I_MPI_HBW_POLICY
20220327 015824.557 INFO             PET4 index=  94                          MPIR_CVAR_IMPI_INTERNAL_MEM_POLICY : @Alias:# I_MPI_INTERNAL_MEM_POLICY
20220327 015824.557 INFO             PET4 index=  95                                 MPIR_CVAR_IMPI_STATIC_BUILD : @Alias:# I_MPI_STATIC_BUILD
20220327 015824.557 INFO             PET4 index=  96                     MPIR_CVAR_IMPI_RETURN_INTERNAL_MEM_NUMA : @Alias:# I_MPI_RETURN_INTERNAL_MEM_NUMA
20220327 015824.557 INFO             PET4 index=  97                                   MPIR_CVAR_OFFLOAD_TOPOLIB : @Alias:# I_MPI_OFFLOAD_TOPOLIB
20220327 015824.557 INFO             PET4 index=  98                                        MPIR_CVAR_ENABLE_GPU : Control support of buffers offloaded to GPU/accelerator in MPI calls.$ Syntax$ I_MPI_OFFLOAD=<value>$ Arguments$ <value> - choice$ -----------------------------------------------------------------------$ <0> - Disabled$ <1> - Enabled only if Level Zero library is loaded at MPI_Init() time$ <2> - Enabled, will fail if Level Zero library is not loadable$ ----------------------------------------------------------------------- @Alias:# I_MPI_OFFLOAD @Default:# 0
20220327 015824.557 INFO             PET4 index=  99                        MPIR_CVAR_ENABLE_GPU_BUFFER_CHECKING : Turn on/off bounds checking of the offloaded buffers.$ @Alias:# I_MPI_OFFLOAD_BUFFER_CHECKING @Default:# 1
20220327 015824.557 INFO             PET4 index= 100                        MPIR_CVAR_OFFLOAD_LEVEL_ZERO_LIBRARY : Specify name or full path to Level Zero ze_loader library.$ @Alias:# I_MPI_OFFLOAD_LEVEL_ZERO_LIBRARY
20220327 015824.557 INFO             PET4 index= 101                            MPIR_CVAR_INTEL_EXTRA_FILESYSTEM : Turn on/off native parallel file systems support.$ Syntax$ I_MPI_EXTRA_FILESYSTEM=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 - Enable native support for parallel file$ systems.$ > disable | no | off | 0 - Disable native support for parallel file$ systems.$ ----------------------------------------------------------------------- @Alias:# I_MPI_EXTRA_FILESYSTEM @Default:# off
20220327 015824.557 INFO             PET4 index= 102                      MPIR_CVAR_INTEL_EXTRA_FILESYSTEM_FORCE : Force filesystem recognition logic.$ Syntax$ I_MPI_EXTRA_FILESYSTEM_FORCE=<ufs|nfs|gpfs|panfs|lustre|daos>$ @Alias:# I_MPI_EXTRA_FILESYSTEM_FORCE @Default:# not defined
20220327 015824.557 INFO             PET4 index= 103                                     MPIR_CVAR_INTEL_FABRICS : Select the particular fabrics to be used.$ Syntax$ I_MPI_FABRICS=<ofi|shm:ofi>$ Arguments$ <fabric> -  Define a network fabric.$ -----------------------------------------------------------------------$ > shm - Shared memory transport (used for intra-node$ communication only).$ > ofi - OpenFabrics Interfaces* (OFI)-capable network fabrics, such as$ Intel(R) True Scale Fabric, Intel(R) Omni-Path Architecture, InfiniBand*$ and Ethernet (through OFI$ API).$ ----------------------------------------------------------------------- @Alias:# I_MPI_FABRICS @Default:# shm:ofi
20220327 015824.557 INFO             PET4 index= 104                                       MPIR_CVAR_IMPI_MALLOC : Enable or disable the Intel MPI private memory allocator.$ ----------------------------------------------------------------------- @Alias:# I_MPI_MALLOC @Default:# 1
20220327 015824.557 INFO             PET4 index= 105                                    MPIR_CVAR_INTEL_SHM_HEAP : Enable or disable the Intel MPI shared memory allocator.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP @Default:# -1
20220327 015824.557 INFO             PET4 index= 106                                MPIR_CVAR_INTEL_SHM_HEAP_OPT : Shared memory heap optimization: "rank", "numa".$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_OPT @Default:# -1
20220327 015824.557 INFO             PET4 index= 107                              MPIR_CVAR_INTEL_SHM_HEAP_VSIZE : Set shared memory heap virtual size (in MB).$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_VSIZE @Default:# -1
20220327 015824.557 INFO             PET4 index= 108                              MPIR_CVAR_INTEL_SHM_HEAP_CSIZE : Set shared memory heap cache size (in MB).$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_CSIZE @Default:# -1
20220327 015824.557 INFO             PET4 index= 109                  MPIR_CVAR_INTEL_SHM_HEAP_NCONTIG_THRESHOLD : Set non-contig object size threshold for use shared memory allocator.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_NCONTIG_THRESHOLD @Default:# -1
20220327 015824.557 INFO             PET4 index= 110                          MPIR_CVAR_INTEL_SHM_HEAP_THRESHOLD : Set object size threshold for use shared memory allocator.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_THRESHOLD @Default:# -1
20220327 015824.557 INFO             PET4 index= 111        MPIR_CVAR_INTEL_SHM_RECV_HEAP_SHORT_MEMCPY_THRESHOLD : Threshold for short size messages receive via SHM HEAP transport.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_HEAP_SHORT_MEMCPY_THRESHOLD @Default:# -1
20220327 015824.557 INFO             PET4 index= 112      MPIR_CVAR_INTEL_SHM_RECV_HEAP_REGULAR_MEMCPY_THRESHOLD : Threshold for regular size message receive via SHM HEAP transport.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_HEAP_REGULAR_MEMCPY_THRESHOLD @Default:# -1
20220327 015824.557 INFO             PET4 index= 113            MPIR_CVAR_INTEL_SHM_RECV_INTER_HEAP_SHORT_MEMCPY : Name of memory copy function for short message receive via SHM HEAP transport, inter socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTER_HEAP_SHORT_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET4 index= 114            MPIR_CVAR_INTEL_SHM_RECV_INTRA_HEAP_SHORT_MEMCPY : Name of memory copy function for short message receive via SHM HEAP transport, intra socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTRA_HEAP_SHORT_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET4 index= 115          MPIR_CVAR_INTEL_SHM_RECV_INTER_HEAP_REGULAR_MEMCPY : Name of memory copy function for regular receive messages via SHM HEAP transport, inter socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTER_HEAP_REGULAR_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET4 index= 116          MPIR_CVAR_INTEL_SHM_RECV_INTRA_HEAP_REGULAR_MEMCPY : Name of memory copy function for regular receive messages via SHM HEAP transport, intra socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTRA_HEAP_REGULAR_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET4 index= 117                  MPIR_CVAR_INTEL_SHM_RECV_INTER_HEAP_MEMCPY : Name of memory copy function for receive messages via SHM HEAP transport, inter socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTER_HEAP_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET4 index= 118                  MPIR_CVAR_INTEL_SHM_RECV_INTRA_HEAP_MEMCPY : Name of memory copy function for receive messages via SHM HEAP transport, intra socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTRA_HEAP_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET4 index= 119                               MPIR_CVAR_CH4_SHM_POSIX_EAGER : Select a shared memory transport to be used.$ Syntax$ I_MPI_SHM=<transport>$ Arguments$ <transport> - Define a shared memory transport solution.$ -----------------------------------------------------------------------$ > disable | no | off | 0 - Do not use shared memory transport.$ > auto - Select a shared memory transport solution automatically.$ > bdw_sse - The shared memory transport solution tuned for Intel(R)$ microarchitecture code name Broadwell. The SSE/SSE2/SSE3 instruction$ set is used.$ > bdw_avx2 - The shared memory transport solution tuned for Intel(R)$ microarchitecture code name Broadwell. The AVX2 instruction set is used.$ > skx_sse - The shared memory transport solution tuned for Intel(R)$ Xeon(R) processors based on Intel(R) microarchitecture code name Skylake.$ The SSE/SSE2/SSE3 instruction set is used.$ > skx_avx2 - The shared memory transport solution tuned for Intel(R)$ Xeon(R) processors based on Intel(R) microarchitecture code name Skylake.$ The AVX2 instruction set is used.$ > skx_av
20220327 015824.557 INFO             PET4 index= 120                                     MPIR_CVAR_INTEL_SHM_OPT : Select a shared memory transport optimization strategy to be used.$ Syntax$ I_MPI_SHM_OPT=<optimization_strategy>$ Arguments$ <optimization_strategy> - Define a shared memory transport optimization strategy.$ -----------------------------------------------------------------------$ > dynamic - Let shared memory transport make decision in runtime.$ > intra - Optimize intra socket message passing.$ > inter - Optimize inter socket message passing.$ -----------------------------------------------------------------------$ @Alias:# I_MPI_SHM_OPT @Default:# dynamic
20220327 015824.557 INFO             PET4 index= 121                           MPIR_CVAR_INTEL_SHM_CELL_FWD_SIZE : Change the size of a shared memory forward cell. @Alias:# I_MPI_SHM_CELL_FWD_SIZE @Default:# -1
20220327 015824.557 INFO             PET4 index= 122                           MPIR_CVAR_INTEL_SHM_CELL_BWD_SIZE : Change the size of a shared memory backward cell. @Alias:# I_MPI_SHM_CELL_BWD_SIZE @Default:# -1
20220327 015824.557 INFO             PET4 index= 123                           MPIR_CVAR_INTEL_SHM_CELL_EXT_SIZE : Change the size of a shared memory extended cell. @Alias:# I_MPI_SHM_CELL_EXT_SIZE @Default:# -1
20220327 015824.557 INFO             PET4 index= 124                            MPIR_CVAR_INTEL_SHM_CELL_FWD_NUM : Change the number of forward cells in the shared memory transport (per rank). @Alias:# I_MPI_SHM_CELL_FWD_NUM @Default:# -1
20220327 015824.557 INFO             PET4 index= 125                       MPIR_CVAR_INTEL_SHM_CELL_FWD_HOLD_NUM : Change the number of hold forward cells in the shared memory transport (per rank). @Alias:# I_MPI_SHM_CELL_FWD_HOLD_NUM @Default:# -1
20220327 015824.557 INFO             PET4 index= 126                            MPIR_CVAR_INTEL_SHM_CELL_BWD_NUM : Change the number of backward cells in the shared memory transport (per rank). @Alias:# I_MPI_SHM_CELL_BWD_NUM @Default:# -1
20220327 015824.557 INFO             PET4 index= 127                     MPIR_CVAR_INTEL_SHM_CELL_BWD_NUMA_AWARE : Use NUMA aware backward cells (1 : true, 0 : false, -1 : auto) (per rank). @Alias:# I_MPI_SHM_CELL_BWD_NUMA_AWARE @Default:# -1
20220327 015824.557 INFO             PET4 index= 128                      MPIR_CVAR_INTEL_SHM_CELL_EXT_NUM_TOTAL : Change the total number of extended cells in the shared memory$ transport. @Alias:# I_MPI_SHM_CELL_EXT_NUM_TOTAL @Default:# -1
20220327 015824.557 INFO             PET4 index= 129                            MPIR_CVAR_INTEL_SHM_MCDRAM_LIMIT : Change the size of the shared memory bound to the multi-channel DRAM (MCDRAM) (size per rank). @Alias:# I_MPI_SHM_MCDRAM_LIMIT @Default:# -1
20220327 015824.557 INFO             PET4 index= 130                         MPIR_CVAR_INTEL_SHM_SEND_SPIN_COUNT : Control the spin count value for the shared memory transport for sending messages. @Alias:# I_MPI_SHM_SEND_SPIN_COUNT @Default:# -1
20220327 015824.557 INFO             PET4 index= 131                         MPIR_CVAR_INTEL_SHM_RECV_SPIN_COUNT : Control the spin count value for the shared memory transport for receiving messages. @Alias:# I_MPI_SHM_RECV_SPIN_COUNT @Default:# -1
20220327 015824.557 INFO             PET4 index= 132                          MPIR_CVAR_INTEL_SHM_FILE_PREFIX_4K : Mount point of 4K page size shm files. @Alias:# I_MPI_SHM_FILE_PREFIX_4K @Default:# ""
20220327 015824.557 INFO             PET4 index= 133                          MPIR_CVAR_INTEL_SHM_FILE_PREFIX_2M : Mount point of 2M huge page size shm files. @Alias:# I_MPI_SHM_FILE_PREFIX_2M @Default:# ""
20220327 015824.557 INFO             PET4 index= 134                          MPIR_CVAR_INTEL_SHM_FILE_PREFIX_1G : Mount point of 1G huge page size shm files. @Alias:# I_MPI_SHM_FILE_PREFIX_1G @Default:# ""
20220327 015824.557 INFO             PET4 index= 135                                   MPIR_CVAR_INTEL_SHM_PAUSE : The number of pauses repeated. @Alias:# I_MPI_SHM_PAUSE
20220327 015824.557 INFO             PET4 index= 136                         MPIR_CVAR_INTEL_SHM_EAGER_THRESHOLD : Eager threshold. @Alias:# I_MPI_SHM_EAGER_THRESHOLD
20220327 015824.557 INFO             PET4 index= 137                               MPIR_CVAR_INTEL_SHM_RING_SIZE : @Alias:# I_MPI_SHM_RING_SIZE
20220327 015824.557 INFO             PET4 index= 138                      MPIR_CVAR_INTEL_SHM_RING_ACK_THRESHOLD : @Alias:# I_MPI_SHM_RING_ACK_THRESHOLD
20220327 015824.557 INFO             PET4 index= 139                          MPIR_CVAR_INTEL_SHM_CELL_FWD_FIRST : @Alias:# I_MPI_SHM_CELL_FWD_FIRST
20220327 015824.557 INFO             PET4 index= 140                      MPIR_CVAR_INTEL_SHM_PROFILER_DIRECTORY : @Alias:# I_MPI_SHM_PROFILER_DIRECTORY
20220327 015824.557 INFO             PET4 index= 141                         MPIR_CVAR_INTEL_SHM_TRACE_DIRECTORY : @Alias:# I_MPI_SHM_TRACE_DIRECTORY
20220327 015824.557 INFO             PET4 index= 142                              MPIR_CVAR_INTEL_SHM_FRAME_SIZE : @Alias:# I_MPI_SHM_FRAME_SIZE
20220327 015824.557 INFO             PET4 index= 143                         MPIR_CVAR_INTEL_SHM_FRAME_THRESHOLD : @Alias:# I_MPI_SHM_FRAME_THRESHOLD
20220327 015824.557 INFO             PET4 index= 144                        MPIR_CVAR_INTEL_SHM_SEND_TINY_MEMCPY : @Alias:# I_MPI_SHM_SEND_TINY_MEMCPY
20220327 015824.557 INFO             PET4 index= 145                  MPIR_CVAR_INTEL_SHM_SEND_INTRA_RING_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_RING_MEMCPY
20220327 015824.557 INFO             PET4 index= 146                  MPIR_CVAR_INTEL_SHM_SEND_INTER_RING_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_RING_MEMCPY
20220327 015824.557 INFO             PET4 index= 147                  MPIR_CVAR_INTEL_SHM_RECV_INTRA_RING_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_RING_MEMCPY
20220327 015824.557 INFO             PET4 index= 148                  MPIR_CVAR_INTEL_SHM_RECV_INTER_RING_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_RING_MEMCPY
20220327 015824.557 INFO             PET4 index= 149              MPIR_CVAR_INTEL_SHM_SEND_INTRA_CELL_FWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_CELL_FWD_MEMCPY
20220327 015824.557 INFO             PET4 index= 150              MPIR_CVAR_INTEL_SHM_SEND_INTER_CELL_FWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_CELL_FWD_MEMCPY
20220327 015824.557 INFO             PET4 index= 151              MPIR_CVAR_INTEL_SHM_SEND_INTRA_CELL_BWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_CELL_BWD_MEMCPY
20220327 015824.557 INFO             PET4 index= 152              MPIR_CVAR_INTEL_SHM_SEND_INTER_CELL_BWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_CELL_BWD_MEMCPY
20220327 015824.557 INFO             PET4 index= 153                  MPIR_CVAR_INTEL_SHM_RECV_INTRA_CELL_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_CELL_MEMCPY
20220327 015824.557 INFO             PET4 index= 154                  MPIR_CVAR_INTEL_SHM_RECV_INTER_CELL_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_CELL_MEMCPY
20220327 015824.557 INFO             PET4 index= 155          MPIR_CVAR_INTEL_SHM_RECV_INTRA_CELL_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_CELL_REGULAR_MEMCPY
20220327 015824.557 INFO             PET4 index= 156          MPIR_CVAR_INTEL_SHM_RECV_INTER_CELL_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_CELL_REGULAR_MEMCPY
20220327 015824.557 INFO             PET4 index= 157             MPIR_CVAR_INTEL_SHM_SEND_INTRA_FRAME_FWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_FRAME_FWD_MEMCPY
20220327 015824.557 INFO             PET4 index= 158             MPIR_CVAR_INTEL_SHM_SEND_INTER_FRAME_FWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_FRAME_FWD_MEMCPY
20220327 015824.557 INFO             PET4 index= 159             MPIR_CVAR_INTEL_SHM_SEND_INTRA_FRAME_BWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_FRAME_BWD_MEMCPY
20220327 015824.557 INFO             PET4 index= 160             MPIR_CVAR_INTEL_SHM_SEND_INTER_FRAME_BWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_FRAME_BWD_MEMCPY
20220327 015824.557 INFO             PET4 index= 161                 MPIR_CVAR_INTEL_SHM_RECV_INTRA_FRAME_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_FRAME_MEMCPY
20220327 015824.557 INFO             PET4 index= 162                 MPIR_CVAR_INTEL_SHM_RECV_INTER_FRAME_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_FRAME_MEMCPY
20220327 015824.557 INFO             PET4 index= 163     MPIR_CVAR_INTEL_SHM_SEND_INTRA_FRAME_FWD_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_FRAME_FWD_REGULAR_MEMCPY
20220327 015824.557 INFO             PET4 index= 164     MPIR_CVAR_INTEL_SHM_SEND_INTER_FRAME_FWD_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_FRAME_FWD_REGULAR_MEMCPY
20220327 015824.557 INFO             PET4 index= 165     MPIR_CVAR_INTEL_SHM_SEND_INTRA_FRAME_BWD_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_FRAME_BWD_REGULAR_MEMCPY
20220327 015824.557 INFO             PET4 index= 166     MPIR_CVAR_INTEL_SHM_SEND_INTER_FRAME_BWD_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_FRAME_BWD_REGULAR_MEMCPY
20220327 015824.558 INFO             PET4 index= 167         MPIR_CVAR_INTEL_SHM_RECV_INTRA_FRAME_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_FRAME_REGULAR_MEMCPY
20220327 015824.558 INFO             PET4 index= 168         MPIR_CVAR_INTEL_SHM_RECV_INTER_FRAME_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_FRAME_REGULAR_MEMCPY
20220327 015824.558 INFO             PET4 index= 169                       MPIR_CVAR_INTEL_SHM_INPLACE_THRESHOLD : @Alias:# I_MPI_SHM_INPLACE_THRESHOLD
20220327 015824.558 INFO             PET4 index= 170              MPIR_CVAR_INTEL_SHM_SEND_TINY_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_TINY_MEMCPY_THRESHOLD
20220327 015824.558 INFO             PET4 index= 171      MPIR_CVAR_INTEL_SHM_RECV_CELL_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_RECV_CELL_REGULAR_MEMCPY_THRESHOLD
20220327 015824.558 INFO             PET4 index= 172MPIR_CVAR_INTEL_SHM_SEND_INTER_UNIDIR_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_INTER_UNIDIR_REGULAR_MEMCPY_THRESHOLD
20220327 015824.558 INFO             PET4 index= 173MPIR_CVAR_INTEL_SHM_SEND_INTER_BIDIR_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_INTER_BIDIR_REGULAR_MEMCPY_THRESHOLD
20220327 015824.558 INFO             PET4 index= 174MPIR_CVAR_INTEL_SHM_SEND_INTRA_UNIDIR_REGULAR_MEMCPY_THRESHOLD_MIN : @Alias:# I_MPI_SHM_SEND_INTRA_UNIDIR_REGULAR_MEMCPY_THRESHOLD_MIN
20220327 015824.558 INFO             PET4 index= 175MPIR_CVAR_INTEL_SHM_SEND_INTRA_UNIDIR_REGULAR_MEMCPY_THRESHOLD_MAX : @Alias:# I_MPI_SHM_SEND_INTRA_UNIDIR_REGULAR_MEMCPY_THRESHOLD_MAX
20220327 015824.558 INFO             PET4 index= 176MPIR_CVAR_INTEL_SHM_SEND_INTRA_BIDIR_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_INTRA_BIDIR_REGULAR_MEMCPY_THRESHOLD
20220327 015824.558 INFO             PET4 index= 177     MPIR_CVAR_INTEL_SHM_RECV_FRAME_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_RECV_FRAME_REGULAR_MEMCPY_THRESHOLD
20220327 015824.558 INFO             PET4 index= 178          MPIR_CVAR_INTEL_SHM_CELL_FWD_UNIDIR_INTRA_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_UNIDIR_INTRA_CAPACITY
20220327 015824.558 INFO             PET4 index= 179  MPIR_CVAR_INTEL_SHM_CELL_FWD_UNIDIR_INTER_REGULAR_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_UNIDIR_INTER_REGULAR_CAPACITY
20220327 015824.558 INFO             PET4 index= 180MPIR_CVAR_INTEL_SHM_CELL_FWD_UNIDIR_INTER_NONTEMPORAL_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_UNIDIR_INTER_NONTEMPORAL_CAPACITY
20220327 015824.558 INFO             PET4 index= 181           MPIR_CVAR_INTEL_SHM_CELL_FWD_BIDIR_INTRA_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_BIDIR_INTRA_CAPACITY
20220327 015824.558 INFO             PET4 index= 182   MPIR_CVAR_INTEL_SHM_CELL_FWD_BIDIR_INTER_REGULAR_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_BIDIR_INTER_REGULAR_CAPACITY
20220327 015824.558 INFO             PET4 index= 183MPIR_CVAR_INTEL_SHM_CELL_FWD_BIDIR_INTER_NONTEMPORAL_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_BIDIR_INTER_NONTEMPORAL_CAPACITY
20220327 015824.558 INFO             PET4 index= 184MPIR_CVAR_INTEL_SHM_SEND_FWD_FRAME_NONTEMPORAL_MEMCPY_THRESHOLD_MIN : @Alias:# I_MPI_SHM_SEND_FWD_FRAME_NONTEMPORAL_MEMCPY_THRESHOLD_MIN
20220327 015824.558 INFO             PET4 index= 185MPIR_CVAR_INTEL_SHM_SEND_FWD_FRAME_NONTEMPORAL_MEMCPY_THRESHOLD_MAX : @Alias:# I_MPI_SHM_SEND_FWD_FRAME_NONTEMPORAL_MEMCPY_THRESHOLD_MAX
20220327 015824.558 INFO             PET4 index= 186MPIR_CVAR_INTEL_SHM_SEND_FWD_CELL_NONTEMPORAL_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_FWD_CELL_NONTEMPORAL_MEMCPY_THRESHOLD
20220327 015824.558 INFO             PET4 index= 187                                MPIR_CVAR_INTEL_SHM_HEAP_THP : @Alias:# I_MPI_SHM_HEAP_THP
20220327 015824.558 INFO             PET4 index= 188                                     MPIR_CVAR_INTEL_SHM_THP : @Alias:# I_MPI_SHM_THP
20220327 015824.558 INFO             PET4 index= 189                                  MPIR_CVAR_OFI_USE_PROVIDER : Define the name of the OFI provider to load.$ Syntax$ I_MPI_OFI_PROVIDER=<name>$ Arguments$ <name> - The name of the OFI provider to load @Alias:# I_MPI_OFI_PROVIDER @Default:# not defined
20220327 015824.558 INFO             PET4 index= 190                                MPIR_CVAR_OFI_DUMP_PROVIDERS : Control the capability of printing information about all OFI providers$ and their attributes from an OFI library.$ Syntax$ I_MPI_OFI_PROVIDER_DUMP=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > yes | on | 1 - Print the list of all OFI providers and their$ attributes from an OFI library$ > no | off | 0 - No action$ ----------------------------------------------------------------------- @Alias:# I_MPI_OFI_PROVIDER_DUMP @Default:# off
20220327 015824.558 INFO             PET4 index= 191                        MPIR_CVAR_CH4_OFI_ENABLE_DIRECT_RECV : Control the capability of the direct receive in the OFI fabric.$ Syntax$ I_MPI_OFI_DRECV=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > 1 - Enable direct receive$ > 0 - Disable direct receive$ ----------------------------------------------------------------------- @Alias:# I_MPI_OFI_DRECV @Default:# not defined
20220327 015824.558 INFO             PET4 index= 192                                  MPIR_CVAR_OFI_MAX_MSG_SIZE : @Alias:# I_MPI_OFI_MAX_MSG_SIZE
20220327 015824.558 INFO             PET4 index= 193                                  MPIR_CVAR_OFI_LMT_WIN_SIZE : @Alias:# I_MPI_OFI_LMT_WIN_SIZE
20220327 015824.558 INFO             PET4 index= 194                    MPIR_CVAR_CH4_OFI_ISEND_INJECT_THRESHOLD : @Alias:# I_MPI_OFI_ISEND_INJECT_THRESHOLD
20220327 015824.558 INFO             PET4 index= 195                     MPIR_CVAR_CH4_OFI_ADDRESS_EXCHANGE_MODE : @Alias:# I_MPI_STARTUP_MODE
20220327 015824.558 INFO             PET4 index= 196                     MPIR_CVAR_CH4_OFI_LARGE_SCALE_THRESHOLD : @Alias:# I_MPI_LARGE_SCALE_THRESHOLD
20220327 015824.558 INFO             PET4 index= 197                   MPIR_CVAR_CH4_OFI_EXTREME_SCALE_THRESHOLD : @Alias:# I_MPI_EXTREME_SCALE_THRESHOLD
20220327 015824.558 INFO             PET4 index= 198                        MPIR_CVAR_CH4_OFI_DYNAMIC_CONNECTION : @Alias:# I_MPI_DYNAMIC_CONNECTION
20220327 015824.558 INFO             PET4 index= 199                              MPIR_CVAR_CH4_OFI_EXPERIMENTAL : @Alias:# I_MPI_OFI_EXPERIMENTAL @Default:# 0
20220327 015824.558 INFO             PET4 index= 200                     MPIR_CVAR_CH4_OFI_CAPABILITY_SETS_DEBUG : Prints out the configuration of each capability selected via the capability sets interface.
20220327 015824.558 INFO             PET4 index= 201                               MPIR_CVAR_CH4_OFI_ENABLE_DATA : Enable immediate data fields in OFI to transmit source rank outside of the match bits
20220327 015824.558 INFO             PET4 index= 202                           MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE : If true, the OFI addressing information will be stored with an FI_AV_TABLE. If false, an FI_AV_MAP will be used.
20220327 015824.558 INFO             PET4 index= 203                 MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS : If true, use OFI scalable endpoints.
20220327 015824.558 INFO             PET4 index= 204                             MPIR_CVAR_CH4_OFI_MAX_ENDPOINTS : Specifies the maximum number of OFI endpoints that can be used by the OFI provider. The default value is -1, indicating that no value is set.
20220327 015824.558 INFO             PET4 index= 205                        MPIR_CVAR_CH4_OFI_ENABLE_MR_SCALABLE : If true, MR_SCALABLE for OFI memory regions. If false, MR_BASIC for OFI memory regions.
20220327 015824.558 INFO             PET4 index= 206                             MPIR_CVAR_CH4_OFI_ENABLE_TAGGED : If true, use tagged message transmission functions in OFI.
20220327 015824.558 INFO             PET4 index= 207                                 MPIR_CVAR_CH4_OFI_ENABLE_AM : If true, enable OFI active message support.
20220327 015824.558 INFO             PET4 index= 208                                MPIR_CVAR_CH4_OFI_ENABLE_RMA : If true, enable OFI RMA support for MPI RMA operations. OFI support for basic RMA is always required to implement large messgage transfers in the active message code path.
20220327 015824.558 INFO             PET4 index= 209                            MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS : If true, enable OFI Atomics support.
20220327 015824.558 INFO             PET4 index= 210                       MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS : Specifies the maximum number of iovecs that can be used by the OFI provider for fetch_atomic operations. The default value is -1, indicating that no value is set.
20220327 015824.558 INFO             PET4 index= 211                 MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS : If true, enable MPI data auto progress.
20220327 015824.558 INFO             PET4 index= 212              MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS : If true, enable MPI control auto progress.
20220327 015824.558 INFO             PET4 index= 213                       MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK : If true, enable iovec for pt2pt.
20220327 015824.558 INFO             PET4 index= 214                                 MPIR_CVAR_CH4_OFI_RANK_BITS : Specifies the number of bits that will be used for matching the MPI rank. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20220327 015824.558 INFO             PET4 index= 215                                  MPIR_CVAR_CH4_OFI_TAG_BITS : Specifies the number of bits that will be used for matching the user tag. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20220327 015824.558 INFO             PET4 index= 216                             MPIR_CVAR_CH4_OFI_MAJOR_VERSION : Specifies the major version of the OFI library. The default is the major version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20220327 015824.558 INFO             PET4 index= 217                             MPIR_CVAR_CH4_OFI_MINOR_VERSION : Specifies the major version of the OFI library. The default is the minor version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20220327 015824.558 INFO             PET4 index= 218                             MPIR_CVAR_CH4_OFI_ZERO_OP_FLAGS : Zeroes rx_attr.op_flags and tx_attr.op_flags, disables use of FI_SELECTIVE_COMPLETION. Can give more optimized behavior of underlying provider.
20220327 015824.558 INFO             PET4 index= 219                            MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS : Specifies the number of buffers for receiving active messages.
20220327 015824.558 INFO             PET4 index= 220                        MPIR_CVAR_INTEL_COLL_DIRECT_PROGRESS : @Alias:# I_MPI_COLL_DIRECT_PROGRESS
20220327 015824.558 INFO             PET4 index= 221                                      MPIR_CVAR_ENABLE_HCOLL : @Alias:# I_MPI_COLL_EXTERNAL
20220327 015824.558 INFO             PET4 index= 222                                     MPIR_CVAR_USE_ALLGATHER : Control selection of MPI_Allgather algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-6 @Alias:# I_MPI_ADJUST_ALLGATHER @Default:# -1
20220327 015824.558 INFO             PET4 index= 223                                MPIR_CVAR_USE_ALLGATHER_LIST : @Alias:# I_MPI_ADJUST_ALLGATHER_LIST @Default:# -1
20220327 015824.558 INFO             PET4 index= 224                         MPIR_CVAR_USE_ALLGATHER_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLGATHER_COMPOSITION @Default:# -1
20220327 015824.558 INFO             PET4 index= 225               MPIR_CVAR_ALLGATHER_COMPOSITION_DELTA_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLGATHER_COMPOSITION_DELTA_SEGSIZE @Default:# -1
20220327 015824.558 INFO             PET4 index= 226                             MPIR_CVAR_USE_ALLGATHER_NETWORK : range: 0-5 @Alias:# I_MPI_ADJUST_ALLGATHER_NETWORK @Default:# -1
20220327 015824.558 INFO             PET4 index= 227                                MPIR_CVAR_USE_ALLGATHER_NODE : range: 0-4 @Alias:# I_MPI_ADJUST_ALLGATHER_NODE @Default:# -1
20220327 015824.558 INFO             PET4 index= 228                                    MPIR_CVAR_USE_ALLGATHERV : Control selection of MPI_Allgatherv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_ALLGATHERV @Default:# -1
20220327 015824.558 INFO             PET4 index= 229                               MPIR_CVAR_USE_ALLGATHERV_LIST : @Alias:# I_MPI_ADJUST_ALLGATHERV_LIST @Default:# -1
20220327 015824.558 INFO             PET4 index= 230                        MPIR_CVAR_USE_ALLGATHERV_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLGATHERV_COMPOSITION @Default:# -1
20220327 015824.558 INFO             PET4 index= 231                            MPIR_CVAR_USE_ALLGATHERV_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_ALLGATHERV_NETWORK @Default:# -1
20220327 015824.558 INFO             PET4 index= 232                               MPIR_CVAR_USE_ALLGATHERV_NODE : range: 0-3 @Alias:# I_MPI_ADJUST_ALLGATHERV_NODE @Default:# -1
20220327 015824.558 INFO             PET4 index= 233                   MPIR_CVAR_ALLGATHER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ALLGATHER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET4 index= 234                      MPIR_CVAR_ALLGATHER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ALLGATHER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET4 index= 235                    MPIR_CVAR_SCATTERV_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_SCATTERV_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET4 index= 236                       MPIR_CVAR_SCATTERV_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_SCATTERV_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET4 index= 237                     MPIR_CVAR_SCATTER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_SCATTER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET4 index= 238                        MPIR_CVAR_SCATTER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_SCATTER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET4 index= 239                                     MPIR_CVAR_USE_ALLREDUCE : Control selection of MPI_Allreduce algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-25 @Alias:# I_MPI_ADJUST_ALLREDUCE @Default:# -1
20220327 015824.558 INFO             PET4 index= 240                                MPIR_CVAR_USE_ALLREDUCE_LIST : @Alias:# I_MPI_ADJUST_ALLREDUCE_LIST @Default:# -1
20220327 015824.558 INFO             PET4 index= 241                         MPIR_CVAR_USE_ALLREDUCE_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLREDUCE_COMPOSITION @Default:# -1
20220327 015824.558 INFO             PET4 index= 242                             MPIR_CVAR_USE_ALLREDUCE_NETWORK : range: 0-16 @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK @Default:# -1
20220327 015824.558 INFO             PET4 index= 243                                MPIR_CVAR_USE_ALLREDUCE_NODE : range: 0-9 @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE @Default:# -1
20220327 015824.558 INFO             PET4 index= 244               MPIR_CVAR_ALLREDUCE_NETWORK_MULTIPLYING_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_MULTIPLYING_RADIX @Default:# -1
20220327 015824.558 INFO             PET4 index= 245                  MPIR_CVAR_ALLREDUCE_NODE_MULTIPLYING_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_MULTIPLYING_RADIX @Default:# -1
20220327 015824.558 INFO             PET4 index= 246                   MPIR_CVAR_ALLREDUCE_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET4 index= 247                      MPIR_CVAR_ALLREDUCE_NETWORK_KARY_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_KARY_RADIX @Default:# -1
20220327 015824.558 INFO             PET4 index= 248                      MPIR_CVAR_ALLREDUCE_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET4 index= 249                         MPIR_CVAR_ALLREDUCE_NODE_KARY_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_KARY_RADIX @Default:# -1
20220327 015824.558 INFO             PET4 index= 250                    MPIR_CVAR_ALLREDUCE_NODE_SHM_GATHER_TYPE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHM_GATHER_TYPE @Default:# -1
20220327 015824.558 INFO             PET4 index= 251                   MPIR_CVAR_ALLREDUCE_NODE_SHM_GATHER_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHM_GATHER_RADIX @Default:# -1
20220327 015824.558 INFO             PET4 index= 252                   MPIR_CVAR_ALLREDUCE_NODE_SHM_RELEASE_TYPE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHM_RELEASE_TYPE @Default:# -1
20220327 015824.558 INFO             PET4 index= 253                  MPIR_CVAR_ALLREDUCE_NODE_SHM_RELEASE_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHM_RELEASE_RADIX @Default:# -1
20220327 015824.558 INFO             PET4 index= 254                MPIR_CVAR_ALLREDUCE_NETWORK_SHUMILIN_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_SHUMILIN_SEGSIZE @Default:# -1
20220327 015824.558 INFO             PET4 index= 255                   MPIR_CVAR_ALLREDUCE_NODE_SHUMILIN_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHUMILIN_SEGSIZE @Default:# -1
20220327 015824.558 INFO             PET4 index= 256                              MPIR_CVAR_ALLREDUCE_ZETA_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_ZETA_RADIX @Default:# -1
20220327 015824.558 INFO             PET4 index= 257                           MPIR_CVAR_ALLREDUCE_ZETA_SHM_TYPE : @Alias:# I_MPI_ADJUST_ALLREDUCE_ZETA_SHM_TYPE @Default:# -1
20220327 015824.558 INFO             PET4 index= 258                              MPIR_CVAR_ALLREDUCE_IOTA_NLEAD : @Alias:# I_MPI_ADJUST_ALLREDUCE_IOTA_NLEAD @Default:# -1
20220327 015824.558 INFO             PET4 index= 259               MPIR_CVAR_ALLREDUCE_NODE_NREDUCE_NB_ALLGATHER : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_NREDUCE_NB_ALLGATHER @Default:# -1
20220327 015824.558 INFO             PET4 index= 260            MPIR_CVAR_ALLREDUCE_NETWORK_NREDUCE_NB_ALLGATHER : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_NREDUCE_NB_ALLGATHER @Default:# -1
20220327 015824.558 INFO             PET4 index= 261                MPIR_CVAR_ALLREDUCE_NODE_NREDUCE_NB_ALLTOALL : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_NREDUCE_NB_ALLTOALL @Default:# -1
20220327 015824.558 INFO             PET4 index= 262             MPIR_CVAR_ALLREDUCE_NETWORK_NREDUCE_NB_ALLTOALL : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_NREDUCE_NB_ALLTOALL @Default:# -1
20220327 015824.558 INFO             PET4 index= 263                    MPIR_CVAR_ALLREDUCE_NODE_NREDUCE_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_NREDUCE_SEGSIZE @Default:# -1
20220327 015824.558 INFO             PET4 index= 264                 MPIR_CVAR_ALLREDUCE_NETWORK_NREDUCE_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_NREDUCE_SEGSIZE @Default:# -1
20220327 015824.558 INFO             PET4 index= 265                                      MPIR_CVAR_USE_ALLTOALL : Control selection of MPI_Alltoall algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-13 @Alias:# I_MPI_ADJUST_ALLTOALL @Default:# -1
20220327 015824.559 INFO             PET4 index= 266                                 MPIR_CVAR_USE_ALLTOALL_LIST : @Alias:# I_MPI_ADJUST_ALLTOALL_LIST @Default:# -1
20220327 015824.559 INFO             PET4 index= 267                          MPIR_CVAR_USE_ALLTOALL_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLTOALL_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET4 index= 268                              MPIR_CVAR_USE_ALLTOALL_NETWORK : range: 0-7 @Alias:# I_MPI_ADJUST_ALLTOALL_NETWORK @Default:# -1
20220327 015824.559 INFO             PET4 index= 269                                 MPIR_CVAR_USE_ALLTOALL_NODE : range: 0-6 @Alias:# I_MPI_ADJUST_ALLTOALL_NODE @Default:# -1
20220327 015824.559 INFO             PET4 index= 270                MPIR_CVAR_ALLTOALL_COMPOSITION_GAMMA_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLTOALL_COMPOSITION_GAMMA_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET4 index= 271                                 MPIR_CVAR_ALLTOALL_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALL_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET4 index= 272               MPIR_CVAR_ALLTOALL_NETWORK_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALL_NETWORK_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET4 index= 273                  MPIR_CVAR_ALLTOALL_NODE_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALL_NODE_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET4 index= 274                             MPIR_CVAR_ALLTOALL_BRUCKS_RADIX : @Alias:# I_MPI_ADJUST_ALLTOALL_BRUCKS_RADIX @Default:# -1
20220327 015824.559 INFO             PET4 index= 275                     MPIR_CVAR_ALLTOALL_NETWORK_BRUCKS_RADIX : @Alias:# I_MPI_ADJUST_ALLTOALL_NETWORK_BRUCKS_RADIX @Default:# -1
20220327 015824.559 INFO             PET4 index= 276                        MPIR_CVAR_ALLTOALL_NODE_BRUCKS_RADIX : @Alias:# I_MPI_ADJUST_ALLTOALL_NODE_BRUCKS_RADIX @Default:# -1
20220327 015824.559 INFO             PET4 index= 277                                     MPIR_CVAR_USE_ALLTOALLV : Control selection of MPI_Alltoallv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-9 @Alias:# I_MPI_ADJUST_ALLTOALLV @Default:# -1
20220327 015824.559 INFO             PET4 index= 278                                MPIR_CVAR_USE_ALLTOALLV_LIST : @Alias:# I_MPI_ADJUST_ALLTOALLV_LIST @Default:# -1
20220327 015824.559 INFO             PET4 index= 279                         MPIR_CVAR_USE_ALLTOALLV_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLTOALLV_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET4 index= 280                             MPIR_CVAR_USE_ALLTOALLV_NETWORK : range: 0-6 @Alias:# I_MPI_ADJUST_ALLTOALLV_NETWORK @Default:# -1
20220327 015824.559 INFO             PET4 index= 281                                MPIR_CVAR_USE_ALLTOALLV_NODE : range: 0-5 @Alias:# I_MPI_ADJUST_ALLTOALLV_NODE @Default:# -1
20220327 015824.559 INFO             PET4 index= 282                                MPIR_CVAR_ALLTOALLV_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLV_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET4 index= 283              MPIR_CVAR_ALLTOALLV_NETWORK_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLV_NETWORK_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET4 index= 284                 MPIR_CVAR_ALLTOALLV_NODE_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLV_NODE_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET4 index= 285                                     MPIR_CVAR_USE_ALLTOALLW : Control selection of MPI_Alltoallw algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-6 @Alias:# I_MPI_ADJUST_ALLTOALLW @Default:# -1
20220327 015824.559 INFO             PET4 index= 286                                MPIR_CVAR_USE_ALLTOALLW_LIST : @Alias:# I_MPI_ADJUST_ALLTOALLW_LIST @Default:# -1
20220327 015824.559 INFO             PET4 index= 287                         MPIR_CVAR_USE_ALLTOALLW_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLTOALLW_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET4 index= 288                             MPIR_CVAR_USE_ALLTOALLW_NETWORK : @Alias:# I_MPI_ADJUST_ALLTOALLW_NETWORK @Default:# -1
20220327 015824.559 INFO             PET4 index= 289                                MPIR_CVAR_USE_ALLTOALLW_NODE : @Alias:# I_MPI_ADJUST_ALLTOALLW_NODE @Default:# -1
20220327 015824.559 INFO             PET4 index= 290                                MPIR_CVAR_ALLTOALLW_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLW_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET4 index= 291              MPIR_CVAR_ALLTOALLW_NETWORK_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLW_NETWORK_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET4 index= 292                 MPIR_CVAR_ALLTOALLW_NODE_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLW_NODE_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET4 index= 293                                       MPIR_CVAR_USE_BARRIER : Control selection of MPI_Barrier algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-11 @Alias:# I_MPI_ADJUST_BARRIER @Default:# -1
20220327 015824.559 INFO             PET4 index= 294                                  MPIR_CVAR_USE_BARRIER_LIST : @Alias:# I_MPI_ADJUST_BARRIER_LIST @Default:# -1
20220327 015824.559 INFO             PET4 index= 295                           MPIR_CVAR_USE_BARRIER_COMPOSITION : @Alias:# I_MPI_ADJUST_BARRIER_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET4 index= 296                               MPIR_CVAR_USE_BARRIER_NETWORK : range: 0-6 @Alias:# I_MPI_ADJUST_BARRIER_NETWORK @Default:# -1
20220327 015824.559 INFO             PET4 index= 297                                  MPIR_CVAR_USE_BARRIER_NODE : range: 0-4 @Alias:# I_MPI_ADJUST_BARRIER_NODE @Default:# -1
20220327 015824.559 INFO             PET4 index= 298                 MPIR_CVAR_BARRIER_NETWORK_MULTIPLYING_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NETWORK_MULTIPLYING_RADIX @Default:# -1
20220327 015824.559 INFO             PET4 index= 299                     MPIR_CVAR_BARRIER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET4 index= 300                        MPIR_CVAR_BARRIER_NETWORK_KARY_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NETWORK_KARY_RADIX @Default:# -1
20220327 015824.559 INFO             PET4 index= 301          MPIR_CVAR_BARRIER_NETWORK_RECURSIVE_EXCHANGE_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NETWORK_RECURSIVE_EXCHANGE_RADIX @Default:# -1
20220327 015824.559 INFO             PET4 index= 302                        MPIR_CVAR_BARRIER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET4 index= 303                           MPIR_CVAR_BARRIER_NODE_KARY_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_KARY_RADIX @Default:# -1
20220327 015824.559 INFO             PET4 index= 304             MPIR_CVAR_BARRIER_NODE_RECURSIVE_EXCHANGE_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_RECURSIVE_EXCHANGE_RADIX @Default:# -1
20220327 015824.559 INFO             PET4 index= 305                      MPIR_CVAR_BARRIER_NODE_SHM_GATHER_TYPE : @Alias:# I_MPI_ADJUST_BARRIER_NODE_SHM_GATHER_TYPE @Default:# -1
20220327 015824.559 INFO             PET4 index= 306                     MPIR_CVAR_BARRIER_NODE_SHM_GATHER_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_SHM_GATHER_RADIX @Default:# -1
20220327 015824.559 INFO             PET4 index= 307                     MPIR_CVAR_BARRIER_NODE_SHM_RELEASE_TYPE : @Alias:# I_MPI_ADJUST_BARRIER_NODE_SHM_RELEASE_TYPE @Default:# -1
20220327 015824.559 INFO             PET4 index= 308                    MPIR_CVAR_BARRIER_NODE_SHM_RELEASE_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_SHM_RELEASE_RADIX @Default:# -1
20220327 015824.559 INFO             PET4 index= 309                                MPIR_CVAR_BARRIER_ZETA_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_ZETA_RADIX @Default:# -1
20220327 015824.559 INFO             PET4 index= 310                             MPIR_CVAR_BARRIER_ZETA_SHM_TYPE : @Alias:# I_MPI_ADJUST_BARRIER_ZETA_SHM_TYPE @Default:# -1
20220327 015824.559 INFO             PET4 index= 311                                         MPIR_CVAR_USE_BCAST : Control selection of MPI_Bcast algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-18 @Alias:# I_MPI_ADJUST_BCAST @Default:# -1
20220327 015824.559 INFO             PET4 index= 312                                    MPIR_CVAR_USE_BCAST_LIST : @Alias:# I_MPI_ADJUST_BCAST_LIST @Default:# -1
20220327 015824.559 INFO             PET4 index= 313                             MPIR_CVAR_USE_BCAST_COMPOSITION : @Alias:# I_MPI_ADJUST_BCAST_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET4 index= 314                                 MPIR_CVAR_USE_BCAST_NETWORK : range: 0-13 @Alias:# I_MPI_ADJUST_BCAST_NETWORK @Default:# -1
20220327 015824.559 INFO             PET4 index= 315                                    MPIR_CVAR_USE_BCAST_NODE : range: 0-9 @Alias:# I_MPI_ADJUST_BCAST_NODE @Default:# -1
20220327 015824.559 INFO             PET4 index= 316                               MPIR_CVAR_BCAST_EPSILON_NRAIL : @Alias:# I_MPI_ADJUST_BCAST_EPSILON_NRAIL @Default:# -1
20220327 015824.559 INFO             PET4 index= 317                          MPIR_CVAR_BCAST_NETWORK_KARY_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_KARY_RADIX @Default:# -1
20220327 015824.559 INFO             PET4 index= 318                       MPIR_CVAR_BCAST_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET4 index= 319                        MPIR_CVAR_BCAST_NETWORK_KARY_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_KARY_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET4 index= 320                     MPIR_CVAR_BCAST_NETWORK_KNOMIAL_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_KNOMIAL_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET4 index= 321                             MPIR_CVAR_BCAST_NODE_KARY_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NODE_KARY_RADIX @Default:# -1
20220327 015824.559 INFO             PET4 index= 322                          MPIR_CVAR_BCAST_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET4 index= 323                           MPIR_CVAR_BCAST_NODE_KARY_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_KARY_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET4 index= 324                        MPIR_CVAR_BCAST_NODE_KNOMIAL_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_KNOMIAL_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET4 index= 325                          MPIR_CVAR_BCAST_NETWORK_TREE_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_TREE_RADIX @Default:# -1
20220327 015824.559 INFO             PET4 index= 326                        MPIR_CVAR_BCAST_NETWORK_TREE_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_TREE_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET4 index= 327                           MPIR_CVAR_BCAST_NETWORK_TREE_TYPE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_TREE_TYPE @Default:# -1
20220327 015824.559 INFO             PET4 index= 328                       MPIR_CVAR_BCAST_NETWORK_TREE_THROTTLE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_TREE_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET4 index= 329                       MPIR_CVAR_BCAST_NODE_SHUMILIN_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHUMILIN_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET4 index= 330                        MPIR_CVAR_BCAST_NODE_SHM_GATHER_TYPE : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHM_GATHER_TYPE @Default:# -1
20220327 015824.559 INFO             PET4 index= 331                       MPIR_CVAR_BCAST_NODE_SHM_GATHER_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHM_GATHER_RADIX @Default:# -1
20220327 015824.559 INFO             PET4 index= 332                       MPIR_CVAR_BCAST_NODE_SHM_RELEASE_TYPE : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHM_RELEASE_TYPE @Default:# -1
20220327 015824.559 INFO             PET4 index= 333                      MPIR_CVAR_BCAST_NODE_SHM_RELEASE_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHM_RELEASE_RADIX @Default:# -1
20220327 015824.559 INFO             PET4 index= 334                    MPIR_CVAR_BCAST_NETWORK_SHUMILIN_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_SHUMILIN_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET4 index= 335                 MPIR_CVAR_BCAST_NODE_NUMA_AWARE_MEMCPY_ARCH : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_MEMCPY_ARCH @Default:# -1
20220327 015824.559 INFO             PET4 index= 336                   MPIR_CVAR_BCAST_NODE_NUMA_AWARE_FRAME_NUM : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_FRAME_NUM
20220327 015824.559 INFO             PET4 index= 337                  MPIR_CVAR_BCAST_NODE_NUMA_AWARE_FRAME_SIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_FRAME_SIZE
20220327 015824.559 INFO             PET4 index= 338  MPIR_CVAR_BCAST_NODE_NUMA_AWARE_RECV_NONTEMPORAL_THRESHOLD : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_NONTEMPORAL_THRESHOLD
20220327 015824.559 INFO             PET4 index= 339      MPIR_CVAR_BCAST_NODE_NUMA_AWARE_FRAME_NONTEMPORAL_SIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_FRAME_NONTEMPORAL_SIZE
20220327 015824.559 INFO             PET4 index= 340 MPIR_CVAR_BCAST_NODE_NUMA_AWARE_TINY_MESSAGE_SIZE_THRESHOLD : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_TINY_MESSAGE_SIZE_THRESHOLD
20220327 015824.559 INFO             PET4 index= 341MPIR_CVAR_BCAST_NODE_NUMA_AWARE_SHM_HEAP_MESSAGE_SIZE_THRESHOLD : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_SHM_HEAP_MESSAGE_SIZE_THRESHOLD
20220327 015824.559 INFO             PET4 index= 342                                        MPIR_CVAR_USE_EXSCAN : Control selection of MPI_Exscan algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_EXSCAN @Default:# -1
20220327 015824.559 INFO             PET4 index= 343                                   MPIR_CVAR_USE_EXSCAN_LIST : @Alias:# I_MPI_ADJUST_EXSCAN_LIST @Default:# -1
20220327 015824.559 INFO             PET4 index= 344                            MPIR_CVAR_USE_EXSCAN_COMPOSITION : @Alias:# I_MPI_ADJUST_EXSCAN_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET4 index= 345                                MPIR_CVAR_USE_EXSCAN_NETWORK : @Alias:# I_MPI_ADJUST_EXSCAN_NETWORK @Default:# -1
20220327 015824.559 INFO             PET4 index= 346                                   MPIR_CVAR_USE_EXSCAN_NODE : @Alias:# I_MPI_ADJUST_EXSCAN_NODE @Default:# -1
20220327 015824.559 INFO             PET4 index= 347                                        MPIR_CVAR_USE_GATHER : Control selection of MPI_Gather algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_GATHER @Default:# -1
20220327 015824.559 INFO             PET4 index= 348                                   MPIR_CVAR_USE_GATHER_LIST : @Alias:# I_MPI_ADJUST_GATHER_LIST @Default:# -1
20220327 015824.559 INFO             PET4 index= 349                            MPIR_CVAR_USE_GATHER_COMPOSITION : @Alias:# I_MPI_ADJUST_GATHER_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET4 index= 350                                MPIR_CVAR_USE_GATHER_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_GATHER_NETWORK @Default:# -1
20220327 015824.559 INFO             PET4 index= 351                                   MPIR_CVAR_USE_GATHER_NODE : range: 0-3 @Alias:# I_MPI_ADJUST_GATHER_NODE @Default:# -1
20220327 015824.559 INFO             PET4 index= 352            MPIR_CVAR_GATHER_NODE_BINOMIAL_SEGMENTED_SEGSIZE : @Alias:# I_MPI_ADJUST_GATHER_NODE_BINOMIAL_SEGMENTED_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET4 index= 353         MPIR_CVAR_GATHER_NETWORK_BINOMIAL_SEGMENTED_SEGSIZE : @Alias:# I_MPI_ADJUST_GATHER_NETWORK_BINOMIAL_SEGMENTED_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET4 index= 354                                       MPIR_CVAR_USE_GATHERV : Control selection of MPI_Gatherv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-4 @Alias:# I_MPI_ADJUST_GATHERV @Default:# -1
20220327 015824.559 INFO             PET4 index= 355                                  MPIR_CVAR_USE_GATHERV_LIST : @Alias:# I_MPI_ADJUST_GATHERV_LIST @Default:# -1
20220327 015824.559 INFO             PET4 index= 356                           MPIR_CVAR_USE_GATHERV_COMPOSITION : @Alias:# I_MPI_ADJUST_GATHERV_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET4 index= 357                               MPIR_CVAR_USE_GATHERV_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_GATHERV_NETWORK @Default:# -1
20220327 015824.559 INFO             PET4 index= 358                                  MPIR_CVAR_USE_GATHERV_NODE : range: 0-2 @Alias:# I_MPI_ADJUST_GATHERV_NODE @Default:# -1
20220327 015824.559 INFO             PET4 index= 359                     MPIR_CVAR_GATHERV_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_GATHERV_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET4 index= 360                        MPIR_CVAR_GATHERV_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_GATHERV_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET4 index= 361                                MPIR_CVAR_USE_REDUCE_SCATTER : Control selection of MPI_Reduce_scatter algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-6 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER @Default:# -1
20220327 015824.559 INFO             PET4 index= 362                           MPIR_CVAR_USE_REDUCE_SCATTER_LIST : @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_LIST @Default:# -1
20220327 015824.559 INFO             PET4 index= 363                    MPIR_CVAR_USE_REDUCE_SCATTER_COMPOSITION : @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET4 index= 364                        MPIR_CVAR_USE_REDUCE_SCATTER_NETWORK : range: 0-5 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_NETWORK @Default:# -1
20220327 015824.560 INFO             PET4 index= 365                           MPIR_CVAR_USE_REDUCE_SCATTER_NODE : range: 0-4 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_NODE @Default:# -1
20220327 015824.560 INFO             PET4 index= 366                          MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK : Control selection of MPI_Reduce_scatter_block algorithm presets. Arguments$ <algid> - Algorithm identifier$ range: 0-4 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK @Default:# -1
20220327 015824.560 INFO             PET4 index= 367                     MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK_LIST : @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK_LIST @Default:# -1
20220327 015824.560 INFO             PET4 index= 368              MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK_COMPOSITION : @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET4 index= 369                  MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK_NETWORK @Default:# -1
20220327 015824.560 INFO             PET4 index= 370                     MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK_NODE : range: 0-3 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK_NODE @Default:# -1
20220327 015824.560 INFO             PET4 index= 371                                        MPIR_CVAR_USE_REDUCE : Control selection of MPI_Reduce algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-13 @Alias:# I_MPI_ADJUST_REDUCE @Default:# -1
20220327 015824.560 INFO             PET4 index= 372                                   MPIR_CVAR_USE_REDUCE_LIST : @Alias:# I_MPI_ADJUST_REDUCE_LIST @Default:# -1
20220327 015824.560 INFO             PET4 index= 373                            MPIR_CVAR_USE_REDUCE_COMPOSITION : @Alias:# I_MPI_ADJUST_REDUCE_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET4 index= 374                                MPIR_CVAR_USE_REDUCE_NETWORK : range: 0-10 @Alias:# I_MPI_ADJUST_REDUCE_NETWORK @Default:# -1
20220327 015824.560 INFO             PET4 index= 375                                   MPIR_CVAR_USE_REDUCE_NODE : range: 0-7 @Alias:# I_MPI_ADJUST_REDUCE_NODE @Default:# -1
20220327 015824.560 INFO             PET4 index= 376                         MPIR_CVAR_REDUCE_NETWORK_KARY_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KARY_RADIX @Default:# -1
20220327 015824.560 INFO             PET4 index= 377                      MPIR_CVAR_REDUCE_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET4 index= 378                      MPIR_CVAR_REDUCE_NETWORK_KARY_NBUFFERS : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KARY_NBUFFERS @Default:# -1
20220327 015824.560 INFO             PET4 index= 379                   MPIR_CVAR_REDUCE_NETWORK_KNOMIAL_NBUFFERS : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KNOMIAL_NBUFFERS @Default:# -1
20220327 015824.560 INFO             PET4 index= 380                       MPIR_CVAR_REDUCE_NETWORK_KARY_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KARY_SEGSIZE @Default:# -1
20220327 015824.560 INFO             PET4 index= 381                    MPIR_CVAR_REDUCE_NETWORK_KNOMIAL_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KNOMIAL_SEGSIZE @Default:# -1
20220327 015824.560 INFO             PET4 index= 382                       MPIR_CVAR_REDUCE_NETWORK_RING_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_RING_SEGSIZE @Default:# -1
20220327 015824.560 INFO             PET4 index= 383                            MPIR_CVAR_REDUCE_NODE_KARY_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KARY_RADIX @Default:# -1
20220327 015824.560 INFO             PET4 index= 384                         MPIR_CVAR_REDUCE_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET4 index= 385                         MPIR_CVAR_REDUCE_NODE_KARY_NBUFFERS : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KARY_NBUFFERS @Default:# -1
20220327 015824.560 INFO             PET4 index= 386                      MPIR_CVAR_REDUCE_NODE_KNOMIAL_NBUFFERS : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KNOMIAL_NBUFFERS @Default:# -1
20220327 015824.560 INFO             PET4 index= 387                          MPIR_CVAR_REDUCE_NODE_KARY_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KARY_SEGSIZE @Default:# -1
20220327 015824.560 INFO             PET4 index= 388                       MPIR_CVAR_REDUCE_NODE_KNOMIAL_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KNOMIAL_SEGSIZE @Default:# -1
20220327 015824.560 INFO             PET4 index= 389                          MPIR_CVAR_REDUCE_NODE_RING_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_RING_SEGSIZE @Default:# -1
20220327 015824.560 INFO             PET4 index= 390                       MPIR_CVAR_REDUCE_NODE_SHM_GATHER_TYPE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_SHM_GATHER_TYPE @Default:# -1
20220327 015824.560 INFO             PET4 index= 391                      MPIR_CVAR_REDUCE_NODE_SHM_GATHER_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NODE_SHM_GATHER_RADIX @Default:# -1
20220327 015824.560 INFO             PET4 index= 392                      MPIR_CVAR_REDUCE_NODE_SHM_RELEASE_TYPE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_SHM_RELEASE_TYPE @Default:# -1
20220327 015824.560 INFO             PET4 index= 393                     MPIR_CVAR_REDUCE_NODE_SHM_RELEASE_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NODE_SHM_RELEASE_RADIX @Default:# -1
20220327 015824.560 INFO             PET4 index= 394                                          MPIR_CVAR_USE_SCAN : Control selection of MPI_Scan algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_SCAN @Default:# -1
20220327 015824.560 INFO             PET4 index= 395                                     MPIR_CVAR_USE_SCAN_LIST : @Alias:# I_MPI_ADJUST_SCAN_LIST @Default:# -1
20220327 015824.560 INFO             PET4 index= 396                              MPIR_CVAR_USE_SCAN_COMPOSITION : @Alias:# I_MPI_ADJUST_SCAN_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET4 index= 397                                  MPIR_CVAR_USE_SCAN_NETWORK : @Alias:# I_MPI_ADJUST_SCAN_NETWORK @Default:# -1
20220327 015824.560 INFO             PET4 index= 398                                     MPIR_CVAR_USE_SCAN_NODE : @Alias:# I_MPI_ADJUST_SCAN_NODE @Default:# -1
20220327 015824.560 INFO             PET4 index= 399                                       MPIR_CVAR_USE_SCATTER : Control selection of MPI_Scatter algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_SCATTER @Default:# -1
20220327 015824.560 INFO             PET4 index= 400                                  MPIR_CVAR_USE_SCATTER_LIST : @Alias:# I_MPI_ADJUST_SCATTER_LIST @Default:# -1
20220327 015824.560 INFO             PET4 index= 401                           MPIR_CVAR_USE_SCATTER_COMPOSITION : @Alias:# I_MPI_ADJUST_SCATTER_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET4 index= 402                               MPIR_CVAR_USE_SCATTER_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_SCATTER_NETWORK @Default:# -1
20220327 015824.560 INFO             PET4 index= 403                                  MPIR_CVAR_USE_SCATTER_NODE : range: 0-3 @Alias:# I_MPI_ADJUST_SCATTER_NODE @Default:# -1
20220327 015824.560 INFO             PET4 index= 404                                      MPIR_CVAR_USE_SCATTERV : Control selection of MPI_Scatterv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-4 @Alias:# I_MPI_ADJUST_SCATTERV @Default:# -1
20220327 015824.560 INFO             PET4 index= 405                                 MPIR_CVAR_USE_SCATTERV_LIST : @Alias:# I_MPI_ADJUST_SCATTERV_LIST @Default:# -1
20220327 015824.560 INFO             PET4 index= 406                          MPIR_CVAR_USE_SCATTERV_COMPOSITION : @Alias:# I_MPI_ADJUST_SCATTERV_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET4 index= 407                              MPIR_CVAR_USE_SCATTERV_NETWORK : range: 0-3 @Alias:# I_MPI_ADJUST_SCATTERV_NETWORK @Default:# -1
20220327 015824.560 INFO             PET4 index= 408                                 MPIR_CVAR_USE_SCATTERV_NODE : range: 0-2 @Alias:# I_MPI_ADJUST_SCATTERV_NODE @Default:# -1
20220327 015824.560 INFO             PET4 index= 409                                    MPIR_CVAR_USE_IALLREDUCE : Control selection of MPI_Iallreduce algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-9 @Alias:# I_MPI_ADJUST_IALLREDUCE @Default:# -1
20220327 015824.560 INFO             PET4 index= 410                               MPIR_CVAR_USE_IALLREDUCE_LIST : @Alias:# I_MPI_ADJUST_IALLREDUCE_LIST @Default:# -1
20220327 015824.560 INFO             PET4 index= 411                        MPIR_CVAR_USE_IALLREDUCE_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLREDUCE_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET4 index= 412                            MPIR_CVAR_USE_IALLREDUCE_NETWORK : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK @Default:# -1
20220327 015824.560 INFO             PET4 index= 413                               MPIR_CVAR_USE_IALLREDUCE_NODE : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE @Default:# -1
20220327 015824.560 INFO             PET4 index= 414                   MPIR_CVAR_IALLREDUCE_KNOMIAL_REDUCE_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_KNOMIAL_REDUCE_RADIX @Default:# -1
20220327 015824.560 INFO             PET4 index= 415                    MPIR_CVAR_IALLREDUCE_KNOMIAL_BCAST_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_KNOMIAL_BCAST_RADIX @Default:# -1
20220327 015824.560 INFO             PET4 index= 416           MPIR_CVAR_IALLREDUCE_NETWORK_KNOMIAL_REDUCE_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK_KNOMIAL_REDUCE_RADIX @Default:# -1
20220327 015824.560 INFO             PET4 index= 417              MPIR_CVAR_IALLREDUCE_NODE_KNOMIAL_REDUCE_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE_KNOMIAL_REDUCE_RADIX @Default:# -1
20220327 015824.560 INFO             PET4 index= 418            MPIR_CVAR_IALLREDUCE_NETWORK_KNOMIAL_BCAST_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK_KNOMIAL_BCAST_RADIX @Default:# -1
20220327 015824.560 INFO             PET4 index= 419               MPIR_CVAR_IALLREDUCE_NODE_KNOMIAL_BCAST_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE_KNOMIAL_BCAST_RADIX @Default:# -1
20220327 015824.560 INFO             PET4 index= 420                 MPIR_CVAR_IALLREDUCE_NODE_NREDUCE_DO_GATHER : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE_NREDUCE_DO_GATHER @Default:# -1
20220327 015824.560 INFO             PET4 index= 421              MPIR_CVAR_IALLREDUCE_NETWORK_NREDUCE_DO_GATHER : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK_NREDUCE_DO_GATHER @Default:# -1
20220327 015824.560 INFO             PET4 index= 422              MPIR_CVAR_IALLREDUCE_NODE_NREDUCE_NB_ALLGATHER : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE_NREDUCE_NB_ALLGATHER @Default:# -1
20220327 015824.560 INFO             PET4 index= 423           MPIR_CVAR_IALLREDUCE_NETWORK_NREDUCE_NB_ALLGATHER : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK_NREDUCE_NB_ALLGATHER @Default:# -1
20220327 015824.560 INFO             PET4 index= 424                                        MPIR_CVAR_USE_IBCAST : Control selection of MPI_Ibcast algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_IBCAST @Default:# -1
20220327 015824.560 INFO             PET4 index= 425                                   MPIR_CVAR_USE_IBCAST_LIST : @Alias:# I_MPI_ADJUST_IBCAST_LIST @Default:# -1
20220327 015824.560 INFO             PET4 index= 426                            MPIR_CVAR_USE_IBCAST_COMPOSITION : @Alias:# I_MPI_ADJUST_IBCAST_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET4 index= 427                                MPIR_CVAR_USE_IBCAST_NETWORK : @Alias:# I_MPI_ADJUST_IBCAST_NETWORK @Default:# -1
20220327 015824.560 INFO             PET4 index= 428                                   MPIR_CVAR_USE_IBCAST_NODE : @Alias:# I_MPI_ADJUST_IBCAST_NODE @Default:# -1
20220327 015824.560 INFO             PET4 index= 429                              MPIR_CVAR_IBCAST_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IBCAST_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET4 index= 430                      MPIR_CVAR_IBCAST_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IBCAST_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET4 index= 431                         MPIR_CVAR_IBCAST_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IBCAST_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET4 index= 432                                       MPIR_CVAR_USE_IREDUCE : Control selection of MPI_Ireduce algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-4 @Alias:# I_MPI_ADJUST_IREDUCE @Default:# -1
20220327 015824.560 INFO             PET4 index= 433                                  MPIR_CVAR_USE_IREDUCE_LIST : @Alias:# I_MPI_ADJUST_IREDUCE_LIST @Default:# -1
20220327 015824.560 INFO             PET4 index= 434                           MPIR_CVAR_USE_IREDUCE_COMPOSITION : @Alias:# I_MPI_ADJUST_IREDUCE_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET4 index= 435                               MPIR_CVAR_USE_IREDUCE_NETWORK : @Alias:# I_MPI_ADJUST_IREDUCE_NETWORK @Default:# -1
20220327 015824.560 INFO             PET4 index= 436                                  MPIR_CVAR_USE_IREDUCE_NODE : @Alias:# I_MPI_ADJUST_IREDUCE_NODE @Default:# -1
20220327 015824.560 INFO             PET4 index= 437                             MPIR_CVAR_IREDUCE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IREDUCE_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET4 index= 438                     MPIR_CVAR_IREDUCE_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IREDUCE_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET4 index= 439                        MPIR_CVAR_IREDUCE_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IREDUCE_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET4 index= 440                                       MPIR_CVAR_USE_IGATHER : Control selection of MPI_Igather algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-2 @Alias:# I_MPI_ADJUST_IGATHER @Default:# -1
20220327 015824.560 INFO             PET4 index= 441                                  MPIR_CVAR_USE_IGATHER_LIST : @Alias:# I_MPI_ADJUST_IGATHER_LIST @Default:# -1
20220327 015824.560 INFO             PET4 index= 442                           MPIR_CVAR_USE_IGATHER_COMPOSITION : @Alias:# I_MPI_ADJUST_IGATHER_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET4 index= 443                               MPIR_CVAR_USE_IGATHER_NETWORK : @Alias:# I_MPI_ADJUST_IGATHER_NETWORK @Default:# -1
20220327 015824.560 INFO             PET4 index= 444                                  MPIR_CVAR_USE_IGATHER_NODE : @Alias:# I_MPI_ADJUST_IGATHER_NODE @Default:# -1
20220327 015824.560 INFO             PET4 index= 445                             MPIR_CVAR_IGATHER_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IGATHER_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET4 index= 446                     MPIR_CVAR_IGATHER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IGATHER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET4 index= 447                        MPIR_CVAR_IGATHER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IGATHER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET4 index= 448                                    MPIR_CVAR_USE_IALLGATHER : Control selection of MPI_Iallgather algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_IALLGATHER @Default:# -1
20220327 015824.560 INFO             PET4 index= 449                               MPIR_CVAR_USE_IALLGATHER_LIST : @Alias:# I_MPI_ADJUST_IALLGATHER_LIST @Default:# -1
20220327 015824.560 INFO             PET4 index= 450                        MPIR_CVAR_USE_IALLGATHER_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLGATHER_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET4 index= 451                            MPIR_CVAR_USE_IALLGATHER_NETWORK : @Alias:# I_MPI_ADJUST_IALLGATHER_NETWORK @Default:# -1
20220327 015824.560 INFO             PET4 index= 452                               MPIR_CVAR_USE_IALLGATHER_NODE : @Alias:# I_MPI_ADJUST_IALLGATHER_NODE @Default:# -1
20220327 015824.560 INFO             PET4 index= 453                  MPIR_CVAR_IALLGATHER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IALLGATHER_NETWORK_KNOMIAL_RADIX
20220327 015824.560 INFO             PET4 index= 454                     MPIR_CVAR_IALLGATHER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IALLGATHER_NODE_KNOMIAL_RADIX
20220327 015824.560 INFO             PET4 index= 455                                     MPIR_CVAR_USE_IALLTOALL : Control selection of MPI_Ialltoall algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-7 @Alias:# I_MPI_ADJUST_IALLTOALL @Default:# -1
20220327 015824.560 INFO             PET4 index= 456                                MPIR_CVAR_USE_IALLTOALL_LIST : @Alias:# I_MPI_ADJUST_IALLTOALL_LIST @Default:# -1
20220327 015824.560 INFO             PET4 index= 457                         MPIR_CVAR_USE_IALLTOALL_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLTOALL_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET4 index= 458                             MPIR_CVAR_USE_IALLTOALL_NETWORK : @Alias:# I_MPI_ADJUST_IALLTOALL_NETWORK @Default:# -1
20220327 015824.560 INFO             PET4 index= 459                                MPIR_CVAR_USE_IALLTOALL_NODE : @Alias:# I_MPI_ADJUST_IALLTOALL_NODE @Default:# -1
20220327 015824.561 INFO             PET4 index= 460              MPIR_CVAR_IALLTOALL_PERMUTED_SENDRECV_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALL_PERMUTED_SENDRECV_THROTTLE @Default:# -1
20220327 015824.561 INFO             PET4 index= 461      MPIR_CVAR_IALLTOALL_NETWORK_PERMUTED_SENDRECV_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALL_NETWORK_PERMUTED_SENDRECV_THROTTLE @Default:# -1
20220327 015824.561 INFO             PET4 index= 462         MPIR_CVAR_IALLTOALL_NODE_PERMUTED_SENDRECV_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALL_NODE_PERMUTED_SENDRECV_THROTTLE @Default:# -1
20220327 015824.561 INFO             PET4 index= 463                                    MPIR_CVAR_USE_IALLTOALLV : Control selection of MPI_Ialltoallv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-6 @Alias:# I_MPI_ADJUST_IALLTOALLV @Default:# -1
20220327 015824.561 INFO             PET4 index= 464                               MPIR_CVAR_USE_IALLTOALLV_LIST : @Alias:# I_MPI_ADJUST_IALLTOALLV_LIST @Default:# -1
20220327 015824.561 INFO             PET4 index= 465                        MPIR_CVAR_USE_IALLTOALLV_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLTOALLV_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET4 index= 466                            MPIR_CVAR_USE_IALLTOALLV_NETWORK : @Alias:# I_MPI_ADJUST_IALLTOALLV_NETWORK @Default:# -1
20220327 015824.561 INFO             PET4 index= 467                               MPIR_CVAR_USE_IALLTOALLV_NODE : @Alias:# I_MPI_ADJUST_IALLTOALLV_NODE @Default:# -1
20220327 015824.561 INFO             PET4 index= 468                       MPIR_CVAR_IALLTOALLV_BLOCKED_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALLV_BLOCKED_THROTTLE @Default:# -1
20220327 015824.561 INFO             PET4 index= 469               MPIR_CVAR_IALLTOALLV_NETWORK_BLOCKED_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALLV_NETWORK_BLOCKED_THROTTLE @Default:# -1
20220327 015824.561 INFO             PET4 index= 470                                    MPIR_CVAR_USE_IALLTOALLW : Control selection of MPI_Ialltoallw algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-1 @Alias:# I_MPI_ADJUST_IALLTOALLW @Default:# -1
20220327 015824.561 INFO             PET4 index= 471                               MPIR_CVAR_USE_IALLTOALLW_LIST : @Alias:# I_MPI_ADJUST_IALLTOALLW_LIST @Default:# -1
20220327 015824.561 INFO             PET4 index= 472                        MPIR_CVAR_USE_IALLTOALLW_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLTOALLW_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET4 index= 473                            MPIR_CVAR_USE_IALLTOALLW_NETWORK : @Alias:# I_MPI_ADJUST_IALLTOALLW_NETWORK @Default:# -1
20220327 015824.561 INFO             PET4 index= 474                               MPIR_CVAR_USE_IALLTOALLW_NODE : @Alias:# I_MPI_ADJUST_IALLTOALLW_NODE @Default:# -1
20220327 015824.561 INFO             PET4 index= 475                       MPIR_CVAR_IALLTOALLW_BLOCKED_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALLW_BLOCKED_THROTTLE @Default:# -1
20220327 015824.561 INFO             PET4 index= 476               MPIR_CVAR_IALLTOALLW_NETWORK_BLOCKED_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALLW_NETWORK_BLOCKED_THROTTLE @Default:# -1
20220327 015824.561 INFO             PET4 index= 477                                      MPIR_CVAR_USE_IGATHERV : Control selection of MPI_Igatherv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_IGATHERV @Default:# -1
20220327 015824.561 INFO             PET4 index= 478                                 MPIR_CVAR_USE_IGATHERV_LIST : @Alias:# I_MPI_ADJUST_IGATHERV_LIST @Default:# -1
20220327 015824.561 INFO             PET4 index= 479                          MPIR_CVAR_USE_IGATHERV_COMPOSITION : @Alias:# I_MPI_ADJUST_IGATHERV_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET4 index= 480                              MPIR_CVAR_USE_IGATHERV_NETWORK : @Alias:# I_MPI_ADJUST_IGATHERV_NETWORK @Default:# -1
20220327 015824.561 INFO             PET4 index= 481                                 MPIR_CVAR_USE_IGATHERV_NODE : @Alias:# I_MPI_ADJUST_IGATHERV_NODE @Default:# -1
20220327 015824.561 INFO             PET4 index= 482                                     MPIR_CVAR_USE_ISCATTERV : Control selection of MPI_Iscatterv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-1 @Alias:# I_MPI_ADJUST_ISCATTERV @Default:# -1
20220327 015824.561 INFO             PET4 index= 483                                MPIR_CVAR_USE_ISCATTERV_LIST : @Alias:# I_MPI_ADJUST_ISCATTERV_LIST @Default:# -1
20220327 015824.561 INFO             PET4 index= 484                         MPIR_CVAR_USE_ISCATTERV_COMPOSITION : @Alias:# I_MPI_ADJUST_ISCATTERV_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET4 index= 485                             MPIR_CVAR_USE_ISCATTERV_NETWORK : @Alias:# I_MPI_ADJUST_ISCATTERV_NETWORK @Default:# -1
20220327 015824.561 INFO             PET4 index= 486                                MPIR_CVAR_USE_ISCATTERV_NODE : @Alias:# I_MPI_ADJUST_ISCATTERV_NODE @Default:# -1
20220327 015824.561 INFO             PET4 index= 487                                      MPIR_CVAR_USE_IBARRIER : Control selection of MPI_Ibarrier algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-2 @Alias:# I_MPI_ADJUST_IBARRIER @Default:# -1
20220327 015824.561 INFO             PET4 index= 488                                 MPIR_CVAR_USE_IBARRIER_LIST : @Alias:# I_MPI_ADJUST_IBARRIER_LIST @Default:# -1
20220327 015824.561 INFO             PET4 index= 489                          MPIR_CVAR_USE_IBARRIER_COMPOSITION : @Alias:# I_MPI_ADJUST_IBARRIER_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET4 index= 490                              MPIR_CVAR_USE_IBARRIER_NETWORK : @Alias:# I_MPI_ADJUST_IBARRIER_NETWORK @Default:# -1
20220327 015824.561 INFO             PET4 index= 491                                 MPIR_CVAR_USE_IBARRIER_NODE : @Alias:# I_MPI_ADJUST_IBARRIER_NODE @Default:# -1
20220327 015824.561 INFO             PET4 index= 492                                      MPIR_CVAR_USE_ISCATTER : Control selection of MPI_Iscatter algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-2 @Alias:# I_MPI_ADJUST_ISCATTER @Default:# -1
20220327 015824.561 INFO             PET4 index= 493                                 MPIR_CVAR_USE_ISCATTER_LIST : @Alias:# I_MPI_ADJUST_ISCATTER_LIST @Default:# -1
20220327 015824.561 INFO             PET4 index= 494                          MPIR_CVAR_USE_ISCATTER_COMPOSITION : @Alias:# I_MPI_ADJUST_ISCATTER_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET4 index= 495                              MPIR_CVAR_USE_ISCATTER_NETWORK : @Alias:# I_MPI_ADJUST_ISCATTER_NETWORK @Default:# -1
20220327 015824.561 INFO             PET4 index= 496                                 MPIR_CVAR_USE_ISCATTER_NODE : @Alias:# I_MPI_ADJUST_ISCATTER_NODE @Default:# -1
20220327 015824.561 INFO             PET4 index= 497                            MPIR_CVAR_ISCATTER_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ISCATTER_KNOMIAL_RADIX @Default:# -1
20220327 015824.561 INFO             PET4 index= 498                    MPIR_CVAR_ISCATTER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ISCATTER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.561 INFO             PET4 index= 499                       MPIR_CVAR_ISCATTER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ISCATTER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.561 INFO             PET4 index= 500                                   MPIR_CVAR_USE_IALLGATHERV : Control selection of MPI_Iallgatherv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_IALLGATHERV @Default:# -1
20220327 015824.561 INFO             PET4 index= 501                              MPIR_CVAR_USE_IALLGATHERV_LIST : @Alias:# I_MPI_ADJUST_IALLGATHERV_LIST @Default:# -1
20220327 015824.561 INFO             PET4 index= 502                       MPIR_CVAR_USE_IALLGATHERV_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLGATHERV_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET4 index= 503                           MPIR_CVAR_USE_IALLGATHERV_NETWORK : @Alias:# I_MPI_ADJUST_IALLGATHERV_NETWORK @Default:# -1
20220327 015824.561 INFO             PET4 index= 504                              MPIR_CVAR_USE_IALLGATHERV_NODE : @Alias:# I_MPI_ADJUST_IALLGATHERV_NODE @Default:# -1
20220327 015824.561 INFO             PET4 index= 505                                       MPIR_CVAR_USE_IEXSCAN : Control selection of MPI_Iexscan algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-1 @Alias:# I_MPI_ADJUST_IEXSCAN @Default:# -1
20220327 015824.561 INFO             PET4 index= 506                                  MPIR_CVAR_USE_IEXSCAN_LIST : @Alias:# I_MPI_ADJUST_IEXSCAN_LIST @Default:# -1
20220327 015824.561 INFO             PET4 index= 507                           MPIR_CVAR_USE_IEXSCAN_COMPOSITION : @Alias:# I_MPI_ADJUST_IEXSCAN_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET4 index= 508                               MPIR_CVAR_USE_IEXSCAN_NETWORK : @Alias:# I_MPI_ADJUST_IEXSCAN_NETWORK @Default:# -1
20220327 015824.561 INFO             PET4 index= 509                                  MPIR_CVAR_USE_IEXSCAN_NODE : @Alias:# I_MPI_ADJUST_IEXSCAN_NODE @Default:# -1
20220327 015824.561 INFO             PET4 index= 510                                         MPIR_CVAR_USE_ISCAN : Control selection of MPI_Iscan algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-1 @Alias:# I_MPI_ADJUST_ISCAN @Default:# -1
20220327 015824.561 INFO             PET4 index= 511                                    MPIR_CVAR_USE_ISCAN_LIST : @Alias:# I_MPI_ADJUST_ISCAN_LIST @Default:# -1
20220327 015824.561 INFO             PET4 index= 512                             MPIR_CVAR_USE_ISCAN_COMPOSITION : @Alias:# I_MPI_ADJUST_ISCAN_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET4 index= 513                                 MPIR_CVAR_USE_ISCAN_NETWORK : @Alias:# I_MPI_ADJUST_ISCAN_NETWORK @Default:# -1
20220327 015824.561 INFO             PET4 index= 514                                    MPIR_CVAR_USE_ISCAN_NODE : @Alias:# I_MPI_ADJUST_ISCAN_NODE @Default:# -1
20220327 015824.561 INFO             PET4 index= 515                               MPIR_CVAR_USE_IREDUCE_SCATTER : Control selection of MPI_Ireduce_scatter algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER @Default:# -1
20220327 015824.561 INFO             PET4 index= 516                          MPIR_CVAR_USE_IREDUCE_SCATTER_LIST : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_LIST @Default:# -1
20220327 015824.561 INFO             PET4 index= 517                   MPIR_CVAR_USE_IREDUCE_SCATTER_COMPOSITION : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET4 index= 518                       MPIR_CVAR_USE_IREDUCE_SCATTER_NETWORK : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_NETWORK @Default:# -1
20220327 015824.561 INFO             PET4 index= 519                          MPIR_CVAR_USE_IREDUCE_SCATTER_NODE : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_NODE @Default:# -1
20220327 015824.561 INFO             PET4 index= 520                         MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK : Control selection of MPI_Ireduce_scatter_block algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK @Default:# -1
20220327 015824.561 INFO             PET4 index= 521                    MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK_LIST : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK_LIST @Default:# -1
20220327 015824.561 INFO             PET4 index= 522             MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK_COMPOSITION : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET4 index= 523                 MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK_NETWORK : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK_NETWORK @Default:# -1
20220327 015824.561 INFO             PET4 index= 524                    MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK_NODE : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK_NODE @Default:# -1
20220327 015824.561 INFO             PET4 index= 525                               MPIR_CVAR_IMPI_SHMGR_DATASIZE : Define the size of shared memory area available for each rank for data placement. Messages greater than this value will not be processed by SHM-based collective operation, but will be processed by point-to-point based collective operation. The value must be a multiple of 4096. @Alias:# I_MPI_COLL_SHM_THRESHOLD @Default:# 16384
20220327 015824.561 INFO             PET4 index= 526                              MPIR_CVAR_IMPI_SHMGR_SPINCOUNT : @Alias:# I_MPI_COLL_SHM_PROGRESS_SPIN_COUNT
20220327 015824.561 INFO             PET4 index= 527                              MPIR_CVAR_INTEL_COLL_INTRANODE : @Alias:# I_MPI_COLL_INTRANODE
20220327 015824.561 INFO             PET4 index= 528                         MPIR_CVAR_ENABLE_EXPERIMENTAL_ALGOS : @Alias:# I_MPI_COLL_EXPERIMENTAL
20220327 015824.561 INFO             PET4 index= 529                                    MPIR_CVAR_IMPI_WAIT_MODE : @Alias:# I_MPI_WAIT_MODE
20220327 015824.561 INFO             PET4 index= 530                                 MPIR_CVAR_IMPI_THREAD_SLEEP : @Alias:# I_MPI_THREAD_SLEEP
20220327 015824.561 INFO             PET4 index= 531                          MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the short message algorithm will be used if the send buffer size is < this value (in bytes). (See also: MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE)
20220327 015824.561 INFO             PET4 index= 532                           MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the long message algorithm will be used if the send buffer size is >= this value (in bytes) (See also: MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE)
20220327 015824.561 INFO             PET4 index= 533                      MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE : The smallest message size that will be used for the pipelined, large-message, ring algorithm in the MPI_Allgatherv implementation.
20220327 015824.561 INFO             PET4 index= 534                          MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20220327 015824.561 INFO             PET4 index= 535                            MPIR_CVAR_ENABLE_SMP_COLLECTIVES : Enable SMP aware collective communication.
20220327 015824.561 INFO             PET4 index= 536                              MPIR_CVAR_ENABLE_SMP_ALLREDUCE : Enable SMP aware allreduce.
20220327 015824.561 INFO             PET4 index= 537                        MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE : Maximum message size for which SMP-aware allreduce is used.  A value of '0' uses SMP-aware allreduce for all message sizes.
20220327 015824.561 INFO             PET4 index= 538                           MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE : the short message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value (See also: MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE)
20220327 015824.561 INFO             PET4 index= 539                          MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE : the medium message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value and larger than MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE (See also: MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE)
20220327 015824.561 INFO             PET4 index= 540                                MPIR_CVAR_ENABLE_SMP_BARRIER : Enable SMP aware barrier.
20220327 015824.561 INFO             PET4 index= 541                                   MPIR_CVAR_BCAST_MIN_PROCS : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_SHORT_MSG_SIZE, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20220327 015824.561 INFO             PET4 index= 542                              MPIR_CVAR_BCAST_SHORT_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20220327 015824.561 INFO             PET4 index= 543                               MPIR_CVAR_BCAST_LONG_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_SHORT_MSG_SIZE)
20220327 015824.561 INFO             PET4 index= 544                                  MPIR_CVAR_ENABLE_SMP_BCAST : Enable SMP aware broadcast (See also: MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE)
20220327 015824.561 INFO             PET4 index= 545                            MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE : Maximum message size for which SMP-aware broadcast is used.  A value of '0' uses SMP-aware broadcast for all message sizes. (See also: MPIR_CVAR_ENABLE_SMP_BCAST)
20220327 015824.561 INFO             PET4 index= 546                            MPIR_CVAR_GATHER_VSMALL_MSG_SIZE : use a temporary buffer for intracommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE)
20220327 015824.561 INFO             PET4 index= 547                       MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_VSMALL_MSG_SIZE)
20220327 015824.561 INFO             PET4 index= 548                     MPIR_CVAR_GATHERV_INTER_SSEND_MIN_PROCS : Use Ssend (synchronous send) for intercommunicator MPI_Gatherv if the "group B" size is >= this value.  Specifying "-1" always avoids using Ssend.  For backwards compatibility, specifying "0" uses the default value.
20220327 015824.561 INFO             PET4 index= 549                             MPIR_CVAR_REDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20220327 015824.561 INFO             PET4 index= 550                                 MPIR_CVAR_ENABLE_SMP_REDUCE : Enable SMP aware reduce.
20220327 015824.561 INFO             PET4 index= 551                           MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE : Maximum message size for which SMP-aware reduce is used.  A value of '0' uses SMP-aware reduce for all message sizes.
20220327 015824.561 INFO             PET4 index= 552                      MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Scatter if the send buffer size is < this value (in bytes)
20220327 015824.561 INFO             PET4 index= 553                                  MPIR_CVAR_USE_CPU_PLATFORM : @Alias:# I_MPI_PLATFORM
20220327 015824.561 INFO             PET4 index= 554                          MPIR_CVAR_FAILURE_ON_COLL_FALLBACK : @Alias:# I_MPI_ADJUST_FAILURE_ON_COLL_FALLBACK
20220327 015824.561 INFO             PET4 index= 555                         MPIR_CVAR_FAILURE_ON_MATCH_FALLBACK : @Alias:# I_MPI_ADJUST_FAILURE_ON_MATCH_FALLBACK
20220327 015824.561 INFO             PET4 index= 556                           MPIR_CVAR_ADJUST_SENDRECV_REPLACE : @Alias:# I_MPI_ADJUST_SENDRECV_REPLACE
20220327 015824.561 INFO             PET4 index= 557                MPIR_CVAR_ADJUST_SENDRECV_REPLACE_FRAME_SIZE : @Alias:# I_MPI_ADJUST_SENDRECV_REPLACE_FRAME_SIZE
20220327 015824.562 INFO             PET4 index= 558                         MPIR_CVAR_NUMERICAL_REPRODUCIBILITY : @Alias:# I_MPI_CBWR
20220327 015824.562 INFO             PET4 index= 559                                    MPIR_CVAR_USE_TUNING_CH4 : @Alias:# I_MPI_TUNING
20220327 015824.562 INFO             PET4 index= 560                                    MPIR_CVAR_USE_TUNING_NET : @Alias:# I_MPI_TUNING_NETWORK
20220327 015824.562 INFO             PET4 index= 561                                    MPIR_CVAR_USE_TUNING_SHM : @Alias:# I_MPI_TUNING_NODE
20220327 015824.562 INFO             PET4 index= 562                                   MPIR_CVAR_DUMP_TUNING_CH4 : @Alias:# I_MPI_TUNING_COMPOSITION_DUMP
20220327 015824.562 INFO             PET4 index= 563                                   MPIR_CVAR_DUMP_TUNING_NET : @Alias:# I_MPI_TUNING_NETWORK_DUMP
20220327 015824.562 INFO             PET4 index= 564                                   MPIR_CVAR_DUMP_TUNING_SHM : @Alias:# I_MPI_TUNING_NODE_DUMP
20220327 015824.562 INFO             PET4 index= 565                                        MPIR_CVAR_BIN_TUNING : @Alias:# I_MPI_TUNING_BIN
20220327 015824.562 INFO             PET4 index= 566                                   MPIR_CVAR_BIN_DUMP_TUNING : @Alias:# I_MPI_TUNING_BIN_DUMP
20220327 015824.562 INFO             PET4 index= 567                                   MPIR_CVAR_TUNING_BIN_PATH : @Alias:# I_MPI_TUNING_BIN_PATH
20220327 015824.562 INFO             PET4 index= 568                            MPIR_CVAR_TUNING_COMPOSITION_PPN : @Alias:# I_MPI_TUNING_COMPOSITION_PPN
20220327 015824.562 INFO             PET4 index= 569                                MPIR_CVAR_TUNING_NETWORK_PPN : @Alias:# I_MPI_TUNING_NETWORK_PPN
20220327 015824.562 INFO             PET4 index= 570                                   MPIR_CVAR_TUNING_NODE_PPN : @Alias:# I_MPI_TUNING_NODE_PPN
20220327 015824.562 INFO             PET4 index= 571                 MPIR_CVAR_TUNING_COMPOSITION_COMM_HIERARCHY : @Alias:# I_MPI_TUNING_COMPOSITION_COMM_HIERARCHY
20220327 015824.562 INFO             PET4 index= 572                     MPIR_CVAR_TUNING_NETWORK_COMM_HIERARCHY : @Alias:# I_MPI_TUNING_NETWORK_COMM_HIERARCHY
20220327 015824.562 INFO             PET4 index= 573                        MPIR_CVAR_TUNING_NODE_COMM_HIERARCHY : @Alias:# I_MPI_TUNING_NODE_COMM_HIERARCHY
20220327 015824.562 INFO             PET4 index= 574                                       MPIR_CVAR_TUNING_MODE : @Alias:# I_MPI_TUNING_MODE
20220327 015824.562 INFO             PET4 index= 575                                MPIR_CVAR_TUNING_AUTO_POLICY : @Alias:# I_MPI_TUNING_AUTO_POLICY
20220327 015824.562 INFO             PET4 index= 576                             MPIR_CVAR_TUNING_AUTO_COMM_LIST : @Alias:# I_MPI_TUNING_AUTO_COMM_LIST
20220327 015824.562 INFO             PET4 index= 577                             MPIR_CVAR_TUNING_AUTO_COMM_USER : @Alias:# I_MPI_TUNING_AUTO_COMM_USER
20220327 015824.562 INFO             PET4 index= 578                          MPIR_CVAR_TUNING_AUTO_COMM_DEFAULT : @Alias:# I_MPI_TUNING_AUTO_COMM_DEFAULT
20220327 015824.562 INFO             PET4 index= 579                              MPIR_CVAR_TUNING_AUTO_ITER_NUM : @Alias:# I_MPI_TUNING_AUTO_ITER_NUM
20220327 015824.562 INFO             PET4 index= 580                           MPIR_CVAR_TUNING_AUTO_ITER_POLICY : @Alias:# I_MPI_TUNING_AUTO_ITER_POLICY
20220327 015824.562 INFO             PET4 index= 581                 MPIR_CVAR_TUNING_AUTO_ITER_POLICY_THRESHOLD : @Alias:# I_MPI_TUNING_AUTO_ITER_POLICY_THRESHOLD
20220327 015824.562 INFO             PET4 index= 582                                  MPIR_CVAR_TUNING_AUTO_SYNC : @Alias:# I_MPI_TUNING_AUTO_SYNC
20220327 015824.562 INFO             PET4 index= 583                          MPIR_CVAR_TUNING_AUTO_STORAGE_SIZE : @Alias:# I_MPI_TUNING_AUTO_STORAGE_SIZE
20220327 015824.562 INFO             PET4 index= 584                       MPIR_CVAR_TUNING_AUTO_WARMUP_ITER_NUM : @Alias:# I_MPI_TUNING_AUTO_WARMUP_ITER_NUM
20220327 015824.562 INFO             PET4 index= 585                                 MPIR_CVAR_TUNING_AUTO_SMART : @Alias:# I_MPI_TUNING_AUTO_SMART
20220327 015824.562 INFO             PET4 index= 586                                  MPIR_CVAR_TUNING_COLL_LIST : @Alias:# I_MPI_TUNING_COLL_LIST
20220327 015824.562 INFO             PET4 index= 587                               MPIR_CVAR_TUNING_COLL_VEC_OPS : @Alias:# I_MPI_TUNING_COLL_VEC_OPS
20220327 015824.562 INFO             PET4 index= 588                              MPIR_CVAR_DEFAULT_THREAD_LEVEL : @Alias:# I_MPI_THREAD_LEVEL
20220327 015824.562 INFO             PET4 index= 589                                      MPIR_CVAR_THREAD_SPLIT : Control the MPI_THREAD_SPLIT model support. @Alias:# I_MPI_THREAD_SPLIT @Default:# false
20220327 015824.562 INFO             PET4 index= 590                                    MPIR_CVAR_THREAD_RUNTIME : Control threading runtimes support. @Alias:# I_MPI_THREAD_RUNTIME @Default:# generic
20220327 015824.562 INFO             PET4 index= 591                                     MPIR_CVAR_THREAD_ID_KEY : Set the MPI info object key that is used to explicitly define the application $thread_id for a communicator. @Alias:# I_MPI_THREAD_ID_KEY @Default:# -1
20220327 015824.562 INFO             PET4 index= 592                                        MPIR_CVAR_THREAD_MAX : Set the maximum number of application threads per rank. @Alias:# I_MPI_THREAD_MAX @Default:# -1
20220327 015824.562 INFO             PET4 index= 593                                    MPIR_CVAR_ASYNC_PROGRESS : Enables asynchronous progress threads. @Alias:# I_MPI_ASYNC_PROGRESS @Default:# false
20220327 015824.562 INFO             PET4 index= 594                          MPIR_CVAR_CH4_MAX_PROGRESS_THREADS : Specifies the maximum number of progress threads. @Alias:# I_MPI_ASYNC_PROGRESS_THREADS @Default:# 1
20220327 015824.562 INFO             PET4 index= 595                      MPIR_CVAR_CH4_PROGRESS_THREAD_AFFINITY : Specifies affinity for all progress threads of local processes. @Alias:# I_MPI_ASYNC_PROGRESS_PIN @Default:# not defined
20220327 015824.562 INFO             PET4 index= 596                                         MPIR_CVAR_EP_ID_KEY : Set the MPI info object key that is used to explicitly define the progress $thread_id for a communicator. @Alias:# I_MPI_ASYNC_PROGRESS_ID_KEY @Default:# thread_id
20220327 015824.562 INFO             PET4 index= 597                                       MPIR_CVAR_THREAD_MODE : @Alias:# I_MPI_THREAD_MODE
20220327 015824.562 INFO             PET4 index= 598                                 MPIR_CVAR_THREAD_LOCK_LEVEL : @Alias:# I_MPI_THREAD_LOCK_LEVEL
20220327 015824.562 INFO             PET4 index= 599                                  MPIR_CVAR_CH4_OFI_MAX_VCIS : @Alias:# I_MPI_THREAD_EP_MAX
20220327 015824.562 INFO             PET4 index= 600                                       MPIR_CVAR_INTEL_DEBUG : Print out debugging information when an MPI program starts running.$ Syntax$ I_MPI_DEBUG=<level>$ Arguments$ <level> - indicate level of debug information provided @Alias:# I_MPI_DEBUG @Default:# 0
20220327 015824.562 INFO             PET4 index= 601                                     MPIR_CVAR_DEBUG_VERSION : Print Intel MPI version. @Alias:# I_MPI_PRINT_VERSION @Default:# 0
20220327 015824.562 INFO             PET4 index= 602                                    MPIR_CVAR_ERROR_CHECKING : @Alias:# I_MPI_ERROR_CHECKING
20220327 015824.562 INFO             PET4 index= 603                                        MPIR_CVAR_MULTI_INIT : @Alias:# I_MPI_MULTI_INIT
20220327 015824.562 INFO             PET4 index= 604                               MPIR_CVAR_REMOVED_VAR_WARNING : Print out a warning if a removed environment variable is set. @Alias:# I_MPI_REMOVED_VAR_WARNING @Default:# 1
20220327 015824.562 INFO             PET4 index= 605                                MPIR_CVAR_VAR_CHECK_SPELLING : Print out a warning if an unknown environment variable is set. @Alias:# I_MPI_VAR_CHECK_SPELLING @Default:# 1
20220327 015824.562 INFO             PET4 index= 606                           MPIR_CVAR_INTEL_MPI_COMPATIBILITY : Select the runtime compatibility mode.$ Syntax$ I_MPI_COMPATIBILITY=<value>$ Arguments$ <value> - Define compatibility mode$ -----------------------------------------------------------------------$ <not defined> - The MPI-3.1 standard compatibility$ <3> - The Intel® MPI Library 3.x compatible mode$ <4> - The Intel® MPI Library 4.x compatible mode$ <5> - The Intel® MPI Library 5.x compatible mode$ ----------------------------------------------------------------------- @Alias:# I_MPI_COMPATIBILITY @Default:# 5
20220327 015824.562 INFO             PET4 index= 607                          MPIR_CVAR_IMPI_PROGRESS_SPIN_COUNT : @Alias:# I_MPI_SPIN_COUNT
20220327 015824.562 INFO             PET4 index= 608                         MPIR_CVAR_IMPI_PROGRESS_PAUSE_COUNT : @Alias:# I_MPI_PAUSE_COUNT
20220327 015824.562 INFO             PET4 index= 609                                 MPIR_CVAR_IMPI_THREAD_YIELD : @Alias:# I_MPI_THREAD_YIELD
20220327 015824.562 INFO             PET4 index= 610                                      MPIR_CVAR_SILENT_ABORT : Do not print abort warning message @Alias:# I_MPI_SILENT_ABORT
20220327 015824.562 INFO             PET4 index= 611                                  MPIR_CVAR_JOB_IDLE_TIMEOUT : Abort job if idle time is larger than the threshold in seconds. @Alias:# I_MPI_JOB_IDLE_TIMEOUT
20220327 015824.562 INFO             PET4 index= 612                              MPIR_CVAR_PMI_VALUE_LENGTH_MAX : Set PMI buffer length as minimum of variable value and PMI_KVS_Get_value_length_max(). @Alias:# I_MPI_PMI_VALUE_LENGTH_MAX
20220327 015824.562 INFO             PET4 index= 613                                       MPIR_CVAR_PMI_LIBRARY : Specify the name to third party implementation of the PMI library. @Alias:# I_MPI_PMI_LIBRARY
20220327 015824.562 INFO             PET4 index= 614                                               MPIR_CVAR_PMI : Select PMI version. Choices: auto, pmi1, pmi2, pmix.$ By default pmi version will be chosen automatically. @Alias:# I_MPI_PMI
20220327 015824.562 INFO             PET4 index= 615                                 MPIR_CVAR_NODEMAP_ALGORITHM : Select algorithm for nodemap creation. Choices: pmi_process_mapping, slurm, pmi_alltoall, auto. @Alias:# I_MPI_NODEMAP_ALGORITHM
20220327 015824.562 INFO             PET4 index= 616                                      MPIR_CVAR_ASYNC_REDUCE : @Alias:# I_MPI_ASYNC_REDUCE @Verbosity:# hidden
20220327 015824.562 INFO             PET4 index= 617                            MPIR_CVAR_CH4_MAX_REDUCE_THREADS : @Alias:# I_MPI_ASYNC_REDUCE_THREADS @Verbosity:# hidden
20220327 015824.562 INFO             PET4 index= 618                      MPIR_CVAR_ASYNC_REDUCE_COUNT_THRESHOLD : @Alias:# I_MPI_ASYNC_REDUCE_COUNT_THRESHOLD @Verbosity:# hidden
20220327 015824.562 INFO             PET4 index= 619                        MPIR_CVAR_CH4_REDUCE_THREAD_AFFINITY : @Alias:# I_MPI_ASYNC_REDUCE_PIN @Verbosity:# hidden
20220327 015824.562 INFO             PET4 --- VMK::logSystem() end ---------------------------------
20220327 015824.562 INFO             PET4 main: --- VMK::log() start -------------------------------------
20220327 015824.562 INFO             PET4 main: vm located at: 0x87ae60
20220327 015824.562 INFO             PET4 main: petCount=6 localPet=4 mypthid=140737352203136 currentSsiPe=16
20220327 015824.562 INFO             PET4 main: Current system level affinity pinning for local PET:
20220327 015824.562 INFO             PET4 main:  SSIPE=16
20220327 015824.562 INFO             PET4 main:  SSIPE=17
20220327 015824.562 INFO             PET4 main:  SSIPE=18
20220327 015824.562 INFO             PET4 main:  SSIPE=19
20220327 015824.563 INFO             PET4 main: Current system level OMP_NUM_THREADS setting for local PET: 1
20220327 015824.563 INFO             PET4 main: ssiCount=1 localSsi=0
20220327 015824.563 INFO             PET4 main: mpionly=1 threadsflag=0
20220327 015824.563 INFO             PET4 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220327 015824.563 INFO             PET4 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220327 015824.563 INFO             PET4 main:  PE=0 SSI=0 SSIPE=0
20220327 015824.563 INFO             PET4 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220327 015824.563 INFO             PET4 main:  PE=1 SSI=0 SSIPE=1
20220327 015824.563 INFO             PET4 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220327 015824.563 INFO             PET4 main:  PE=2 SSI=0 SSIPE=2
20220327 015824.563 INFO             PET4 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220327 015824.563 INFO             PET4 main:  PE=3 SSI=0 SSIPE=3
20220327 015824.563 INFO             PET4 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220327 015824.563 INFO             PET4 main:  PE=4 SSI=0 SSIPE=4
20220327 015824.563 INFO             PET4 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220327 015824.563 INFO             PET4 main:  PE=5 SSI=0 SSIPE=5
20220327 015824.563 INFO             PET4 main: --- VMK::log() end ---------------------------------------
20220327 015824.565 INFO             PET4 Executing 'userm1_setvm'
20220327 015824.566 INFO             PET4 Executing 'userm1_register'
20220327 015824.566 INFO             PET4 Executing 'userm2_setvm'
20220327 015824.566 DEBUG            PET4 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220327 015824.566 DEBUG            PET4 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220327 015824.649 INFO             PET4 Entering 'user1_run'
20220327 015824.649 INFO             PET4 model1: --- VMK::log() start -------------------------------------
20220327 015824.649 INFO             PET4 model1: vm located at: 0xa38690
20220327 015824.649 INFO             PET4 model1: petCount=6 localPet=4 mypthid=140737352203136 currentSsiPe=4
20220327 015824.649 INFO             PET4 model1: Current system level affinity pinning for local PET:
20220327 015824.649 INFO             PET4 model1:  SSIPE=4
20220327 015824.649 INFO             PET4 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220327 015824.649 INFO             PET4 model1: ssiCount=1 localSsi=0
20220327 015824.649 INFO             PET4 model1: mpionly=1 threadsflag=0
20220327 015824.649 INFO             PET4 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220327 015824.649 INFO             PET4 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220327 015824.649 INFO             PET4 model1:  PE=0 SSI=0 SSIPE=0
20220327 015824.649 INFO             PET4 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220327 015824.649 INFO             PET4 model1:  PE=1 SSI=0 SSIPE=1
20220327 015824.649 INFO             PET4 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220327 015824.649 INFO             PET4 model1:  PE=2 SSI=0 SSIPE=2
20220327 015824.649 INFO             PET4 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220327 015824.649 INFO             PET4 model1:  PE=3 SSI=0 SSIPE=3
20220327 015824.649 INFO             PET4 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220327 015824.649 INFO             PET4 model1:  PE=4 SSI=0 SSIPE=4
20220327 015824.649 INFO             PET4 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220327 015824.649 INFO             PET4 model1:  PE=5 SSI=0 SSIPE=5
20220327 015824.649 INFO             PET4 model1: --- VMK::log() end ---------------------------------------
20220327 015824.649 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220327 015824.774 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220327 015824.899 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220327 015825.024 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220327 015825.149 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220327 015825.274 INFO             PET4 Exiting 'user1_run'
20220327 015826.769 INFO             PET4 Entering 'user1_run'
20220327 015826.769 INFO             PET4 model1: --- VMK::log() start -------------------------------------
20220327 015826.770 INFO             PET4 model1: vm located at: 0xa38690
20220327 015826.770 INFO             PET4 model1: petCount=6 localPet=4 mypthid=140737352203136 currentSsiPe=4
20220327 015826.770 INFO             PET4 model1: Current system level affinity pinning for local PET:
20220327 015826.770 INFO             PET4 model1:  SSIPE=4
20220327 015826.770 INFO             PET4 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220327 015826.770 INFO             PET4 model1: ssiCount=1 localSsi=0
20220327 015826.770 INFO             PET4 model1: mpionly=1 threadsflag=0
20220327 015826.770 INFO             PET4 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220327 015826.770 INFO             PET4 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220327 015826.770 INFO             PET4 model1:  PE=0 SSI=0 SSIPE=0
20220327 015826.770 INFO             PET4 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220327 015826.770 INFO             PET4 model1:  PE=1 SSI=0 SSIPE=1
20220327 015826.770 INFO             PET4 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220327 015826.770 INFO             PET4 model1:  PE=2 SSI=0 SSIPE=2
20220327 015826.770 INFO             PET4 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220327 015826.770 INFO             PET4 model1:  PE=3 SSI=0 SSIPE=3
20220327 015826.770 INFO             PET4 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220327 015826.770 INFO             PET4 model1:  PE=4 SSI=0 SSIPE=4
20220327 015826.770 INFO             PET4 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220327 015826.770 INFO             PET4 model1:  PE=5 SSI=0 SSIPE=5
20220327 015826.770 INFO             PET4 model1: --- VMK::log() end ---------------------------------------
20220327 015826.770 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220327 015826.895 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220327 015827.019 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220327 015827.144 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220327 015827.269 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220327 015827.394 INFO             PET4 Exiting 'user1_run'
20220327 015828.883 INFO             PET4  NUMBER_OF_PROCESSORS           6
20220327 015828.883 INFO             PET4  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220327 015828.883 INFO             PET4 Finalizing ESMF
20220327 015824.554 INFO             PET5 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220327 015824.554 INFO             PET5 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220327 015824.554 INFO             PET5 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220327 015824.554 INFO             PET5 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220327 015824.554 INFO             PET5 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220327 015824.554 INFO             PET5 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220327 015824.554 INFO             PET5 Running with ESMF Version   : v8.3.0b10-105-ga5295e34ae
20220327 015824.554 INFO             PET5 ESMF library build date/time: "Mar 27 2022" "01:23:54"
20220327 015824.554 INFO             PET5 ESMF library build location : /gpfsm/dnb04/projects/p98/mpotts/esmf/intel_19.1.3_intelmpi_O_develop
20220327 015824.554 INFO             PET5 ESMF_COMM                   : intelmpi
20220327 015824.555 INFO             PET5 ESMF_MOAB                   : enabled
20220327 015824.555 INFO             PET5 ESMF_LAPACK                 : enabled
20220327 015824.555 INFO             PET5 ESMF_NETCDF                 : enabled
20220327 015824.555 INFO             PET5 ESMF_PNETCDF                : disabled
20220327 015824.555 INFO             PET5 ESMF_PIO                    : enabled
20220327 015824.555 INFO             PET5 ESMF_YAMLCPP                : enabled
20220327 015824.555 INFO             PET5 --- VMK::logSystem() start -------------------------------
20220327 015824.555 INFO             PET5 esmfComm=intelmpi
20220327 015824.555 INFO             PET5 isPthreadsEnabled=1
20220327 015824.555 INFO             PET5 isOpenMPEnabled=1
20220327 015824.555 INFO             PET5 isOpenACCEnabled=0
20220327 015824.555 INFO             PET5 isSsiSharedMemoryEnabled=1
20220327 015824.556 INFO             PET5 ssiCount=1 peCount=6
20220327 015824.556 INFO             PET5 PE=0 SSI=0 SSIPE=0
20220327 015824.556 INFO             PET5 PE=1 SSI=0 SSIPE=1
20220327 015824.556 INFO             PET5 PE=2 SSI=0 SSIPE=2
20220327 015824.556 INFO             PET5 PE=3 SSI=0 SSIPE=3
20220327 015824.556 INFO             PET5 PE=4 SSI=0 SSIPE=4
20220327 015824.556 INFO             PET5 PE=5 SSI=0 SSIPE=5
20220327 015824.556 INFO             PET5 --- VMK::logSystem() MPI Control Variables ---------------
20220327 015824.556 INFO             PET5 index=   0                                          I_MPI_DEBUG_OUTPUT : @Default:# not defined
20220327 015824.556 INFO             PET5 index=   1                                        I_MPI_DEBUG_COREDUMP : @Default:# 1
20220327 015824.556 INFO             PET5 index=   2                                          I_MPI_LIBRARY_KIND : @Default:# not defined
20220327 015824.556 INFO             PET5 index=   3                                  I_MPI_OFI_LIBRARY_INTERNAL : @Default:# not defined
20220327 015824.556 INFO             PET5 index=   4                                            I_MPI_CC_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET5 index=   5                                           I_MPI_CXX_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET5 index=   6                                            I_MPI_FC_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET5 index=   7                                           I_MPI_F77_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET5 index=   8                                           I_MPI_F90_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET5 index=   9                                         I_MPI_TRACE_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  10                                         I_MPI_CHECK_PROFILE : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  11                                        I_MPI_CHECK_COMPILER : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  12                                                    I_MPI_CC : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  13                                                   I_MPI_CXX : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  14                                                    I_MPI_FC : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  15                                                   I_MPI_F90 : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  16                                                   I_MPI_F77 : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  17                                                  I_MPI_ROOT : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  18                                           I_MPI_ONEAPI_ROOT : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  19                                           MPIR_CVAR_VT_ROOT : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  20                                   I_MPI_COMPILER_CONFIG_DIR : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  21                                                  I_MPI_LINK : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  22                                      I_MPI_DEBUG_INFO_STRIP : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  23                                                I_MPI_CFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  24                                              I_MPI_CXXFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  25                                               I_MPI_FCFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  26                                                I_MPI_FFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  27                                               I_MPI_LDFLAGS : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  28                                             I_MPI_FORT_BIND : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  29                                           I_MPI_AUTH_METHOD : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  30                               I_MPI_HYDRA_COLLECTIVE_LAUNCH : @Default:# 1
20220327 015824.556 INFO             PET5 index=  31                                  I_MPI_HYDRA_UNIQUE_PROXIES : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  32                                        I_MPI_FAULT_CONTINUE : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  33                                   I_MPI_FAULT_NODE_CONTINUE : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  34                                                I_MPI_MPIRUN : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  35                                            I_MPI_BIND_ORDER : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  36                                             I_MPI_BIND_NUMA : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  37                                     I_MPI_BIND_WIN_ALLOCATE : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  38                                      I_MPI_HYDRA_NAMESERVER : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  39                                        I_MPI_JOB_CHECK_LIBS : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  40                                    I_MPI_HYDRA_SERVICE_PORT : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  41                                           I_MPI_HYDRA_DEBUG : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  42                                             I_MPI_HYDRA_ENV : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  43                                           I_MPI_JOB_TIMEOUT : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  44                                       I_MPI_MPIEXEC_TIMEOUT : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  45                                   I_MPI_JOB_STARTUP_TIMEOUT : Set this environment variable to make mpiexec.hydra terminate$ the job in <timeout> seconds if some processes are not launched. @Default:# -1
20220327 015824.556 INFO             PET5 index=  46                                    I_MPI_JOB_TIMEOUT_SIGNAL : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  47                                      I_MPI_JOB_ABORT_SIGNAL : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  48                                I_MPI_JOB_SIGNAL_PROPAGATION : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  49                                  I_MPI_HYDRA_BOOTSTRAP_EXEC : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  50                       I_MPI_HYDRA_BOOTSTRAP_EXEC_EXTRA_ARGS : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  51                              I_MPI_HYDRA_BOOTSTRAP_AUTOFORK : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  52                                             I_MPI_HYDRA_RMK : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  53                                     I_MPI_HYDRA_PMI_CONNECT : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  54                                         I_MPI_HYDRA_TOPOLIB : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  55                                            I_MPI_PORT_RANGE : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  56                         I_MPI_JOB_RESPECT_PROCESS_PLACEMENT : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  57                                                I_MPI_TMPDIR : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  58                                           I_MPI_HYDRA_DEMUX : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  59                                           I_MPI_HYDRA_IFACE : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  60                                I_MPI_HYDRA_GDB_REMOTE_SHELL : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  61                                   I_MPI_HYDRA_PMI_AGGREGATE : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  62                                        I_MPI_JOB_TRACE_LIBS : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  63                                       I_MPI_HYDRA_HOST_FILE : Set the host file to run the application.$ Syntax$ I_MPI_HYDRA_HOST_FILE=<arg>$ Arguments$ <arg> - String parameter$ -----------------------------------------------------------------------$ <hostsfile> - The full or relative path to the host file$ ----------------------------------------------------------------------- @Default:# not defined
20220327 015824.556 INFO             PET5 index=  64                                     I_MPI_HYDRA_HOSTS_GROUP : This environment variable allows to set node ranges using brackets,$ commas, and dashes (like in Slurm* Workload Manager). @Default:# not defined
20220327 015824.556 INFO             PET5 index=  65                                               I_MPI_PERHOST : Define the default behavior for the -perhost option of the mpiexec.hydra$ command.$ Syntax$ I_MPI_PERHOST=<value>$ Arguments$ <value> - Define a value used for -perhost by default$ -----------------------------------------------------------------------$ <integer > 0> - Exact value for the option$ <all>         - All logical CPUs on the node$ <allcores>    - All cores (physical CPUs) on the node. This is the$  default value.$ ----------------------------------------------------------------------- @Default:# not defined
20220327 015824.556 INFO             PET5 index=  66                                                 I_MPI_GTOOL : Specify the tools to be launched for selected ranks. An alternative to$ this variable is the -gtool option$ Syntax$ I_MPI_GTOOL="<command line for a tool 1>:<ranks set 1>[=exclusive]$ [@arch 1];<command line for a tool 2>:<ranks set 2>[=exclusive]$ [@arch 2]; â€¦ ;<command line for a tool n>:<ranks set n>[=exclusive]$ [@arch n]"$ Arguments$ <arg> - Specify a tool launch command, including parameters.$ -----------------------------------------------------------------------$ <command line>     - Specify tool launch command, including parameters$ <rank set>         - Specify the range of ranks that are involved in the$  tool execution. Separate ranks with a comma or use the '-' symbol for$ a set ofcontiguous ranks. To run the tool forall ranks, use the all$  argument.$ [=exclusive]       - All cores (physical CPUs) on the node. This is$ the default value.$ [@arch]            - Specify the architecture on which the tool runs$  optional). For a given <rank set>, if you specify this argument,$ the tool is launched
20220327 015824.556 INFO             PET5 index=  67                                    I_MPI_HYDRA_BRANCH_COUNT : Set this environment variable to restrict the number of child management$ processes launched by the mpiexec.hydra operation or by each pmi_proxy$ anagement process.$ Syntax$ I_MPI_HYDRA_BRANCH_COUNT=<num>$ Arguments$ <value> - Number$ -----------------------------------------------------------------------$ <n> >= 0 - The default value is -1 if less than 128 nodes are used. $ This value also means that there is no hierarchical structure$ The default value is 32 if more than 127 nodes are used$ ----------------------------------------------------------------------- @Default:# not defined
20220327 015824.556 INFO             PET5 index=  68                                       I_MPI_HYDRA_BOOTSTRAP : Set this environment variable to specify the bootstrap server Syntax$ I_MPI_HYDRA_BOOTSTRAP=<arg>$ Arguments$ <arg> - String parameter$ -----------------------------------------------------------------------$ <ssh>     - Use secure shell. This is the default value$ <rsh>     - Use remote shell$ <pdsh>    - Use parallel distributed shell$ <pbsdsh>  - Use Torque* and PBS* pbsdsh command$ <fork>    - Use fork call$ <slurm>   - Use SLURM* srun command$ <ll>      - Use LoadLeveler* llspawn.stdio command$ <lsf>     - Use LSF* blaunch command$ <sge>     - Use Univa* Grid Engine* qrsh command$ ----------------------------------------------------------------------- @Default:# not defined
20220327 015824.556 INFO             PET5 index=  69                                              I_MPI_PIN_UNIT : @Default:# not defined
20220327 015824.556 INFO             PET5 index=  70                                                 I_MPI_STATS :
20220327 015824.556 INFO             PET5 index=  71                                             I_MPI_TIMER_ART : @Default:# 1
20220327 015824.556 INFO             PET5 index=  72                                   I_MPI_OFFLOAD_DOMAIN_SIZE : Control the number of devices/tiles per MPI rank
20220327 015824.556 INFO             PET5 index=  73                                          I_MPI_OFFLOAD_CELL : Variable to choose the base unit: tile or device
20220327 015824.556 INFO             PET5 index=  74                                       I_MPI_OFFLOAD_DEVICES : Variable to select available devices
20220327 015824.556 INFO             PET5 index=  75                                   I_MPI_OFFLOAD_DEVICE_LIST : A comma-separated list of tiles and/or ranges of tiles.$ The process with the i-th rank is pinned to the i-th tile in the list.$ If I_MPI_OFFLOAD_CELL=device then it is comma-separated list of devices.
20220327 015824.556 INFO             PET5 index=  76                                        I_MPI_OFFLOAD_DOMAIN : Define domains through the comma separated list of hexadecimal numbers (domain masks).
20220327 015824.556 INFO             PET5 index=  77                               I_MPI_OFFLOAD_INFO_SET_NTILES : Set the number of tiles
20220327 015824.556 INFO             PET5 index=  78                                I_MPI_OFFLOAD_INFO_SET_NGPUS : Set the number of gpus
20220327 015824.556 INFO             PET5 index=  79                           I_MPI_OFFLOAD_INFO_SET_NNUMANODES : Set the number of numanodes
20220327 015824.556 INFO             PET5 index=  80                               I_MPI_OFFLOAD_INFO_SET_GPU_ID : Set gpu id for each tile
20220327 015824.556 INFO             PET5 index=  81                 I_MPI_OFFLOAD_INFO_SET_NUMANODE_ID_FOR_GPUS : Set numanode id for each gpu
20220327 015824.557 INFO             PET5 index=  82                I_MPI_OFFLOAD_INFO_SET_NUMANODE_ID_FOR_RANKS : Set numanode id for each rank
20220327 015824.557 INFO             PET5 index=  83                            I_MPI_OFFLOAD_INFO_SET_VENDOR_ID : Set vendor id for each device
20220327 015824.557 INFO             PET5 index=  84                                         MPIR_CVAR_INTEL_PIN : Turn on/off process pinning.$ Syntax$ I_MPI_PIN=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 - Enable process pinning$ > disable | no | off | 0 - Disable processes pinning$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN @Default:# on
20220327 015824.557 INFO             PET5 index=  85                          MPIR_CVAR_INTEL_PIN_SHOW_REAL_MASK : Turn on/off real masks pinning print.$ Syntax$ I_MPI_PIN_SHOW_REAL_MASK=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 - Enable real pinning print$ > disable | no | off | 0 - Disable real pinning print$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_SHOW_REAL_MASK @Default:# on
20220327 015824.557 INFO             PET5 index=  86                          MPIR_CVAR_INTEL_PIN_PROCESSOR_LIST : Define a processor subset and the mapping rules for MPI processes within$ this subset.$ Syntax$ I_MPI_PIN_PROCESSOR_LIST=<value>$ The environment variable value has the following syntax forms:$ 1. <proclist>$ 2. [<procset>][:[grain=<grain>][,shift=<shift>]$ [,preoffset=<preoffset>][,postoffset=<postoffset>]$ 3. [<procset>][:map=<map>]$ @Alias:# I_MPI_PIN_PROCESSOR_LIST @Default:# not defined
20220327 015824.557 INFO             PET5 index=  87                  MPIR_CVAR_INTEL_PIN_PROCESSOR_EXCLUDE_LIST : Define a subset of logical processors to be excluded for the pinning$ capability on the intended hosts.$ Syntax$ I_MPI_PIN_PROCESSOR_EXCLUDE_LIST=<proclist>$ Arguments$ <proclist> - A comma-separated list of logical processor numbers$ and/or ranges of processors.$ -----------------------------------------------------------------------$ > <l> - Processor with logical number <l>.$ > <l>-<m> - Range of processors with logical numbers from <l> to <m>.$ > <k>,<l>-<m> - Processors <k>, as well as <l> through <m>.$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_PROCESSOR_EXCLUDE_LIST @Default:# not defined
20220327 015824.557 INFO             PET5 index=  88                                    MPIR_CVAR_INTEL_PIN_CELL : Set this environment variable to define the pinning resolution$ granularity. I_MPI_PIN_CELL specifies the minimal processor cell$ allocated when an MPI process is running.$ Syntax$ I_MPI_PIN_CELL=<cell>$ Arguments$ <cell> - Specify the resolution granularity$ -----------------------------------------------------------------------$ > unit - Basic processor unit (logical CPU)$ > core - Physical processor core$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_CELL @Default:# unit
20220327 015824.557 INFO             PET5 index=  89                          MPIR_CVAR_INTEL_PIN_RESPECT_CPUSET : Respect the process affinity mask.$ Syntax$ I_MPI_PIN_RESPECT_CPUSET=<value>$ Arguments$ <value> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 - Respect the process affinity mask$ > disable | no | off | 0 - Do not respect the process affinity mask$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_RESPECT_CPUSET @Default:# on
20220327 015824.557 INFO             PET5 index=  90                             MPIR_CVAR_INTEL_PIN_RESPECT_HCA : In the presence of Intel(R) Omni-Path Architecture (Intel(R) OPA) or$ Infiniband architecture* host channel adapter (IBA* HCA),$ adjust the pinning according to the location of adapter.$ Syntax$ I_MPI_PIN_RESPECT_HCA=<value>$ Arguments$ <value> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 Use the location of IBA HCA if available$ > disable | no | off | 0 Do not use the location of IBA HCA$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_RESPECT_HCA @Default:# on
20220327 015824.557 INFO             PET5 index=  91                                  MPIR_CVAR_INTEL_PIN_DOMAIN : Intel(R) MPI Library provides environment variable to control process pinning for hybrid MPI/OpenMP* applications. This environment variable is used to define a number of non-overlapping subsets (domains) of logical processors on a node, and a set of rules on how MPI processes are bound to these domains by the following formula: one MPI process per one domain. Multi-core Shape:$ I_MPI_PIN_DOMAIN=<mc-shape>$ <mc-shape> - Define domains through multi-core terms.$ -----------------------------------------------------------------------$ > core - Each domain consists of the logical processors that share a$ particular core. The number of domains on a node is equal to the number$ of cores on the node.$ > socket | sock - Each domain consists of the logical processors that$ share a particular socket. The number of domains on a node is equal to$ the number of sockets on the node.$ > numa - Each domain consists of the logical processors that share a$ particular NUMA node. The number of domains on a machine is equal to$
20220327 015824.557 INFO             PET5 index=  92                                   MPIR_CVAR_INTEL_PIN_ORDER : Set this environment variable to define the mapping order for MPI$ processes to domains as specified by the$ I_MPI_PIN_DOMAIN environment variable.$ Syntax$ I_MPI_PIN_ORDER=<order>$ <order> - Specify the ranking order$ -----------------------------------------------------------------------$ > range - The domains are ordered according to the processor's BIOS$ numbering. This is a platform dependent numbering$ > scatter - The domains are ordered so that adjacent domains have$ minimal sharing of common resources$ > compact - The domains are ordered so that adjacent domains share$ common resources as much as possible. This is the default value$ > spread - The domains are ordered consecutively with the possibility$ not to share common resources$ > bunch - The processes are mapped proportionally to sockets and the$ domains are ordered as close as possible on the sockets$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_ORDER @Default:# compact
20220327 015824.557 INFO             PET5 index=  93                                   MPIR_CVAR_IMPI_HBW_POLICY : @Alias:# I_MPI_HBW_POLICY
20220327 015824.557 INFO             PET5 index=  94                          MPIR_CVAR_IMPI_INTERNAL_MEM_POLICY : @Alias:# I_MPI_INTERNAL_MEM_POLICY
20220327 015824.557 INFO             PET5 index=  95                                 MPIR_CVAR_IMPI_STATIC_BUILD : @Alias:# I_MPI_STATIC_BUILD
20220327 015824.557 INFO             PET5 index=  96                     MPIR_CVAR_IMPI_RETURN_INTERNAL_MEM_NUMA : @Alias:# I_MPI_RETURN_INTERNAL_MEM_NUMA
20220327 015824.557 INFO             PET5 index=  97                                   MPIR_CVAR_OFFLOAD_TOPOLIB : @Alias:# I_MPI_OFFLOAD_TOPOLIB
20220327 015824.557 INFO             PET5 index=  98                                        MPIR_CVAR_ENABLE_GPU : Control support of buffers offloaded to GPU/accelerator in MPI calls.$ Syntax$ I_MPI_OFFLOAD=<value>$ Arguments$ <value> - choice$ -----------------------------------------------------------------------$ <0> - Disabled$ <1> - Enabled only if Level Zero library is loaded at MPI_Init() time$ <2> - Enabled, will fail if Level Zero library is not loadable$ ----------------------------------------------------------------------- @Alias:# I_MPI_OFFLOAD @Default:# 0
20220327 015824.557 INFO             PET5 index=  99                        MPIR_CVAR_ENABLE_GPU_BUFFER_CHECKING : Turn on/off bounds checking of the offloaded buffers.$ @Alias:# I_MPI_OFFLOAD_BUFFER_CHECKING @Default:# 1
20220327 015824.557 INFO             PET5 index= 100                        MPIR_CVAR_OFFLOAD_LEVEL_ZERO_LIBRARY : Specify name or full path to Level Zero ze_loader library.$ @Alias:# I_MPI_OFFLOAD_LEVEL_ZERO_LIBRARY
20220327 015824.557 INFO             PET5 index= 101                            MPIR_CVAR_INTEL_EXTRA_FILESYSTEM : Turn on/off native parallel file systems support.$ Syntax$ I_MPI_EXTRA_FILESYSTEM=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 - Enable native support for parallel file$ systems.$ > disable | no | off | 0 - Disable native support for parallel file$ systems.$ ----------------------------------------------------------------------- @Alias:# I_MPI_EXTRA_FILESYSTEM @Default:# off
20220327 015824.557 INFO             PET5 index= 102                      MPIR_CVAR_INTEL_EXTRA_FILESYSTEM_FORCE : Force filesystem recognition logic.$ Syntax$ I_MPI_EXTRA_FILESYSTEM_FORCE=<ufs|nfs|gpfs|panfs|lustre|daos>$ @Alias:# I_MPI_EXTRA_FILESYSTEM_FORCE @Default:# not defined
20220327 015824.557 INFO             PET5 index= 103                                     MPIR_CVAR_INTEL_FABRICS : Select the particular fabrics to be used.$ Syntax$ I_MPI_FABRICS=<ofi|shm:ofi>$ Arguments$ <fabric> -  Define a network fabric.$ -----------------------------------------------------------------------$ > shm - Shared memory transport (used for intra-node$ communication only).$ > ofi - OpenFabrics Interfaces* (OFI)-capable network fabrics, such as$ Intel(R) True Scale Fabric, Intel(R) Omni-Path Architecture, InfiniBand*$ and Ethernet (through OFI$ API).$ ----------------------------------------------------------------------- @Alias:# I_MPI_FABRICS @Default:# shm:ofi
20220327 015824.557 INFO             PET5 index= 104                                       MPIR_CVAR_IMPI_MALLOC : Enable or disable the Intel MPI private memory allocator.$ ----------------------------------------------------------------------- @Alias:# I_MPI_MALLOC @Default:# 1
20220327 015824.557 INFO             PET5 index= 105                                    MPIR_CVAR_INTEL_SHM_HEAP : Enable or disable the Intel MPI shared memory allocator.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP @Default:# -1
20220327 015824.557 INFO             PET5 index= 106                                MPIR_CVAR_INTEL_SHM_HEAP_OPT : Shared memory heap optimization: "rank", "numa".$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_OPT @Default:# -1
20220327 015824.557 INFO             PET5 index= 107                              MPIR_CVAR_INTEL_SHM_HEAP_VSIZE : Set shared memory heap virtual size (in MB).$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_VSIZE @Default:# -1
20220327 015824.557 INFO             PET5 index= 108                              MPIR_CVAR_INTEL_SHM_HEAP_CSIZE : Set shared memory heap cache size (in MB).$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_CSIZE @Default:# -1
20220327 015824.557 INFO             PET5 index= 109                  MPIR_CVAR_INTEL_SHM_HEAP_NCONTIG_THRESHOLD : Set non-contig object size threshold for use shared memory allocator.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_NCONTIG_THRESHOLD @Default:# -1
20220327 015824.557 INFO             PET5 index= 110                          MPIR_CVAR_INTEL_SHM_HEAP_THRESHOLD : Set object size threshold for use shared memory allocator.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_THRESHOLD @Default:# -1
20220327 015824.557 INFO             PET5 index= 111        MPIR_CVAR_INTEL_SHM_RECV_HEAP_SHORT_MEMCPY_THRESHOLD : Threshold for short size messages receive via SHM HEAP transport.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_HEAP_SHORT_MEMCPY_THRESHOLD @Default:# -1
20220327 015824.557 INFO             PET5 index= 112      MPIR_CVAR_INTEL_SHM_RECV_HEAP_REGULAR_MEMCPY_THRESHOLD : Threshold for regular size message receive via SHM HEAP transport.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_HEAP_REGULAR_MEMCPY_THRESHOLD @Default:# -1
20220327 015824.557 INFO             PET5 index= 113            MPIR_CVAR_INTEL_SHM_RECV_INTER_HEAP_SHORT_MEMCPY : Name of memory copy function for short message receive via SHM HEAP transport, inter socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTER_HEAP_SHORT_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET5 index= 114            MPIR_CVAR_INTEL_SHM_RECV_INTRA_HEAP_SHORT_MEMCPY : Name of memory copy function for short message receive via SHM HEAP transport, intra socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTRA_HEAP_SHORT_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET5 index= 115          MPIR_CVAR_INTEL_SHM_RECV_INTER_HEAP_REGULAR_MEMCPY : Name of memory copy function for regular receive messages via SHM HEAP transport, inter socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTER_HEAP_REGULAR_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET5 index= 116          MPIR_CVAR_INTEL_SHM_RECV_INTRA_HEAP_REGULAR_MEMCPY : Name of memory copy function for regular receive messages via SHM HEAP transport, intra socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTRA_HEAP_REGULAR_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET5 index= 117                  MPIR_CVAR_INTEL_SHM_RECV_INTER_HEAP_MEMCPY : Name of memory copy function for receive messages via SHM HEAP transport, inter socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTER_HEAP_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET5 index= 118                  MPIR_CVAR_INTEL_SHM_RECV_INTRA_HEAP_MEMCPY : Name of memory copy function for receive messages via SHM HEAP transport, intra socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTRA_HEAP_MEMCPY @Default:# ""
20220327 015824.557 INFO             PET5 index= 119                               MPIR_CVAR_CH4_SHM_POSIX_EAGER : Select a shared memory transport to be used.$ Syntax$ I_MPI_SHM=<transport>$ Arguments$ <transport> - Define a shared memory transport solution.$ -----------------------------------------------------------------------$ > disable | no | off | 0 - Do not use shared memory transport.$ > auto - Select a shared memory transport solution automatically.$ > bdw_sse - The shared memory transport solution tuned for Intel(R)$ microarchitecture code name Broadwell. The SSE/SSE2/SSE3 instruction$ set is used.$ > bdw_avx2 - The shared memory transport solution tuned for Intel(R)$ microarchitecture code name Broadwell. The AVX2 instruction set is used.$ > skx_sse - The shared memory transport solution tuned for Intel(R)$ Xeon(R) processors based on Intel(R) microarchitecture code name Skylake.$ The SSE/SSE2/SSE3 instruction set is used.$ > skx_avx2 - The shared memory transport solution tuned for Intel(R)$ Xeon(R) processors based on Intel(R) microarchitecture code name Skylake.$ The AVX2 instruction set is used.$ > skx_av
20220327 015824.557 INFO             PET5 index= 120                                     MPIR_CVAR_INTEL_SHM_OPT : Select a shared memory transport optimization strategy to be used.$ Syntax$ I_MPI_SHM_OPT=<optimization_strategy>$ Arguments$ <optimization_strategy> - Define a shared memory transport optimization strategy.$ -----------------------------------------------------------------------$ > dynamic - Let shared memory transport make decision in runtime.$ > intra - Optimize intra socket message passing.$ > inter - Optimize inter socket message passing.$ -----------------------------------------------------------------------$ @Alias:# I_MPI_SHM_OPT @Default:# dynamic
20220327 015824.557 INFO             PET5 index= 121                           MPIR_CVAR_INTEL_SHM_CELL_FWD_SIZE : Change the size of a shared memory forward cell. @Alias:# I_MPI_SHM_CELL_FWD_SIZE @Default:# -1
20220327 015824.557 INFO             PET5 index= 122                           MPIR_CVAR_INTEL_SHM_CELL_BWD_SIZE : Change the size of a shared memory backward cell. @Alias:# I_MPI_SHM_CELL_BWD_SIZE @Default:# -1
20220327 015824.557 INFO             PET5 index= 123                           MPIR_CVAR_INTEL_SHM_CELL_EXT_SIZE : Change the size of a shared memory extended cell. @Alias:# I_MPI_SHM_CELL_EXT_SIZE @Default:# -1
20220327 015824.557 INFO             PET5 index= 124                            MPIR_CVAR_INTEL_SHM_CELL_FWD_NUM : Change the number of forward cells in the shared memory transport (per rank). @Alias:# I_MPI_SHM_CELL_FWD_NUM @Default:# -1
20220327 015824.557 INFO             PET5 index= 125                       MPIR_CVAR_INTEL_SHM_CELL_FWD_HOLD_NUM : Change the number of hold forward cells in the shared memory transport (per rank). @Alias:# I_MPI_SHM_CELL_FWD_HOLD_NUM @Default:# -1
20220327 015824.557 INFO             PET5 index= 126                            MPIR_CVAR_INTEL_SHM_CELL_BWD_NUM : Change the number of backward cells in the shared memory transport (per rank). @Alias:# I_MPI_SHM_CELL_BWD_NUM @Default:# -1
20220327 015824.557 INFO             PET5 index= 127                     MPIR_CVAR_INTEL_SHM_CELL_BWD_NUMA_AWARE : Use NUMA aware backward cells (1 : true, 0 : false, -1 : auto) (per rank). @Alias:# I_MPI_SHM_CELL_BWD_NUMA_AWARE @Default:# -1
20220327 015824.557 INFO             PET5 index= 128                      MPIR_CVAR_INTEL_SHM_CELL_EXT_NUM_TOTAL : Change the total number of extended cells in the shared memory$ transport. @Alias:# I_MPI_SHM_CELL_EXT_NUM_TOTAL @Default:# -1
20220327 015824.557 INFO             PET5 index= 129                            MPIR_CVAR_INTEL_SHM_MCDRAM_LIMIT : Change the size of the shared memory bound to the multi-channel DRAM (MCDRAM) (size per rank). @Alias:# I_MPI_SHM_MCDRAM_LIMIT @Default:# -1
20220327 015824.557 INFO             PET5 index= 130                         MPIR_CVAR_INTEL_SHM_SEND_SPIN_COUNT : Control the spin count value for the shared memory transport for sending messages. @Alias:# I_MPI_SHM_SEND_SPIN_COUNT @Default:# -1
20220327 015824.557 INFO             PET5 index= 131                         MPIR_CVAR_INTEL_SHM_RECV_SPIN_COUNT : Control the spin count value for the shared memory transport for receiving messages. @Alias:# I_MPI_SHM_RECV_SPIN_COUNT @Default:# -1
20220327 015824.557 INFO             PET5 index= 132                          MPIR_CVAR_INTEL_SHM_FILE_PREFIX_4K : Mount point of 4K page size shm files. @Alias:# I_MPI_SHM_FILE_PREFIX_4K @Default:# ""
20220327 015824.557 INFO             PET5 index= 133                          MPIR_CVAR_INTEL_SHM_FILE_PREFIX_2M : Mount point of 2M huge page size shm files. @Alias:# I_MPI_SHM_FILE_PREFIX_2M @Default:# ""
20220327 015824.557 INFO             PET5 index= 134                          MPIR_CVAR_INTEL_SHM_FILE_PREFIX_1G : Mount point of 1G huge page size shm files. @Alias:# I_MPI_SHM_FILE_PREFIX_1G @Default:# ""
20220327 015824.557 INFO             PET5 index= 135                                   MPIR_CVAR_INTEL_SHM_PAUSE : The number of pauses repeated. @Alias:# I_MPI_SHM_PAUSE
20220327 015824.557 INFO             PET5 index= 136                         MPIR_CVAR_INTEL_SHM_EAGER_THRESHOLD : Eager threshold. @Alias:# I_MPI_SHM_EAGER_THRESHOLD
20220327 015824.557 INFO             PET5 index= 137                               MPIR_CVAR_INTEL_SHM_RING_SIZE : @Alias:# I_MPI_SHM_RING_SIZE
20220327 015824.557 INFO             PET5 index= 138                      MPIR_CVAR_INTEL_SHM_RING_ACK_THRESHOLD : @Alias:# I_MPI_SHM_RING_ACK_THRESHOLD
20220327 015824.557 INFO             PET5 index= 139                          MPIR_CVAR_INTEL_SHM_CELL_FWD_FIRST : @Alias:# I_MPI_SHM_CELL_FWD_FIRST
20220327 015824.557 INFO             PET5 index= 140                      MPIR_CVAR_INTEL_SHM_PROFILER_DIRECTORY : @Alias:# I_MPI_SHM_PROFILER_DIRECTORY
20220327 015824.557 INFO             PET5 index= 141                         MPIR_CVAR_INTEL_SHM_TRACE_DIRECTORY : @Alias:# I_MPI_SHM_TRACE_DIRECTORY
20220327 015824.557 INFO             PET5 index= 142                              MPIR_CVAR_INTEL_SHM_FRAME_SIZE : @Alias:# I_MPI_SHM_FRAME_SIZE
20220327 015824.557 INFO             PET5 index= 143                         MPIR_CVAR_INTEL_SHM_FRAME_THRESHOLD : @Alias:# I_MPI_SHM_FRAME_THRESHOLD
20220327 015824.557 INFO             PET5 index= 144                        MPIR_CVAR_INTEL_SHM_SEND_TINY_MEMCPY : @Alias:# I_MPI_SHM_SEND_TINY_MEMCPY
20220327 015824.557 INFO             PET5 index= 145                  MPIR_CVAR_INTEL_SHM_SEND_INTRA_RING_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_RING_MEMCPY
20220327 015824.557 INFO             PET5 index= 146                  MPIR_CVAR_INTEL_SHM_SEND_INTER_RING_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_RING_MEMCPY
20220327 015824.557 INFO             PET5 index= 147                  MPIR_CVAR_INTEL_SHM_RECV_INTRA_RING_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_RING_MEMCPY
20220327 015824.557 INFO             PET5 index= 148                  MPIR_CVAR_INTEL_SHM_RECV_INTER_RING_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_RING_MEMCPY
20220327 015824.557 INFO             PET5 index= 149              MPIR_CVAR_INTEL_SHM_SEND_INTRA_CELL_FWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_CELL_FWD_MEMCPY
20220327 015824.557 INFO             PET5 index= 150              MPIR_CVAR_INTEL_SHM_SEND_INTER_CELL_FWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_CELL_FWD_MEMCPY
20220327 015824.557 INFO             PET5 index= 151              MPIR_CVAR_INTEL_SHM_SEND_INTRA_CELL_BWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_CELL_BWD_MEMCPY
20220327 015824.557 INFO             PET5 index= 152              MPIR_CVAR_INTEL_SHM_SEND_INTER_CELL_BWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_CELL_BWD_MEMCPY
20220327 015824.557 INFO             PET5 index= 153                  MPIR_CVAR_INTEL_SHM_RECV_INTRA_CELL_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_CELL_MEMCPY
20220327 015824.557 INFO             PET5 index= 154                  MPIR_CVAR_INTEL_SHM_RECV_INTER_CELL_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_CELL_MEMCPY
20220327 015824.557 INFO             PET5 index= 155          MPIR_CVAR_INTEL_SHM_RECV_INTRA_CELL_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_CELL_REGULAR_MEMCPY
20220327 015824.557 INFO             PET5 index= 156          MPIR_CVAR_INTEL_SHM_RECV_INTER_CELL_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_CELL_REGULAR_MEMCPY
20220327 015824.557 INFO             PET5 index= 157             MPIR_CVAR_INTEL_SHM_SEND_INTRA_FRAME_FWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_FRAME_FWD_MEMCPY
20220327 015824.557 INFO             PET5 index= 158             MPIR_CVAR_INTEL_SHM_SEND_INTER_FRAME_FWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_FRAME_FWD_MEMCPY
20220327 015824.557 INFO             PET5 index= 159             MPIR_CVAR_INTEL_SHM_SEND_INTRA_FRAME_BWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_FRAME_BWD_MEMCPY
20220327 015824.557 INFO             PET5 index= 160             MPIR_CVAR_INTEL_SHM_SEND_INTER_FRAME_BWD_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_FRAME_BWD_MEMCPY
20220327 015824.557 INFO             PET5 index= 161                 MPIR_CVAR_INTEL_SHM_RECV_INTRA_FRAME_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_FRAME_MEMCPY
20220327 015824.557 INFO             PET5 index= 162                 MPIR_CVAR_INTEL_SHM_RECV_INTER_FRAME_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_FRAME_MEMCPY
20220327 015824.557 INFO             PET5 index= 163     MPIR_CVAR_INTEL_SHM_SEND_INTRA_FRAME_FWD_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_FRAME_FWD_REGULAR_MEMCPY
20220327 015824.557 INFO             PET5 index= 164     MPIR_CVAR_INTEL_SHM_SEND_INTER_FRAME_FWD_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_FRAME_FWD_REGULAR_MEMCPY
20220327 015824.557 INFO             PET5 index= 165     MPIR_CVAR_INTEL_SHM_SEND_INTRA_FRAME_BWD_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTRA_FRAME_BWD_REGULAR_MEMCPY
20220327 015824.557 INFO             PET5 index= 166     MPIR_CVAR_INTEL_SHM_SEND_INTER_FRAME_BWD_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_SEND_INTER_FRAME_BWD_REGULAR_MEMCPY
20220327 015824.557 INFO             PET5 index= 167         MPIR_CVAR_INTEL_SHM_RECV_INTRA_FRAME_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTRA_FRAME_REGULAR_MEMCPY
20220327 015824.557 INFO             PET5 index= 168         MPIR_CVAR_INTEL_SHM_RECV_INTER_FRAME_REGULAR_MEMCPY : @Alias:# I_MPI_SHM_RECV_INTER_FRAME_REGULAR_MEMCPY
20220327 015824.557 INFO             PET5 index= 169                       MPIR_CVAR_INTEL_SHM_INPLACE_THRESHOLD : @Alias:# I_MPI_SHM_INPLACE_THRESHOLD
20220327 015824.557 INFO             PET5 index= 170              MPIR_CVAR_INTEL_SHM_SEND_TINY_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_TINY_MEMCPY_THRESHOLD
20220327 015824.557 INFO             PET5 index= 171      MPIR_CVAR_INTEL_SHM_RECV_CELL_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_RECV_CELL_REGULAR_MEMCPY_THRESHOLD
20220327 015824.557 INFO             PET5 index= 172MPIR_CVAR_INTEL_SHM_SEND_INTER_UNIDIR_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_INTER_UNIDIR_REGULAR_MEMCPY_THRESHOLD
20220327 015824.557 INFO             PET5 index= 173MPIR_CVAR_INTEL_SHM_SEND_INTER_BIDIR_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_INTER_BIDIR_REGULAR_MEMCPY_THRESHOLD
20220327 015824.557 INFO             PET5 index= 174MPIR_CVAR_INTEL_SHM_SEND_INTRA_UNIDIR_REGULAR_MEMCPY_THRESHOLD_MIN : @Alias:# I_MPI_SHM_SEND_INTRA_UNIDIR_REGULAR_MEMCPY_THRESHOLD_MIN
20220327 015824.557 INFO             PET5 index= 175MPIR_CVAR_INTEL_SHM_SEND_INTRA_UNIDIR_REGULAR_MEMCPY_THRESHOLD_MAX : @Alias:# I_MPI_SHM_SEND_INTRA_UNIDIR_REGULAR_MEMCPY_THRESHOLD_MAX
20220327 015824.557 INFO             PET5 index= 176MPIR_CVAR_INTEL_SHM_SEND_INTRA_BIDIR_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_INTRA_BIDIR_REGULAR_MEMCPY_THRESHOLD
20220327 015824.557 INFO             PET5 index= 177     MPIR_CVAR_INTEL_SHM_RECV_FRAME_REGULAR_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_RECV_FRAME_REGULAR_MEMCPY_THRESHOLD
20220327 015824.557 INFO             PET5 index= 178          MPIR_CVAR_INTEL_SHM_CELL_FWD_UNIDIR_INTRA_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_UNIDIR_INTRA_CAPACITY
20220327 015824.558 INFO             PET5 index= 179  MPIR_CVAR_INTEL_SHM_CELL_FWD_UNIDIR_INTER_REGULAR_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_UNIDIR_INTER_REGULAR_CAPACITY
20220327 015824.558 INFO             PET5 index= 180MPIR_CVAR_INTEL_SHM_CELL_FWD_UNIDIR_INTER_NONTEMPORAL_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_UNIDIR_INTER_NONTEMPORAL_CAPACITY
20220327 015824.558 INFO             PET5 index= 181           MPIR_CVAR_INTEL_SHM_CELL_FWD_BIDIR_INTRA_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_BIDIR_INTRA_CAPACITY
20220327 015824.558 INFO             PET5 index= 182   MPIR_CVAR_INTEL_SHM_CELL_FWD_BIDIR_INTER_REGULAR_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_BIDIR_INTER_REGULAR_CAPACITY
20220327 015824.558 INFO             PET5 index= 183MPIR_CVAR_INTEL_SHM_CELL_FWD_BIDIR_INTER_NONTEMPORAL_CAPACITY : @Alias:# I_MPI_SHM_CELL_FWD_BIDIR_INTER_NONTEMPORAL_CAPACITY
20220327 015824.558 INFO             PET5 index= 184MPIR_CVAR_INTEL_SHM_SEND_FWD_FRAME_NONTEMPORAL_MEMCPY_THRESHOLD_MIN : @Alias:# I_MPI_SHM_SEND_FWD_FRAME_NONTEMPORAL_MEMCPY_THRESHOLD_MIN
20220327 015824.558 INFO             PET5 index= 185MPIR_CVAR_INTEL_SHM_SEND_FWD_FRAME_NONTEMPORAL_MEMCPY_THRESHOLD_MAX : @Alias:# I_MPI_SHM_SEND_FWD_FRAME_NONTEMPORAL_MEMCPY_THRESHOLD_MAX
20220327 015824.558 INFO             PET5 index= 186MPIR_CVAR_INTEL_SHM_SEND_FWD_CELL_NONTEMPORAL_MEMCPY_THRESHOLD : @Alias:# I_MPI_SHM_SEND_FWD_CELL_NONTEMPORAL_MEMCPY_THRESHOLD
20220327 015824.558 INFO             PET5 index= 187                                MPIR_CVAR_INTEL_SHM_HEAP_THP : @Alias:# I_MPI_SHM_HEAP_THP
20220327 015824.558 INFO             PET5 index= 188                                     MPIR_CVAR_INTEL_SHM_THP : @Alias:# I_MPI_SHM_THP
20220327 015824.558 INFO             PET5 index= 189                                  MPIR_CVAR_OFI_USE_PROVIDER : Define the name of the OFI provider to load.$ Syntax$ I_MPI_OFI_PROVIDER=<name>$ Arguments$ <name> - The name of the OFI provider to load @Alias:# I_MPI_OFI_PROVIDER @Default:# not defined
20220327 015824.558 INFO             PET5 index= 190                                MPIR_CVAR_OFI_DUMP_PROVIDERS : Control the capability of printing information about all OFI providers$ and their attributes from an OFI library.$ Syntax$ I_MPI_OFI_PROVIDER_DUMP=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > yes | on | 1 - Print the list of all OFI providers and their$ attributes from an OFI library$ > no | off | 0 - No action$ ----------------------------------------------------------------------- @Alias:# I_MPI_OFI_PROVIDER_DUMP @Default:# off
20220327 015824.558 INFO             PET5 index= 191                        MPIR_CVAR_CH4_OFI_ENABLE_DIRECT_RECV : Control the capability of the direct receive in the OFI fabric.$ Syntax$ I_MPI_OFI_DRECV=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > 1 - Enable direct receive$ > 0 - Disable direct receive$ ----------------------------------------------------------------------- @Alias:# I_MPI_OFI_DRECV @Default:# not defined
20220327 015824.558 INFO             PET5 index= 192                                  MPIR_CVAR_OFI_MAX_MSG_SIZE : @Alias:# I_MPI_OFI_MAX_MSG_SIZE
20220327 015824.558 INFO             PET5 index= 193                                  MPIR_CVAR_OFI_LMT_WIN_SIZE : @Alias:# I_MPI_OFI_LMT_WIN_SIZE
20220327 015824.558 INFO             PET5 index= 194                    MPIR_CVAR_CH4_OFI_ISEND_INJECT_THRESHOLD : @Alias:# I_MPI_OFI_ISEND_INJECT_THRESHOLD
20220327 015824.558 INFO             PET5 index= 195                     MPIR_CVAR_CH4_OFI_ADDRESS_EXCHANGE_MODE : @Alias:# I_MPI_STARTUP_MODE
20220327 015824.558 INFO             PET5 index= 196                     MPIR_CVAR_CH4_OFI_LARGE_SCALE_THRESHOLD : @Alias:# I_MPI_LARGE_SCALE_THRESHOLD
20220327 015824.558 INFO             PET5 index= 197                   MPIR_CVAR_CH4_OFI_EXTREME_SCALE_THRESHOLD : @Alias:# I_MPI_EXTREME_SCALE_THRESHOLD
20220327 015824.558 INFO             PET5 index= 198                        MPIR_CVAR_CH4_OFI_DYNAMIC_CONNECTION : @Alias:# I_MPI_DYNAMIC_CONNECTION
20220327 015824.558 INFO             PET5 index= 199                              MPIR_CVAR_CH4_OFI_EXPERIMENTAL : @Alias:# I_MPI_OFI_EXPERIMENTAL @Default:# 0
20220327 015824.558 INFO             PET5 index= 200                     MPIR_CVAR_CH4_OFI_CAPABILITY_SETS_DEBUG : Prints out the configuration of each capability selected via the capability sets interface.
20220327 015824.558 INFO             PET5 index= 201                               MPIR_CVAR_CH4_OFI_ENABLE_DATA : Enable immediate data fields in OFI to transmit source rank outside of the match bits
20220327 015824.558 INFO             PET5 index= 202                           MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE : If true, the OFI addressing information will be stored with an FI_AV_TABLE. If false, an FI_AV_MAP will be used.
20220327 015824.558 INFO             PET5 index= 203                 MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS : If true, use OFI scalable endpoints.
20220327 015824.558 INFO             PET5 index= 204                             MPIR_CVAR_CH4_OFI_MAX_ENDPOINTS : Specifies the maximum number of OFI endpoints that can be used by the OFI provider. The default value is -1, indicating that no value is set.
20220327 015824.558 INFO             PET5 index= 205                        MPIR_CVAR_CH4_OFI_ENABLE_MR_SCALABLE : If true, MR_SCALABLE for OFI memory regions. If false, MR_BASIC for OFI memory regions.
20220327 015824.558 INFO             PET5 index= 206                             MPIR_CVAR_CH4_OFI_ENABLE_TAGGED : If true, use tagged message transmission functions in OFI.
20220327 015824.558 INFO             PET5 index= 207                                 MPIR_CVAR_CH4_OFI_ENABLE_AM : If true, enable OFI active message support.
20220327 015824.558 INFO             PET5 index= 208                                MPIR_CVAR_CH4_OFI_ENABLE_RMA : If true, enable OFI RMA support for MPI RMA operations. OFI support for basic RMA is always required to implement large messgage transfers in the active message code path.
20220327 015824.558 INFO             PET5 index= 209                            MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS : If true, enable OFI Atomics support.
20220327 015824.558 INFO             PET5 index= 210                       MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS : Specifies the maximum number of iovecs that can be used by the OFI provider for fetch_atomic operations. The default value is -1, indicating that no value is set.
20220327 015824.558 INFO             PET5 index= 211                 MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS : If true, enable MPI data auto progress.
20220327 015824.558 INFO             PET5 index= 212              MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS : If true, enable MPI control auto progress.
20220327 015824.558 INFO             PET5 index= 213                       MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK : If true, enable iovec for pt2pt.
20220327 015824.558 INFO             PET5 index= 214                                 MPIR_CVAR_CH4_OFI_RANK_BITS : Specifies the number of bits that will be used for matching the MPI rank. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20220327 015824.558 INFO             PET5 index= 215                                  MPIR_CVAR_CH4_OFI_TAG_BITS : Specifies the number of bits that will be used for matching the user tag. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20220327 015824.558 INFO             PET5 index= 216                             MPIR_CVAR_CH4_OFI_MAJOR_VERSION : Specifies the major version of the OFI library. The default is the major version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20220327 015824.558 INFO             PET5 index= 217                             MPIR_CVAR_CH4_OFI_MINOR_VERSION : Specifies the major version of the OFI library. The default is the minor version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20220327 015824.558 INFO             PET5 index= 218                             MPIR_CVAR_CH4_OFI_ZERO_OP_FLAGS : Zeroes rx_attr.op_flags and tx_attr.op_flags, disables use of FI_SELECTIVE_COMPLETION. Can give more optimized behavior of underlying provider.
20220327 015824.558 INFO             PET5 index= 219                            MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS : Specifies the number of buffers for receiving active messages.
20220327 015824.558 INFO             PET5 index= 220                        MPIR_CVAR_INTEL_COLL_DIRECT_PROGRESS : @Alias:# I_MPI_COLL_DIRECT_PROGRESS
20220327 015824.558 INFO             PET5 index= 221                                      MPIR_CVAR_ENABLE_HCOLL : @Alias:# I_MPI_COLL_EXTERNAL
20220327 015824.558 INFO             PET5 index= 222                                     MPIR_CVAR_USE_ALLGATHER : Control selection of MPI_Allgather algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-6 @Alias:# I_MPI_ADJUST_ALLGATHER @Default:# -1
20220327 015824.558 INFO             PET5 index= 223                                MPIR_CVAR_USE_ALLGATHER_LIST : @Alias:# I_MPI_ADJUST_ALLGATHER_LIST @Default:# -1
20220327 015824.558 INFO             PET5 index= 224                         MPIR_CVAR_USE_ALLGATHER_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLGATHER_COMPOSITION @Default:# -1
20220327 015824.558 INFO             PET5 index= 225               MPIR_CVAR_ALLGATHER_COMPOSITION_DELTA_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLGATHER_COMPOSITION_DELTA_SEGSIZE @Default:# -1
20220327 015824.558 INFO             PET5 index= 226                             MPIR_CVAR_USE_ALLGATHER_NETWORK : range: 0-5 @Alias:# I_MPI_ADJUST_ALLGATHER_NETWORK @Default:# -1
20220327 015824.558 INFO             PET5 index= 227                                MPIR_CVAR_USE_ALLGATHER_NODE : range: 0-4 @Alias:# I_MPI_ADJUST_ALLGATHER_NODE @Default:# -1
20220327 015824.558 INFO             PET5 index= 228                                    MPIR_CVAR_USE_ALLGATHERV : Control selection of MPI_Allgatherv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_ALLGATHERV @Default:# -1
20220327 015824.558 INFO             PET5 index= 229                               MPIR_CVAR_USE_ALLGATHERV_LIST : @Alias:# I_MPI_ADJUST_ALLGATHERV_LIST @Default:# -1
20220327 015824.558 INFO             PET5 index= 230                        MPIR_CVAR_USE_ALLGATHERV_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLGATHERV_COMPOSITION @Default:# -1
20220327 015824.558 INFO             PET5 index= 231                            MPIR_CVAR_USE_ALLGATHERV_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_ALLGATHERV_NETWORK @Default:# -1
20220327 015824.558 INFO             PET5 index= 232                               MPIR_CVAR_USE_ALLGATHERV_NODE : range: 0-3 @Alias:# I_MPI_ADJUST_ALLGATHERV_NODE @Default:# -1
20220327 015824.558 INFO             PET5 index= 233                   MPIR_CVAR_ALLGATHER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ALLGATHER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET5 index= 234                      MPIR_CVAR_ALLGATHER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ALLGATHER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET5 index= 235                    MPIR_CVAR_SCATTERV_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_SCATTERV_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET5 index= 236                       MPIR_CVAR_SCATTERV_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_SCATTERV_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET5 index= 237                     MPIR_CVAR_SCATTER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_SCATTER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET5 index= 238                        MPIR_CVAR_SCATTER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_SCATTER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET5 index= 239                                     MPIR_CVAR_USE_ALLREDUCE : Control selection of MPI_Allreduce algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-25 @Alias:# I_MPI_ADJUST_ALLREDUCE @Default:# -1
20220327 015824.558 INFO             PET5 index= 240                                MPIR_CVAR_USE_ALLREDUCE_LIST : @Alias:# I_MPI_ADJUST_ALLREDUCE_LIST @Default:# -1
20220327 015824.558 INFO             PET5 index= 241                         MPIR_CVAR_USE_ALLREDUCE_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLREDUCE_COMPOSITION @Default:# -1
20220327 015824.558 INFO             PET5 index= 242                             MPIR_CVAR_USE_ALLREDUCE_NETWORK : range: 0-16 @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK @Default:# -1
20220327 015824.558 INFO             PET5 index= 243                                MPIR_CVAR_USE_ALLREDUCE_NODE : range: 0-9 @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE @Default:# -1
20220327 015824.558 INFO             PET5 index= 244               MPIR_CVAR_ALLREDUCE_NETWORK_MULTIPLYING_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_MULTIPLYING_RADIX @Default:# -1
20220327 015824.558 INFO             PET5 index= 245                  MPIR_CVAR_ALLREDUCE_NODE_MULTIPLYING_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_MULTIPLYING_RADIX @Default:# -1
20220327 015824.558 INFO             PET5 index= 246                   MPIR_CVAR_ALLREDUCE_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET5 index= 247                      MPIR_CVAR_ALLREDUCE_NETWORK_KARY_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_KARY_RADIX @Default:# -1
20220327 015824.558 INFO             PET5 index= 248                      MPIR_CVAR_ALLREDUCE_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.558 INFO             PET5 index= 249                         MPIR_CVAR_ALLREDUCE_NODE_KARY_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_KARY_RADIX @Default:# -1
20220327 015824.558 INFO             PET5 index= 250                    MPIR_CVAR_ALLREDUCE_NODE_SHM_GATHER_TYPE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHM_GATHER_TYPE @Default:# -1
20220327 015824.558 INFO             PET5 index= 251                   MPIR_CVAR_ALLREDUCE_NODE_SHM_GATHER_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHM_GATHER_RADIX @Default:# -1
20220327 015824.558 INFO             PET5 index= 252                   MPIR_CVAR_ALLREDUCE_NODE_SHM_RELEASE_TYPE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHM_RELEASE_TYPE @Default:# -1
20220327 015824.558 INFO             PET5 index= 253                  MPIR_CVAR_ALLREDUCE_NODE_SHM_RELEASE_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHM_RELEASE_RADIX @Default:# -1
20220327 015824.558 INFO             PET5 index= 254                MPIR_CVAR_ALLREDUCE_NETWORK_SHUMILIN_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_SHUMILIN_SEGSIZE @Default:# -1
20220327 015824.558 INFO             PET5 index= 255                   MPIR_CVAR_ALLREDUCE_NODE_SHUMILIN_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHUMILIN_SEGSIZE @Default:# -1
20220327 015824.558 INFO             PET5 index= 256                              MPIR_CVAR_ALLREDUCE_ZETA_RADIX : @Alias:# I_MPI_ADJUST_ALLREDUCE_ZETA_RADIX @Default:# -1
20220327 015824.558 INFO             PET5 index= 257                           MPIR_CVAR_ALLREDUCE_ZETA_SHM_TYPE : @Alias:# I_MPI_ADJUST_ALLREDUCE_ZETA_SHM_TYPE @Default:# -1
20220327 015824.558 INFO             PET5 index= 258                              MPIR_CVAR_ALLREDUCE_IOTA_NLEAD : @Alias:# I_MPI_ADJUST_ALLREDUCE_IOTA_NLEAD @Default:# -1
20220327 015824.558 INFO             PET5 index= 259               MPIR_CVAR_ALLREDUCE_NODE_NREDUCE_NB_ALLGATHER : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_NREDUCE_NB_ALLGATHER @Default:# -1
20220327 015824.558 INFO             PET5 index= 260            MPIR_CVAR_ALLREDUCE_NETWORK_NREDUCE_NB_ALLGATHER : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_NREDUCE_NB_ALLGATHER @Default:# -1
20220327 015824.558 INFO             PET5 index= 261                MPIR_CVAR_ALLREDUCE_NODE_NREDUCE_NB_ALLTOALL : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_NREDUCE_NB_ALLTOALL @Default:# -1
20220327 015824.558 INFO             PET5 index= 262             MPIR_CVAR_ALLREDUCE_NETWORK_NREDUCE_NB_ALLTOALL : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_NREDUCE_NB_ALLTOALL @Default:# -1
20220327 015824.558 INFO             PET5 index= 263                    MPIR_CVAR_ALLREDUCE_NODE_NREDUCE_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_NREDUCE_SEGSIZE @Default:# -1
20220327 015824.558 INFO             PET5 index= 264                 MPIR_CVAR_ALLREDUCE_NETWORK_NREDUCE_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_NREDUCE_SEGSIZE @Default:# -1
20220327 015824.558 INFO             PET5 index= 265                                      MPIR_CVAR_USE_ALLTOALL : Control selection of MPI_Alltoall algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-13 @Alias:# I_MPI_ADJUST_ALLTOALL @Default:# -1
20220327 015824.558 INFO             PET5 index= 266                                 MPIR_CVAR_USE_ALLTOALL_LIST : @Alias:# I_MPI_ADJUST_ALLTOALL_LIST @Default:# -1
20220327 015824.558 INFO             PET5 index= 267                          MPIR_CVAR_USE_ALLTOALL_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLTOALL_COMPOSITION @Default:# -1
20220327 015824.558 INFO             PET5 index= 268                              MPIR_CVAR_USE_ALLTOALL_NETWORK : range: 0-7 @Alias:# I_MPI_ADJUST_ALLTOALL_NETWORK @Default:# -1
20220327 015824.558 INFO             PET5 index= 269                                 MPIR_CVAR_USE_ALLTOALL_NODE : range: 0-6 @Alias:# I_MPI_ADJUST_ALLTOALL_NODE @Default:# -1
20220327 015824.558 INFO             PET5 index= 270                MPIR_CVAR_ALLTOALL_COMPOSITION_GAMMA_SEGSIZE : @Alias:# I_MPI_ADJUST_ALLTOALL_COMPOSITION_GAMMA_SEGSIZE @Default:# -1
20220327 015824.558 INFO             PET5 index= 271                                 MPIR_CVAR_ALLTOALL_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALL_SCATTERED_THROTTLE @Default:# -1
20220327 015824.558 INFO             PET5 index= 272               MPIR_CVAR_ALLTOALL_NETWORK_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALL_NETWORK_SCATTERED_THROTTLE @Default:# -1
20220327 015824.558 INFO             PET5 index= 273                  MPIR_CVAR_ALLTOALL_NODE_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALL_NODE_SCATTERED_THROTTLE @Default:# -1
20220327 015824.558 INFO             PET5 index= 274                             MPIR_CVAR_ALLTOALL_BRUCKS_RADIX : @Alias:# I_MPI_ADJUST_ALLTOALL_BRUCKS_RADIX @Default:# -1
20220327 015824.558 INFO             PET5 index= 275                     MPIR_CVAR_ALLTOALL_NETWORK_BRUCKS_RADIX : @Alias:# I_MPI_ADJUST_ALLTOALL_NETWORK_BRUCKS_RADIX @Default:# -1
20220327 015824.558 INFO             PET5 index= 276                        MPIR_CVAR_ALLTOALL_NODE_BRUCKS_RADIX : @Alias:# I_MPI_ADJUST_ALLTOALL_NODE_BRUCKS_RADIX @Default:# -1
20220327 015824.558 INFO             PET5 index= 277                                     MPIR_CVAR_USE_ALLTOALLV : Control selection of MPI_Alltoallv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-9 @Alias:# I_MPI_ADJUST_ALLTOALLV @Default:# -1
20220327 015824.558 INFO             PET5 index= 278                                MPIR_CVAR_USE_ALLTOALLV_LIST : @Alias:# I_MPI_ADJUST_ALLTOALLV_LIST @Default:# -1
20220327 015824.558 INFO             PET5 index= 279                         MPIR_CVAR_USE_ALLTOALLV_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLTOALLV_COMPOSITION @Default:# -1
20220327 015824.558 INFO             PET5 index= 280                             MPIR_CVAR_USE_ALLTOALLV_NETWORK : range: 0-6 @Alias:# I_MPI_ADJUST_ALLTOALLV_NETWORK @Default:# -1
20220327 015824.558 INFO             PET5 index= 281                                MPIR_CVAR_USE_ALLTOALLV_NODE : range: 0-5 @Alias:# I_MPI_ADJUST_ALLTOALLV_NODE @Default:# -1
20220327 015824.558 INFO             PET5 index= 282                                MPIR_CVAR_ALLTOALLV_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLV_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET5 index= 283              MPIR_CVAR_ALLTOALLV_NETWORK_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLV_NETWORK_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET5 index= 284                 MPIR_CVAR_ALLTOALLV_NODE_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLV_NODE_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET5 index= 285                                     MPIR_CVAR_USE_ALLTOALLW : Control selection of MPI_Alltoallw algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-6 @Alias:# I_MPI_ADJUST_ALLTOALLW @Default:# -1
20220327 015824.559 INFO             PET5 index= 286                                MPIR_CVAR_USE_ALLTOALLW_LIST : @Alias:# I_MPI_ADJUST_ALLTOALLW_LIST @Default:# -1
20220327 015824.559 INFO             PET5 index= 287                         MPIR_CVAR_USE_ALLTOALLW_COMPOSITION : @Alias:# I_MPI_ADJUST_ALLTOALLW_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET5 index= 288                             MPIR_CVAR_USE_ALLTOALLW_NETWORK : @Alias:# I_MPI_ADJUST_ALLTOALLW_NETWORK @Default:# -1
20220327 015824.559 INFO             PET5 index= 289                                MPIR_CVAR_USE_ALLTOALLW_NODE : @Alias:# I_MPI_ADJUST_ALLTOALLW_NODE @Default:# -1
20220327 015824.559 INFO             PET5 index= 290                                MPIR_CVAR_ALLTOALLW_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLW_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET5 index= 291              MPIR_CVAR_ALLTOALLW_NETWORK_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLW_NETWORK_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET5 index= 292                 MPIR_CVAR_ALLTOALLW_NODE_SCATTERED_THROTTLE : @Alias:# I_MPI_ADJUST_ALLTOALLW_NODE_SCATTERED_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET5 index= 293                                       MPIR_CVAR_USE_BARRIER : Control selection of MPI_Barrier algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-11 @Alias:# I_MPI_ADJUST_BARRIER @Default:# -1
20220327 015824.559 INFO             PET5 index= 294                                  MPIR_CVAR_USE_BARRIER_LIST : @Alias:# I_MPI_ADJUST_BARRIER_LIST @Default:# -1
20220327 015824.559 INFO             PET5 index= 295                           MPIR_CVAR_USE_BARRIER_COMPOSITION : @Alias:# I_MPI_ADJUST_BARRIER_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET5 index= 296                               MPIR_CVAR_USE_BARRIER_NETWORK : range: 0-6 @Alias:# I_MPI_ADJUST_BARRIER_NETWORK @Default:# -1
20220327 015824.559 INFO             PET5 index= 297                                  MPIR_CVAR_USE_BARRIER_NODE : range: 0-4 @Alias:# I_MPI_ADJUST_BARRIER_NODE @Default:# -1
20220327 015824.559 INFO             PET5 index= 298                 MPIR_CVAR_BARRIER_NETWORK_MULTIPLYING_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NETWORK_MULTIPLYING_RADIX @Default:# -1
20220327 015824.559 INFO             PET5 index= 299                     MPIR_CVAR_BARRIER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET5 index= 300                        MPIR_CVAR_BARRIER_NETWORK_KARY_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NETWORK_KARY_RADIX @Default:# -1
20220327 015824.559 INFO             PET5 index= 301          MPIR_CVAR_BARRIER_NETWORK_RECURSIVE_EXCHANGE_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NETWORK_RECURSIVE_EXCHANGE_RADIX @Default:# -1
20220327 015824.559 INFO             PET5 index= 302                        MPIR_CVAR_BARRIER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET5 index= 303                           MPIR_CVAR_BARRIER_NODE_KARY_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_KARY_RADIX @Default:# -1
20220327 015824.559 INFO             PET5 index= 304             MPIR_CVAR_BARRIER_NODE_RECURSIVE_EXCHANGE_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_RECURSIVE_EXCHANGE_RADIX @Default:# -1
20220327 015824.559 INFO             PET5 index= 305                      MPIR_CVAR_BARRIER_NODE_SHM_GATHER_TYPE : @Alias:# I_MPI_ADJUST_BARRIER_NODE_SHM_GATHER_TYPE @Default:# -1
20220327 015824.559 INFO             PET5 index= 306                     MPIR_CVAR_BARRIER_NODE_SHM_GATHER_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_SHM_GATHER_RADIX @Default:# -1
20220327 015824.559 INFO             PET5 index= 307                     MPIR_CVAR_BARRIER_NODE_SHM_RELEASE_TYPE : @Alias:# I_MPI_ADJUST_BARRIER_NODE_SHM_RELEASE_TYPE @Default:# -1
20220327 015824.559 INFO             PET5 index= 308                    MPIR_CVAR_BARRIER_NODE_SHM_RELEASE_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_NODE_SHM_RELEASE_RADIX @Default:# -1
20220327 015824.559 INFO             PET5 index= 309                                MPIR_CVAR_BARRIER_ZETA_RADIX : @Alias:# I_MPI_ADJUST_BARRIER_ZETA_RADIX @Default:# -1
20220327 015824.559 INFO             PET5 index= 310                             MPIR_CVAR_BARRIER_ZETA_SHM_TYPE : @Alias:# I_MPI_ADJUST_BARRIER_ZETA_SHM_TYPE @Default:# -1
20220327 015824.559 INFO             PET5 index= 311                                         MPIR_CVAR_USE_BCAST : Control selection of MPI_Bcast algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-18 @Alias:# I_MPI_ADJUST_BCAST @Default:# -1
20220327 015824.559 INFO             PET5 index= 312                                    MPIR_CVAR_USE_BCAST_LIST : @Alias:# I_MPI_ADJUST_BCAST_LIST @Default:# -1
20220327 015824.559 INFO             PET5 index= 313                             MPIR_CVAR_USE_BCAST_COMPOSITION : @Alias:# I_MPI_ADJUST_BCAST_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET5 index= 314                                 MPIR_CVAR_USE_BCAST_NETWORK : range: 0-13 @Alias:# I_MPI_ADJUST_BCAST_NETWORK @Default:# -1
20220327 015824.559 INFO             PET5 index= 315                                    MPIR_CVAR_USE_BCAST_NODE : range: 0-9 @Alias:# I_MPI_ADJUST_BCAST_NODE @Default:# -1
20220327 015824.559 INFO             PET5 index= 316                               MPIR_CVAR_BCAST_EPSILON_NRAIL : @Alias:# I_MPI_ADJUST_BCAST_EPSILON_NRAIL @Default:# -1
20220327 015824.559 INFO             PET5 index= 317                          MPIR_CVAR_BCAST_NETWORK_KARY_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_KARY_RADIX @Default:# -1
20220327 015824.559 INFO             PET5 index= 318                       MPIR_CVAR_BCAST_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET5 index= 319                        MPIR_CVAR_BCAST_NETWORK_KARY_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_KARY_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET5 index= 320                     MPIR_CVAR_BCAST_NETWORK_KNOMIAL_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_KNOMIAL_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET5 index= 321                             MPIR_CVAR_BCAST_NODE_KARY_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NODE_KARY_RADIX @Default:# -1
20220327 015824.559 INFO             PET5 index= 322                          MPIR_CVAR_BCAST_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET5 index= 323                           MPIR_CVAR_BCAST_NODE_KARY_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_KARY_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET5 index= 324                        MPIR_CVAR_BCAST_NODE_KNOMIAL_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_KNOMIAL_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET5 index= 325                          MPIR_CVAR_BCAST_NETWORK_TREE_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_TREE_RADIX @Default:# -1
20220327 015824.559 INFO             PET5 index= 326                        MPIR_CVAR_BCAST_NETWORK_TREE_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_TREE_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET5 index= 327                           MPIR_CVAR_BCAST_NETWORK_TREE_TYPE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_TREE_TYPE @Default:# -1
20220327 015824.559 INFO             PET5 index= 328                       MPIR_CVAR_BCAST_NETWORK_TREE_THROTTLE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_TREE_THROTTLE @Default:# -1
20220327 015824.559 INFO             PET5 index= 329                       MPIR_CVAR_BCAST_NODE_SHUMILIN_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHUMILIN_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET5 index= 330                        MPIR_CVAR_BCAST_NODE_SHM_GATHER_TYPE : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHM_GATHER_TYPE @Default:# -1
20220327 015824.559 INFO             PET5 index= 331                       MPIR_CVAR_BCAST_NODE_SHM_GATHER_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHM_GATHER_RADIX @Default:# -1
20220327 015824.559 INFO             PET5 index= 332                       MPIR_CVAR_BCAST_NODE_SHM_RELEASE_TYPE : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHM_RELEASE_TYPE @Default:# -1
20220327 015824.559 INFO             PET5 index= 333                      MPIR_CVAR_BCAST_NODE_SHM_RELEASE_RADIX : @Alias:# I_MPI_ADJUST_BCAST_NODE_SHM_RELEASE_RADIX @Default:# -1
20220327 015824.559 INFO             PET5 index= 334                    MPIR_CVAR_BCAST_NETWORK_SHUMILIN_SEGSIZE : @Alias:# I_MPI_ADJUST_BCAST_NETWORK_SHUMILIN_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET5 index= 335                 MPIR_CVAR_BCAST_NODE_NUMA_AWARE_MEMCPY_ARCH : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_MEMCPY_ARCH @Default:# -1
20220327 015824.559 INFO             PET5 index= 336                   MPIR_CVAR_BCAST_NODE_NUMA_AWARE_FRAME_NUM : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_FRAME_NUM
20220327 015824.559 INFO             PET5 index= 337                  MPIR_CVAR_BCAST_NODE_NUMA_AWARE_FRAME_SIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_FRAME_SIZE
20220327 015824.559 INFO             PET5 index= 338  MPIR_CVAR_BCAST_NODE_NUMA_AWARE_RECV_NONTEMPORAL_THRESHOLD : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_NONTEMPORAL_THRESHOLD
20220327 015824.559 INFO             PET5 index= 339      MPIR_CVAR_BCAST_NODE_NUMA_AWARE_FRAME_NONTEMPORAL_SIZE : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_FRAME_NONTEMPORAL_SIZE
20220327 015824.559 INFO             PET5 index= 340 MPIR_CVAR_BCAST_NODE_NUMA_AWARE_TINY_MESSAGE_SIZE_THRESHOLD : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_TINY_MESSAGE_SIZE_THRESHOLD
20220327 015824.559 INFO             PET5 index= 341MPIR_CVAR_BCAST_NODE_NUMA_AWARE_SHM_HEAP_MESSAGE_SIZE_THRESHOLD : @Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_SHM_HEAP_MESSAGE_SIZE_THRESHOLD
20220327 015824.559 INFO             PET5 index= 342                                        MPIR_CVAR_USE_EXSCAN : Control selection of MPI_Exscan algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_EXSCAN @Default:# -1
20220327 015824.559 INFO             PET5 index= 343                                   MPIR_CVAR_USE_EXSCAN_LIST : @Alias:# I_MPI_ADJUST_EXSCAN_LIST @Default:# -1
20220327 015824.559 INFO             PET5 index= 344                            MPIR_CVAR_USE_EXSCAN_COMPOSITION : @Alias:# I_MPI_ADJUST_EXSCAN_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET5 index= 345                                MPIR_CVAR_USE_EXSCAN_NETWORK : @Alias:# I_MPI_ADJUST_EXSCAN_NETWORK @Default:# -1
20220327 015824.559 INFO             PET5 index= 346                                   MPIR_CVAR_USE_EXSCAN_NODE : @Alias:# I_MPI_ADJUST_EXSCAN_NODE @Default:# -1
20220327 015824.559 INFO             PET5 index= 347                                        MPIR_CVAR_USE_GATHER : Control selection of MPI_Gather algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_GATHER @Default:# -1
20220327 015824.559 INFO             PET5 index= 348                                   MPIR_CVAR_USE_GATHER_LIST : @Alias:# I_MPI_ADJUST_GATHER_LIST @Default:# -1
20220327 015824.559 INFO             PET5 index= 349                            MPIR_CVAR_USE_GATHER_COMPOSITION : @Alias:# I_MPI_ADJUST_GATHER_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET5 index= 350                                MPIR_CVAR_USE_GATHER_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_GATHER_NETWORK @Default:# -1
20220327 015824.559 INFO             PET5 index= 351                                   MPIR_CVAR_USE_GATHER_NODE : range: 0-3 @Alias:# I_MPI_ADJUST_GATHER_NODE @Default:# -1
20220327 015824.559 INFO             PET5 index= 352            MPIR_CVAR_GATHER_NODE_BINOMIAL_SEGMENTED_SEGSIZE : @Alias:# I_MPI_ADJUST_GATHER_NODE_BINOMIAL_SEGMENTED_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET5 index= 353         MPIR_CVAR_GATHER_NETWORK_BINOMIAL_SEGMENTED_SEGSIZE : @Alias:# I_MPI_ADJUST_GATHER_NETWORK_BINOMIAL_SEGMENTED_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET5 index= 354                                       MPIR_CVAR_USE_GATHERV : Control selection of MPI_Gatherv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-4 @Alias:# I_MPI_ADJUST_GATHERV @Default:# -1
20220327 015824.559 INFO             PET5 index= 355                                  MPIR_CVAR_USE_GATHERV_LIST : @Alias:# I_MPI_ADJUST_GATHERV_LIST @Default:# -1
20220327 015824.559 INFO             PET5 index= 356                           MPIR_CVAR_USE_GATHERV_COMPOSITION : @Alias:# I_MPI_ADJUST_GATHERV_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET5 index= 357                               MPIR_CVAR_USE_GATHERV_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_GATHERV_NETWORK @Default:# -1
20220327 015824.559 INFO             PET5 index= 358                                  MPIR_CVAR_USE_GATHERV_NODE : range: 0-2 @Alias:# I_MPI_ADJUST_GATHERV_NODE @Default:# -1
20220327 015824.559 INFO             PET5 index= 359                     MPIR_CVAR_GATHERV_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_GATHERV_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET5 index= 360                        MPIR_CVAR_GATHERV_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_GATHERV_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET5 index= 361                                MPIR_CVAR_USE_REDUCE_SCATTER : Control selection of MPI_Reduce_scatter algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-6 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER @Default:# -1
20220327 015824.559 INFO             PET5 index= 362                           MPIR_CVAR_USE_REDUCE_SCATTER_LIST : @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_LIST @Default:# -1
20220327 015824.559 INFO             PET5 index= 363                    MPIR_CVAR_USE_REDUCE_SCATTER_COMPOSITION : @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET5 index= 364                        MPIR_CVAR_USE_REDUCE_SCATTER_NETWORK : range: 0-5 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_NETWORK @Default:# -1
20220327 015824.559 INFO             PET5 index= 365                           MPIR_CVAR_USE_REDUCE_SCATTER_NODE : range: 0-4 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_NODE @Default:# -1
20220327 015824.559 INFO             PET5 index= 366                          MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK : Control selection of MPI_Reduce_scatter_block algorithm presets. Arguments$ <algid> - Algorithm identifier$ range: 0-4 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK @Default:# -1
20220327 015824.559 INFO             PET5 index= 367                     MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK_LIST : @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK_LIST @Default:# -1
20220327 015824.559 INFO             PET5 index= 368              MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK_COMPOSITION : @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET5 index= 369                  MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK_NETWORK @Default:# -1
20220327 015824.559 INFO             PET5 index= 370                     MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK_NODE : range: 0-3 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK_NODE @Default:# -1
20220327 015824.559 INFO             PET5 index= 371                                        MPIR_CVAR_USE_REDUCE : Control selection of MPI_Reduce algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-13 @Alias:# I_MPI_ADJUST_REDUCE @Default:# -1
20220327 015824.559 INFO             PET5 index= 372                                   MPIR_CVAR_USE_REDUCE_LIST : @Alias:# I_MPI_ADJUST_REDUCE_LIST @Default:# -1
20220327 015824.559 INFO             PET5 index= 373                            MPIR_CVAR_USE_REDUCE_COMPOSITION : @Alias:# I_MPI_ADJUST_REDUCE_COMPOSITION @Default:# -1
20220327 015824.559 INFO             PET5 index= 374                                MPIR_CVAR_USE_REDUCE_NETWORK : range: 0-10 @Alias:# I_MPI_ADJUST_REDUCE_NETWORK @Default:# -1
20220327 015824.559 INFO             PET5 index= 375                                   MPIR_CVAR_USE_REDUCE_NODE : range: 0-7 @Alias:# I_MPI_ADJUST_REDUCE_NODE @Default:# -1
20220327 015824.559 INFO             PET5 index= 376                         MPIR_CVAR_REDUCE_NETWORK_KARY_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KARY_RADIX @Default:# -1
20220327 015824.559 INFO             PET5 index= 377                      MPIR_CVAR_REDUCE_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET5 index= 378                      MPIR_CVAR_REDUCE_NETWORK_KARY_NBUFFERS : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KARY_NBUFFERS @Default:# -1
20220327 015824.559 INFO             PET5 index= 379                   MPIR_CVAR_REDUCE_NETWORK_KNOMIAL_NBUFFERS : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KNOMIAL_NBUFFERS @Default:# -1
20220327 015824.559 INFO             PET5 index= 380                       MPIR_CVAR_REDUCE_NETWORK_KARY_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KARY_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET5 index= 381                    MPIR_CVAR_REDUCE_NETWORK_KNOMIAL_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KNOMIAL_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET5 index= 382                       MPIR_CVAR_REDUCE_NETWORK_RING_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NETWORK_RING_SEGSIZE @Default:# -1
20220327 015824.559 INFO             PET5 index= 383                            MPIR_CVAR_REDUCE_NODE_KARY_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KARY_RADIX @Default:# -1
20220327 015824.559 INFO             PET5 index= 384                         MPIR_CVAR_REDUCE_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.559 INFO             PET5 index= 385                         MPIR_CVAR_REDUCE_NODE_KARY_NBUFFERS : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KARY_NBUFFERS @Default:# -1
20220327 015824.559 INFO             PET5 index= 386                      MPIR_CVAR_REDUCE_NODE_KNOMIAL_NBUFFERS : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KNOMIAL_NBUFFERS @Default:# -1
20220327 015824.560 INFO             PET5 index= 387                          MPIR_CVAR_REDUCE_NODE_KARY_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KARY_SEGSIZE @Default:# -1
20220327 015824.560 INFO             PET5 index= 388                       MPIR_CVAR_REDUCE_NODE_KNOMIAL_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_KNOMIAL_SEGSIZE @Default:# -1
20220327 015824.560 INFO             PET5 index= 389                          MPIR_CVAR_REDUCE_NODE_RING_SEGSIZE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_RING_SEGSIZE @Default:# -1
20220327 015824.560 INFO             PET5 index= 390                       MPIR_CVAR_REDUCE_NODE_SHM_GATHER_TYPE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_SHM_GATHER_TYPE @Default:# -1
20220327 015824.560 INFO             PET5 index= 391                      MPIR_CVAR_REDUCE_NODE_SHM_GATHER_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NODE_SHM_GATHER_RADIX @Default:# -1
20220327 015824.560 INFO             PET5 index= 392                      MPIR_CVAR_REDUCE_NODE_SHM_RELEASE_TYPE : @Alias:# I_MPI_ADJUST_REDUCE_NODE_SHM_RELEASE_TYPE @Default:# -1
20220327 015824.560 INFO             PET5 index= 393                     MPIR_CVAR_REDUCE_NODE_SHM_RELEASE_RADIX : @Alias:# I_MPI_ADJUST_REDUCE_NODE_SHM_RELEASE_RADIX @Default:# -1
20220327 015824.560 INFO             PET5 index= 394                                          MPIR_CVAR_USE_SCAN : Control selection of MPI_Scan algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_SCAN @Default:# -1
20220327 015824.560 INFO             PET5 index= 395                                     MPIR_CVAR_USE_SCAN_LIST : @Alias:# I_MPI_ADJUST_SCAN_LIST @Default:# -1
20220327 015824.560 INFO             PET5 index= 396                              MPIR_CVAR_USE_SCAN_COMPOSITION : @Alias:# I_MPI_ADJUST_SCAN_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET5 index= 397                                  MPIR_CVAR_USE_SCAN_NETWORK : @Alias:# I_MPI_ADJUST_SCAN_NETWORK @Default:# -1
20220327 015824.560 INFO             PET5 index= 398                                     MPIR_CVAR_USE_SCAN_NODE : @Alias:# I_MPI_ADJUST_SCAN_NODE @Default:# -1
20220327 015824.560 INFO             PET5 index= 399                                       MPIR_CVAR_USE_SCATTER : Control selection of MPI_Scatter algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_SCATTER @Default:# -1
20220327 015824.560 INFO             PET5 index= 400                                  MPIR_CVAR_USE_SCATTER_LIST : @Alias:# I_MPI_ADJUST_SCATTER_LIST @Default:# -1
20220327 015824.560 INFO             PET5 index= 401                           MPIR_CVAR_USE_SCATTER_COMPOSITION : @Alias:# I_MPI_ADJUST_SCATTER_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET5 index= 402                               MPIR_CVAR_USE_SCATTER_NETWORK : range: 0-4 @Alias:# I_MPI_ADJUST_SCATTER_NETWORK @Default:# -1
20220327 015824.560 INFO             PET5 index= 403                                  MPIR_CVAR_USE_SCATTER_NODE : range: 0-3 @Alias:# I_MPI_ADJUST_SCATTER_NODE @Default:# -1
20220327 015824.560 INFO             PET5 index= 404                                      MPIR_CVAR_USE_SCATTERV : Control selection of MPI_Scatterv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-4 @Alias:# I_MPI_ADJUST_SCATTERV @Default:# -1
20220327 015824.560 INFO             PET5 index= 405                                 MPIR_CVAR_USE_SCATTERV_LIST : @Alias:# I_MPI_ADJUST_SCATTERV_LIST @Default:# -1
20220327 015824.560 INFO             PET5 index= 406                          MPIR_CVAR_USE_SCATTERV_COMPOSITION : @Alias:# I_MPI_ADJUST_SCATTERV_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET5 index= 407                              MPIR_CVAR_USE_SCATTERV_NETWORK : range: 0-3 @Alias:# I_MPI_ADJUST_SCATTERV_NETWORK @Default:# -1
20220327 015824.560 INFO             PET5 index= 408                                 MPIR_CVAR_USE_SCATTERV_NODE : range: 0-2 @Alias:# I_MPI_ADJUST_SCATTERV_NODE @Default:# -1
20220327 015824.560 INFO             PET5 index= 409                                    MPIR_CVAR_USE_IALLREDUCE : Control selection of MPI_Iallreduce algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-9 @Alias:# I_MPI_ADJUST_IALLREDUCE @Default:# -1
20220327 015824.560 INFO             PET5 index= 410                               MPIR_CVAR_USE_IALLREDUCE_LIST : @Alias:# I_MPI_ADJUST_IALLREDUCE_LIST @Default:# -1
20220327 015824.560 INFO             PET5 index= 411                        MPIR_CVAR_USE_IALLREDUCE_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLREDUCE_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET5 index= 412                            MPIR_CVAR_USE_IALLREDUCE_NETWORK : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK @Default:# -1
20220327 015824.560 INFO             PET5 index= 413                               MPIR_CVAR_USE_IALLREDUCE_NODE : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE @Default:# -1
20220327 015824.560 INFO             PET5 index= 414                   MPIR_CVAR_IALLREDUCE_KNOMIAL_REDUCE_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_KNOMIAL_REDUCE_RADIX @Default:# -1
20220327 015824.560 INFO             PET5 index= 415                    MPIR_CVAR_IALLREDUCE_KNOMIAL_BCAST_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_KNOMIAL_BCAST_RADIX @Default:# -1
20220327 015824.560 INFO             PET5 index= 416           MPIR_CVAR_IALLREDUCE_NETWORK_KNOMIAL_REDUCE_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK_KNOMIAL_REDUCE_RADIX @Default:# -1
20220327 015824.560 INFO             PET5 index= 417              MPIR_CVAR_IALLREDUCE_NODE_KNOMIAL_REDUCE_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE_KNOMIAL_REDUCE_RADIX @Default:# -1
20220327 015824.560 INFO             PET5 index= 418            MPIR_CVAR_IALLREDUCE_NETWORK_KNOMIAL_BCAST_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK_KNOMIAL_BCAST_RADIX @Default:# -1
20220327 015824.560 INFO             PET5 index= 419               MPIR_CVAR_IALLREDUCE_NODE_KNOMIAL_BCAST_RADIX : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE_KNOMIAL_BCAST_RADIX @Default:# -1
20220327 015824.560 INFO             PET5 index= 420                 MPIR_CVAR_IALLREDUCE_NODE_NREDUCE_DO_GATHER : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE_NREDUCE_DO_GATHER @Default:# -1
20220327 015824.560 INFO             PET5 index= 421              MPIR_CVAR_IALLREDUCE_NETWORK_NREDUCE_DO_GATHER : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK_NREDUCE_DO_GATHER @Default:# -1
20220327 015824.560 INFO             PET5 index= 422              MPIR_CVAR_IALLREDUCE_NODE_NREDUCE_NB_ALLGATHER : @Alias:# I_MPI_ADJUST_IALLREDUCE_NODE_NREDUCE_NB_ALLGATHER @Default:# -1
20220327 015824.560 INFO             PET5 index= 423           MPIR_CVAR_IALLREDUCE_NETWORK_NREDUCE_NB_ALLGATHER : @Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK_NREDUCE_NB_ALLGATHER @Default:# -1
20220327 015824.560 INFO             PET5 index= 424                                        MPIR_CVAR_USE_IBCAST : Control selection of MPI_Ibcast algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_IBCAST @Default:# -1
20220327 015824.560 INFO             PET5 index= 425                                   MPIR_CVAR_USE_IBCAST_LIST : @Alias:# I_MPI_ADJUST_IBCAST_LIST @Default:# -1
20220327 015824.560 INFO             PET5 index= 426                            MPIR_CVAR_USE_IBCAST_COMPOSITION : @Alias:# I_MPI_ADJUST_IBCAST_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET5 index= 427                                MPIR_CVAR_USE_IBCAST_NETWORK : @Alias:# I_MPI_ADJUST_IBCAST_NETWORK @Default:# -1
20220327 015824.560 INFO             PET5 index= 428                                   MPIR_CVAR_USE_IBCAST_NODE : @Alias:# I_MPI_ADJUST_IBCAST_NODE @Default:# -1
20220327 015824.560 INFO             PET5 index= 429                              MPIR_CVAR_IBCAST_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IBCAST_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET5 index= 430                      MPIR_CVAR_IBCAST_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IBCAST_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET5 index= 431                         MPIR_CVAR_IBCAST_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IBCAST_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET5 index= 432                                       MPIR_CVAR_USE_IREDUCE : Control selection of MPI_Ireduce algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-4 @Alias:# I_MPI_ADJUST_IREDUCE @Default:# -1
20220327 015824.560 INFO             PET5 index= 433                                  MPIR_CVAR_USE_IREDUCE_LIST : @Alias:# I_MPI_ADJUST_IREDUCE_LIST @Default:# -1
20220327 015824.560 INFO             PET5 index= 434                           MPIR_CVAR_USE_IREDUCE_COMPOSITION : @Alias:# I_MPI_ADJUST_IREDUCE_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET5 index= 435                               MPIR_CVAR_USE_IREDUCE_NETWORK : @Alias:# I_MPI_ADJUST_IREDUCE_NETWORK @Default:# -1
20220327 015824.560 INFO             PET5 index= 436                                  MPIR_CVAR_USE_IREDUCE_NODE : @Alias:# I_MPI_ADJUST_IREDUCE_NODE @Default:# -1
20220327 015824.560 INFO             PET5 index= 437                             MPIR_CVAR_IREDUCE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IREDUCE_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET5 index= 438                     MPIR_CVAR_IREDUCE_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IREDUCE_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET5 index= 439                        MPIR_CVAR_IREDUCE_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IREDUCE_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET5 index= 440                                       MPIR_CVAR_USE_IGATHER : Control selection of MPI_Igather algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-2 @Alias:# I_MPI_ADJUST_IGATHER @Default:# -1
20220327 015824.560 INFO             PET5 index= 441                                  MPIR_CVAR_USE_IGATHER_LIST : @Alias:# I_MPI_ADJUST_IGATHER_LIST @Default:# -1
20220327 015824.560 INFO             PET5 index= 442                           MPIR_CVAR_USE_IGATHER_COMPOSITION : @Alias:# I_MPI_ADJUST_IGATHER_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET5 index= 443                               MPIR_CVAR_USE_IGATHER_NETWORK : @Alias:# I_MPI_ADJUST_IGATHER_NETWORK @Default:# -1
20220327 015824.560 INFO             PET5 index= 444                                  MPIR_CVAR_USE_IGATHER_NODE : @Alias:# I_MPI_ADJUST_IGATHER_NODE @Default:# -1
20220327 015824.560 INFO             PET5 index= 445                             MPIR_CVAR_IGATHER_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IGATHER_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET5 index= 446                     MPIR_CVAR_IGATHER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IGATHER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET5 index= 447                        MPIR_CVAR_IGATHER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IGATHER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.560 INFO             PET5 index= 448                                    MPIR_CVAR_USE_IALLGATHER : Control selection of MPI_Iallgather algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_IALLGATHER @Default:# -1
20220327 015824.560 INFO             PET5 index= 449                               MPIR_CVAR_USE_IALLGATHER_LIST : @Alias:# I_MPI_ADJUST_IALLGATHER_LIST @Default:# -1
20220327 015824.560 INFO             PET5 index= 450                        MPIR_CVAR_USE_IALLGATHER_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLGATHER_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET5 index= 451                            MPIR_CVAR_USE_IALLGATHER_NETWORK : @Alias:# I_MPI_ADJUST_IALLGATHER_NETWORK @Default:# -1
20220327 015824.560 INFO             PET5 index= 452                               MPIR_CVAR_USE_IALLGATHER_NODE : @Alias:# I_MPI_ADJUST_IALLGATHER_NODE @Default:# -1
20220327 015824.560 INFO             PET5 index= 453                  MPIR_CVAR_IALLGATHER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IALLGATHER_NETWORK_KNOMIAL_RADIX
20220327 015824.560 INFO             PET5 index= 454                     MPIR_CVAR_IALLGATHER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_IALLGATHER_NODE_KNOMIAL_RADIX
20220327 015824.560 INFO             PET5 index= 455                                     MPIR_CVAR_USE_IALLTOALL : Control selection of MPI_Ialltoall algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-7 @Alias:# I_MPI_ADJUST_IALLTOALL @Default:# -1
20220327 015824.560 INFO             PET5 index= 456                                MPIR_CVAR_USE_IALLTOALL_LIST : @Alias:# I_MPI_ADJUST_IALLTOALL_LIST @Default:# -1
20220327 015824.560 INFO             PET5 index= 457                         MPIR_CVAR_USE_IALLTOALL_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLTOALL_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET5 index= 458                             MPIR_CVAR_USE_IALLTOALL_NETWORK : @Alias:# I_MPI_ADJUST_IALLTOALL_NETWORK @Default:# -1
20220327 015824.560 INFO             PET5 index= 459                                MPIR_CVAR_USE_IALLTOALL_NODE : @Alias:# I_MPI_ADJUST_IALLTOALL_NODE @Default:# -1
20220327 015824.560 INFO             PET5 index= 460              MPIR_CVAR_IALLTOALL_PERMUTED_SENDRECV_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALL_PERMUTED_SENDRECV_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET5 index= 461      MPIR_CVAR_IALLTOALL_NETWORK_PERMUTED_SENDRECV_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALL_NETWORK_PERMUTED_SENDRECV_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET5 index= 462         MPIR_CVAR_IALLTOALL_NODE_PERMUTED_SENDRECV_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALL_NODE_PERMUTED_SENDRECV_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET5 index= 463                                    MPIR_CVAR_USE_IALLTOALLV : Control selection of MPI_Ialltoallv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-6 @Alias:# I_MPI_ADJUST_IALLTOALLV @Default:# -1
20220327 015824.560 INFO             PET5 index= 464                               MPIR_CVAR_USE_IALLTOALLV_LIST : @Alias:# I_MPI_ADJUST_IALLTOALLV_LIST @Default:# -1
20220327 015824.560 INFO             PET5 index= 465                        MPIR_CVAR_USE_IALLTOALLV_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLTOALLV_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET5 index= 466                            MPIR_CVAR_USE_IALLTOALLV_NETWORK : @Alias:# I_MPI_ADJUST_IALLTOALLV_NETWORK @Default:# -1
20220327 015824.560 INFO             PET5 index= 467                               MPIR_CVAR_USE_IALLTOALLV_NODE : @Alias:# I_MPI_ADJUST_IALLTOALLV_NODE @Default:# -1
20220327 015824.560 INFO             PET5 index= 468                       MPIR_CVAR_IALLTOALLV_BLOCKED_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALLV_BLOCKED_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET5 index= 469               MPIR_CVAR_IALLTOALLV_NETWORK_BLOCKED_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALLV_NETWORK_BLOCKED_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET5 index= 470                                    MPIR_CVAR_USE_IALLTOALLW : Control selection of MPI_Ialltoallw algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-1 @Alias:# I_MPI_ADJUST_IALLTOALLW @Default:# -1
20220327 015824.560 INFO             PET5 index= 471                               MPIR_CVAR_USE_IALLTOALLW_LIST : @Alias:# I_MPI_ADJUST_IALLTOALLW_LIST @Default:# -1
20220327 015824.560 INFO             PET5 index= 472                        MPIR_CVAR_USE_IALLTOALLW_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLTOALLW_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET5 index= 473                            MPIR_CVAR_USE_IALLTOALLW_NETWORK : @Alias:# I_MPI_ADJUST_IALLTOALLW_NETWORK @Default:# -1
20220327 015824.560 INFO             PET5 index= 474                               MPIR_CVAR_USE_IALLTOALLW_NODE : @Alias:# I_MPI_ADJUST_IALLTOALLW_NODE @Default:# -1
20220327 015824.560 INFO             PET5 index= 475                       MPIR_CVAR_IALLTOALLW_BLOCKED_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALLW_BLOCKED_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET5 index= 476               MPIR_CVAR_IALLTOALLW_NETWORK_BLOCKED_THROTTLE : @Alias:# I_MPI_ADJUST_IALLTOALLW_NETWORK_BLOCKED_THROTTLE @Default:# -1
20220327 015824.560 INFO             PET5 index= 477                                      MPIR_CVAR_USE_IGATHERV : Control selection of MPI_Igatherv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_IGATHERV @Default:# -1
20220327 015824.560 INFO             PET5 index= 478                                 MPIR_CVAR_USE_IGATHERV_LIST : @Alias:# I_MPI_ADJUST_IGATHERV_LIST @Default:# -1
20220327 015824.560 INFO             PET5 index= 479                          MPIR_CVAR_USE_IGATHERV_COMPOSITION : @Alias:# I_MPI_ADJUST_IGATHERV_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET5 index= 480                              MPIR_CVAR_USE_IGATHERV_NETWORK : @Alias:# I_MPI_ADJUST_IGATHERV_NETWORK @Default:# -1
20220327 015824.560 INFO             PET5 index= 481                                 MPIR_CVAR_USE_IGATHERV_NODE : @Alias:# I_MPI_ADJUST_IGATHERV_NODE @Default:# -1
20220327 015824.560 INFO             PET5 index= 482                                     MPIR_CVAR_USE_ISCATTERV : Control selection of MPI_Iscatterv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-1 @Alias:# I_MPI_ADJUST_ISCATTERV @Default:# -1
20220327 015824.560 INFO             PET5 index= 483                                MPIR_CVAR_USE_ISCATTERV_LIST : @Alias:# I_MPI_ADJUST_ISCATTERV_LIST @Default:# -1
20220327 015824.560 INFO             PET5 index= 484                         MPIR_CVAR_USE_ISCATTERV_COMPOSITION : @Alias:# I_MPI_ADJUST_ISCATTERV_COMPOSITION @Default:# -1
20220327 015824.560 INFO             PET5 index= 485                             MPIR_CVAR_USE_ISCATTERV_NETWORK : @Alias:# I_MPI_ADJUST_ISCATTERV_NETWORK @Default:# -1
20220327 015824.561 INFO             PET5 index= 486                                MPIR_CVAR_USE_ISCATTERV_NODE : @Alias:# I_MPI_ADJUST_ISCATTERV_NODE @Default:# -1
20220327 015824.561 INFO             PET5 index= 487                                      MPIR_CVAR_USE_IBARRIER : Control selection of MPI_Ibarrier algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-2 @Alias:# I_MPI_ADJUST_IBARRIER @Default:# -1
20220327 015824.561 INFO             PET5 index= 488                                 MPIR_CVAR_USE_IBARRIER_LIST : @Alias:# I_MPI_ADJUST_IBARRIER_LIST @Default:# -1
20220327 015824.561 INFO             PET5 index= 489                          MPIR_CVAR_USE_IBARRIER_COMPOSITION : @Alias:# I_MPI_ADJUST_IBARRIER_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET5 index= 490                              MPIR_CVAR_USE_IBARRIER_NETWORK : @Alias:# I_MPI_ADJUST_IBARRIER_NETWORK @Default:# -1
20220327 015824.561 INFO             PET5 index= 491                                 MPIR_CVAR_USE_IBARRIER_NODE : @Alias:# I_MPI_ADJUST_IBARRIER_NODE @Default:# -1
20220327 015824.561 INFO             PET5 index= 492                                      MPIR_CVAR_USE_ISCATTER : Control selection of MPI_Iscatter algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-2 @Alias:# I_MPI_ADJUST_ISCATTER @Default:# -1
20220327 015824.561 INFO             PET5 index= 493                                 MPIR_CVAR_USE_ISCATTER_LIST : @Alias:# I_MPI_ADJUST_ISCATTER_LIST @Default:# -1
20220327 015824.561 INFO             PET5 index= 494                          MPIR_CVAR_USE_ISCATTER_COMPOSITION : @Alias:# I_MPI_ADJUST_ISCATTER_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET5 index= 495                              MPIR_CVAR_USE_ISCATTER_NETWORK : @Alias:# I_MPI_ADJUST_ISCATTER_NETWORK @Default:# -1
20220327 015824.561 INFO             PET5 index= 496                                 MPIR_CVAR_USE_ISCATTER_NODE : @Alias:# I_MPI_ADJUST_ISCATTER_NODE @Default:# -1
20220327 015824.561 INFO             PET5 index= 497                            MPIR_CVAR_ISCATTER_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ISCATTER_KNOMIAL_RADIX @Default:# -1
20220327 015824.561 INFO             PET5 index= 498                    MPIR_CVAR_ISCATTER_NETWORK_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ISCATTER_NETWORK_KNOMIAL_RADIX @Default:# -1
20220327 015824.561 INFO             PET5 index= 499                       MPIR_CVAR_ISCATTER_NODE_KNOMIAL_RADIX : @Alias:# I_MPI_ADJUST_ISCATTER_NODE_KNOMIAL_RADIX @Default:# -1
20220327 015824.561 INFO             PET5 index= 500                                   MPIR_CVAR_USE_IALLGATHERV : Control selection of MPI_Iallgatherv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_IALLGATHERV @Default:# -1
20220327 015824.561 INFO             PET5 index= 501                              MPIR_CVAR_USE_IALLGATHERV_LIST : @Alias:# I_MPI_ADJUST_IALLGATHERV_LIST @Default:# -1
20220327 015824.561 INFO             PET5 index= 502                       MPIR_CVAR_USE_IALLGATHERV_COMPOSITION : @Alias:# I_MPI_ADJUST_IALLGATHERV_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET5 index= 503                           MPIR_CVAR_USE_IALLGATHERV_NETWORK : @Alias:# I_MPI_ADJUST_IALLGATHERV_NETWORK @Default:# -1
20220327 015824.561 INFO             PET5 index= 504                              MPIR_CVAR_USE_IALLGATHERV_NODE : @Alias:# I_MPI_ADJUST_IALLGATHERV_NODE @Default:# -1
20220327 015824.561 INFO             PET5 index= 505                                       MPIR_CVAR_USE_IEXSCAN : Control selection of MPI_Iexscan algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-1 @Alias:# I_MPI_ADJUST_IEXSCAN @Default:# -1
20220327 015824.561 INFO             PET5 index= 506                                  MPIR_CVAR_USE_IEXSCAN_LIST : @Alias:# I_MPI_ADJUST_IEXSCAN_LIST @Default:# -1
20220327 015824.561 INFO             PET5 index= 507                           MPIR_CVAR_USE_IEXSCAN_COMPOSITION : @Alias:# I_MPI_ADJUST_IEXSCAN_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET5 index= 508                               MPIR_CVAR_USE_IEXSCAN_NETWORK : @Alias:# I_MPI_ADJUST_IEXSCAN_NETWORK @Default:# -1
20220327 015824.561 INFO             PET5 index= 509                                  MPIR_CVAR_USE_IEXSCAN_NODE : @Alias:# I_MPI_ADJUST_IEXSCAN_NODE @Default:# -1
20220327 015824.561 INFO             PET5 index= 510                                         MPIR_CVAR_USE_ISCAN : Control selection of MPI_Iscan algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-1 @Alias:# I_MPI_ADJUST_ISCAN @Default:# -1
20220327 015824.561 INFO             PET5 index= 511                                    MPIR_CVAR_USE_ISCAN_LIST : @Alias:# I_MPI_ADJUST_ISCAN_LIST @Default:# -1
20220327 015824.561 INFO             PET5 index= 512                             MPIR_CVAR_USE_ISCAN_COMPOSITION : @Alias:# I_MPI_ADJUST_ISCAN_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET5 index= 513                                 MPIR_CVAR_USE_ISCAN_NETWORK : @Alias:# I_MPI_ADJUST_ISCAN_NETWORK @Default:# -1
20220327 015824.561 INFO             PET5 index= 514                                    MPIR_CVAR_USE_ISCAN_NODE : @Alias:# I_MPI_ADJUST_ISCAN_NODE @Default:# -1
20220327 015824.561 INFO             PET5 index= 515                               MPIR_CVAR_USE_IREDUCE_SCATTER : Control selection of MPI_Ireduce_scatter algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER @Default:# -1
20220327 015824.561 INFO             PET5 index= 516                          MPIR_CVAR_USE_IREDUCE_SCATTER_LIST : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_LIST @Default:# -1
20220327 015824.561 INFO             PET5 index= 517                   MPIR_CVAR_USE_IREDUCE_SCATTER_COMPOSITION : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET5 index= 518                       MPIR_CVAR_USE_IREDUCE_SCATTER_NETWORK : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_NETWORK @Default:# -1
20220327 015824.561 INFO             PET5 index= 519                          MPIR_CVAR_USE_IREDUCE_SCATTER_NODE : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_NODE @Default:# -1
20220327 015824.561 INFO             PET5 index= 520                         MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK : Control selection of MPI_Ireduce_scatter_block algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK @Default:# -1
20220327 015824.561 INFO             PET5 index= 521                    MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK_LIST : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK_LIST @Default:# -1
20220327 015824.561 INFO             PET5 index= 522             MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK_COMPOSITION : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK_COMPOSITION @Default:# -1
20220327 015824.561 INFO             PET5 index= 523                 MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK_NETWORK : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK_NETWORK @Default:# -1
20220327 015824.561 INFO             PET5 index= 524                    MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK_NODE : @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK_NODE @Default:# -1
20220327 015824.561 INFO             PET5 index= 525                               MPIR_CVAR_IMPI_SHMGR_DATASIZE : Define the size of shared memory area available for each rank for data placement. Messages greater than this value will not be processed by SHM-based collective operation, but will be processed by point-to-point based collective operation. The value must be a multiple of 4096. @Alias:# I_MPI_COLL_SHM_THRESHOLD @Default:# 16384
20220327 015824.561 INFO             PET5 index= 526                              MPIR_CVAR_IMPI_SHMGR_SPINCOUNT : @Alias:# I_MPI_COLL_SHM_PROGRESS_SPIN_COUNT
20220327 015824.561 INFO             PET5 index= 527                              MPIR_CVAR_INTEL_COLL_INTRANODE : @Alias:# I_MPI_COLL_INTRANODE
20220327 015824.561 INFO             PET5 index= 528                         MPIR_CVAR_ENABLE_EXPERIMENTAL_ALGOS : @Alias:# I_MPI_COLL_EXPERIMENTAL
20220327 015824.561 INFO             PET5 index= 529                                    MPIR_CVAR_IMPI_WAIT_MODE : @Alias:# I_MPI_WAIT_MODE
20220327 015824.561 INFO             PET5 index= 530                                 MPIR_CVAR_IMPI_THREAD_SLEEP : @Alias:# I_MPI_THREAD_SLEEP
20220327 015824.561 INFO             PET5 index= 531                          MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the short message algorithm will be used if the send buffer size is < this value (in bytes). (See also: MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE)
20220327 015824.561 INFO             PET5 index= 532                           MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the long message algorithm will be used if the send buffer size is >= this value (in bytes) (See also: MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE)
20220327 015824.561 INFO             PET5 index= 533                      MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE : The smallest message size that will be used for the pipelined, large-message, ring algorithm in the MPI_Allgatherv implementation.
20220327 015824.561 INFO             PET5 index= 534                          MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20220327 015824.561 INFO             PET5 index= 535                            MPIR_CVAR_ENABLE_SMP_COLLECTIVES : Enable SMP aware collective communication.
20220327 015824.561 INFO             PET5 index= 536                              MPIR_CVAR_ENABLE_SMP_ALLREDUCE : Enable SMP aware allreduce.
20220327 015824.561 INFO             PET5 index= 537                        MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE : Maximum message size for which SMP-aware allreduce is used.  A value of '0' uses SMP-aware allreduce for all message sizes.
20220327 015824.561 INFO             PET5 index= 538                           MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE : the short message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value (See also: MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE)
20220327 015824.561 INFO             PET5 index= 539                          MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE : the medium message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value and larger than MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE (See also: MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE)
20220327 015824.561 INFO             PET5 index= 540                                MPIR_CVAR_ENABLE_SMP_BARRIER : Enable SMP aware barrier.
20220327 015824.561 INFO             PET5 index= 541                                   MPIR_CVAR_BCAST_MIN_PROCS : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_SHORT_MSG_SIZE, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20220327 015824.561 INFO             PET5 index= 542                              MPIR_CVAR_BCAST_SHORT_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20220327 015824.561 INFO             PET5 index= 543                               MPIR_CVAR_BCAST_LONG_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_SHORT_MSG_SIZE)
20220327 015824.561 INFO             PET5 index= 544                                  MPIR_CVAR_ENABLE_SMP_BCAST : Enable SMP aware broadcast (See also: MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE)
20220327 015824.561 INFO             PET5 index= 545                            MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE : Maximum message size for which SMP-aware broadcast is used.  A value of '0' uses SMP-aware broadcast for all message sizes. (See also: MPIR_CVAR_ENABLE_SMP_BCAST)
20220327 015824.561 INFO             PET5 index= 546                            MPIR_CVAR_GATHER_VSMALL_MSG_SIZE : use a temporary buffer for intracommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE)
20220327 015824.561 INFO             PET5 index= 547                       MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_VSMALL_MSG_SIZE)
20220327 015824.561 INFO             PET5 index= 548                     MPIR_CVAR_GATHERV_INTER_SSEND_MIN_PROCS : Use Ssend (synchronous send) for intercommunicator MPI_Gatherv if the "group B" size is >= this value.  Specifying "-1" always avoids using Ssend.  For backwards compatibility, specifying "0" uses the default value.
20220327 015824.561 INFO             PET5 index= 549                             MPIR_CVAR_REDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20220327 015824.561 INFO             PET5 index= 550                                 MPIR_CVAR_ENABLE_SMP_REDUCE : Enable SMP aware reduce.
20220327 015824.561 INFO             PET5 index= 551                           MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE : Maximum message size for which SMP-aware reduce is used.  A value of '0' uses SMP-aware reduce for all message sizes.
20220327 015824.561 INFO             PET5 index= 552                      MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Scatter if the send buffer size is < this value (in bytes)
20220327 015824.561 INFO             PET5 index= 553                                  MPIR_CVAR_USE_CPU_PLATFORM : @Alias:# I_MPI_PLATFORM
20220327 015824.561 INFO             PET5 index= 554                          MPIR_CVAR_FAILURE_ON_COLL_FALLBACK : @Alias:# I_MPI_ADJUST_FAILURE_ON_COLL_FALLBACK
20220327 015824.561 INFO             PET5 index= 555                         MPIR_CVAR_FAILURE_ON_MATCH_FALLBACK : @Alias:# I_MPI_ADJUST_FAILURE_ON_MATCH_FALLBACK
20220327 015824.561 INFO             PET5 index= 556                           MPIR_CVAR_ADJUST_SENDRECV_REPLACE : @Alias:# I_MPI_ADJUST_SENDRECV_REPLACE
20220327 015824.561 INFO             PET5 index= 557                MPIR_CVAR_ADJUST_SENDRECV_REPLACE_FRAME_SIZE : @Alias:# I_MPI_ADJUST_SENDRECV_REPLACE_FRAME_SIZE
20220327 015824.561 INFO             PET5 index= 558                         MPIR_CVAR_NUMERICAL_REPRODUCIBILITY : @Alias:# I_MPI_CBWR
20220327 015824.561 INFO             PET5 index= 559                                    MPIR_CVAR_USE_TUNING_CH4 : @Alias:# I_MPI_TUNING
20220327 015824.561 INFO             PET5 index= 560                                    MPIR_CVAR_USE_TUNING_NET : @Alias:# I_MPI_TUNING_NETWORK
20220327 015824.561 INFO             PET5 index= 561                                    MPIR_CVAR_USE_TUNING_SHM : @Alias:# I_MPI_TUNING_NODE
20220327 015824.561 INFO             PET5 index= 562                                   MPIR_CVAR_DUMP_TUNING_CH4 : @Alias:# I_MPI_TUNING_COMPOSITION_DUMP
20220327 015824.561 INFO             PET5 index= 563                                   MPIR_CVAR_DUMP_TUNING_NET : @Alias:# I_MPI_TUNING_NETWORK_DUMP
20220327 015824.561 INFO             PET5 index= 564                                   MPIR_CVAR_DUMP_TUNING_SHM : @Alias:# I_MPI_TUNING_NODE_DUMP
20220327 015824.561 INFO             PET5 index= 565                                        MPIR_CVAR_BIN_TUNING : @Alias:# I_MPI_TUNING_BIN
20220327 015824.561 INFO             PET5 index= 566                                   MPIR_CVAR_BIN_DUMP_TUNING : @Alias:# I_MPI_TUNING_BIN_DUMP
20220327 015824.561 INFO             PET5 index= 567                                   MPIR_CVAR_TUNING_BIN_PATH : @Alias:# I_MPI_TUNING_BIN_PATH
20220327 015824.561 INFO             PET5 index= 568                            MPIR_CVAR_TUNING_COMPOSITION_PPN : @Alias:# I_MPI_TUNING_COMPOSITION_PPN
20220327 015824.561 INFO             PET5 index= 569                                MPIR_CVAR_TUNING_NETWORK_PPN : @Alias:# I_MPI_TUNING_NETWORK_PPN
20220327 015824.561 INFO             PET5 index= 570                                   MPIR_CVAR_TUNING_NODE_PPN : @Alias:# I_MPI_TUNING_NODE_PPN
20220327 015824.561 INFO             PET5 index= 571                 MPIR_CVAR_TUNING_COMPOSITION_COMM_HIERARCHY : @Alias:# I_MPI_TUNING_COMPOSITION_COMM_HIERARCHY
20220327 015824.561 INFO             PET5 index= 572                     MPIR_CVAR_TUNING_NETWORK_COMM_HIERARCHY : @Alias:# I_MPI_TUNING_NETWORK_COMM_HIERARCHY
20220327 015824.561 INFO             PET5 index= 573                        MPIR_CVAR_TUNING_NODE_COMM_HIERARCHY : @Alias:# I_MPI_TUNING_NODE_COMM_HIERARCHY
20220327 015824.561 INFO             PET5 index= 574                                       MPIR_CVAR_TUNING_MODE : @Alias:# I_MPI_TUNING_MODE
20220327 015824.561 INFO             PET5 index= 575                                MPIR_CVAR_TUNING_AUTO_POLICY : @Alias:# I_MPI_TUNING_AUTO_POLICY
20220327 015824.561 INFO             PET5 index= 576                             MPIR_CVAR_TUNING_AUTO_COMM_LIST : @Alias:# I_MPI_TUNING_AUTO_COMM_LIST
20220327 015824.561 INFO             PET5 index= 577                             MPIR_CVAR_TUNING_AUTO_COMM_USER : @Alias:# I_MPI_TUNING_AUTO_COMM_USER
20220327 015824.561 INFO             PET5 index= 578                          MPIR_CVAR_TUNING_AUTO_COMM_DEFAULT : @Alias:# I_MPI_TUNING_AUTO_COMM_DEFAULT
20220327 015824.561 INFO             PET5 index= 579                              MPIR_CVAR_TUNING_AUTO_ITER_NUM : @Alias:# I_MPI_TUNING_AUTO_ITER_NUM
20220327 015824.561 INFO             PET5 index= 580                           MPIR_CVAR_TUNING_AUTO_ITER_POLICY : @Alias:# I_MPI_TUNING_AUTO_ITER_POLICY
20220327 015824.561 INFO             PET5 index= 581                 MPIR_CVAR_TUNING_AUTO_ITER_POLICY_THRESHOLD : @Alias:# I_MPI_TUNING_AUTO_ITER_POLICY_THRESHOLD
20220327 015824.561 INFO             PET5 index= 582                                  MPIR_CVAR_TUNING_AUTO_SYNC : @Alias:# I_MPI_TUNING_AUTO_SYNC
20220327 015824.561 INFO             PET5 index= 583                          MPIR_CVAR_TUNING_AUTO_STORAGE_SIZE : @Alias:# I_MPI_TUNING_AUTO_STORAGE_SIZE
20220327 015824.561 INFO             PET5 index= 584                       MPIR_CVAR_TUNING_AUTO_WARMUP_ITER_NUM : @Alias:# I_MPI_TUNING_AUTO_WARMUP_ITER_NUM
20220327 015824.561 INFO             PET5 index= 585                                 MPIR_CVAR_TUNING_AUTO_SMART : @Alias:# I_MPI_TUNING_AUTO_SMART
20220327 015824.561 INFO             PET5 index= 586                                  MPIR_CVAR_TUNING_COLL_LIST : @Alias:# I_MPI_TUNING_COLL_LIST
20220327 015824.561 INFO             PET5 index= 587                               MPIR_CVAR_TUNING_COLL_VEC_OPS : @Alias:# I_MPI_TUNING_COLL_VEC_OPS
20220327 015824.561 INFO             PET5 index= 588                              MPIR_CVAR_DEFAULT_THREAD_LEVEL : @Alias:# I_MPI_THREAD_LEVEL
20220327 015824.562 INFO             PET5 index= 589                                      MPIR_CVAR_THREAD_SPLIT : Control the MPI_THREAD_SPLIT model support. @Alias:# I_MPI_THREAD_SPLIT @Default:# false
20220327 015824.562 INFO             PET5 index= 590                                    MPIR_CVAR_THREAD_RUNTIME : Control threading runtimes support. @Alias:# I_MPI_THREAD_RUNTIME @Default:# generic
20220327 015824.562 INFO             PET5 index= 591                                     MPIR_CVAR_THREAD_ID_KEY : Set the MPI info object key that is used to explicitly define the application $thread_id for a communicator. @Alias:# I_MPI_THREAD_ID_KEY @Default:# -1
20220327 015824.562 INFO             PET5 index= 592                                        MPIR_CVAR_THREAD_MAX : Set the maximum number of application threads per rank. @Alias:# I_MPI_THREAD_MAX @Default:# -1
20220327 015824.562 INFO             PET5 index= 593                                    MPIR_CVAR_ASYNC_PROGRESS : Enables asynchronous progress threads. @Alias:# I_MPI_ASYNC_PROGRESS @Default:# false
20220327 015824.562 INFO             PET5 index= 594                          MPIR_CVAR_CH4_MAX_PROGRESS_THREADS : Specifies the maximum number of progress threads. @Alias:# I_MPI_ASYNC_PROGRESS_THREADS @Default:# 1
20220327 015824.562 INFO             PET5 index= 595                      MPIR_CVAR_CH4_PROGRESS_THREAD_AFFINITY : Specifies affinity for all progress threads of local processes. @Alias:# I_MPI_ASYNC_PROGRESS_PIN @Default:# not defined
20220327 015824.562 INFO             PET5 index= 596                                         MPIR_CVAR_EP_ID_KEY : Set the MPI info object key that is used to explicitly define the progress $thread_id for a communicator. @Alias:# I_MPI_ASYNC_PROGRESS_ID_KEY @Default:# thread_id
20220327 015824.562 INFO             PET5 index= 597                                       MPIR_CVAR_THREAD_MODE : @Alias:# I_MPI_THREAD_MODE
20220327 015824.562 INFO             PET5 index= 598                                 MPIR_CVAR_THREAD_LOCK_LEVEL : @Alias:# I_MPI_THREAD_LOCK_LEVEL
20220327 015824.562 INFO             PET5 index= 599                                  MPIR_CVAR_CH4_OFI_MAX_VCIS : @Alias:# I_MPI_THREAD_EP_MAX
20220327 015824.562 INFO             PET5 index= 600                                       MPIR_CVAR_INTEL_DEBUG : Print out debugging information when an MPI program starts running.$ Syntax$ I_MPI_DEBUG=<level>$ Arguments$ <level> - indicate level of debug information provided @Alias:# I_MPI_DEBUG @Default:# 0
20220327 015824.562 INFO             PET5 index= 601                                     MPIR_CVAR_DEBUG_VERSION : Print Intel MPI version. @Alias:# I_MPI_PRINT_VERSION @Default:# 0
20220327 015824.562 INFO             PET5 index= 602                                    MPIR_CVAR_ERROR_CHECKING : @Alias:# I_MPI_ERROR_CHECKING
20220327 015824.562 INFO             PET5 index= 603                                        MPIR_CVAR_MULTI_INIT : @Alias:# I_MPI_MULTI_INIT
20220327 015824.562 INFO             PET5 index= 604                               MPIR_CVAR_REMOVED_VAR_WARNING : Print out a warning if a removed environment variable is set. @Alias:# I_MPI_REMOVED_VAR_WARNING @Default:# 1
20220327 015824.562 INFO             PET5 index= 605                                MPIR_CVAR_VAR_CHECK_SPELLING : Print out a warning if an unknown environment variable is set. @Alias:# I_MPI_VAR_CHECK_SPELLING @Default:# 1
20220327 015824.562 INFO             PET5 index= 606                           MPIR_CVAR_INTEL_MPI_COMPATIBILITY : Select the runtime compatibility mode.$ Syntax$ I_MPI_COMPATIBILITY=<value>$ Arguments$ <value> - Define compatibility mode$ -----------------------------------------------------------------------$ <not defined> - The MPI-3.1 standard compatibility$ <3> - The Intel® MPI Library 3.x compatible mode$ <4> - The Intel® MPI Library 4.x compatible mode$ <5> - The Intel® MPI Library 5.x compatible mode$ ----------------------------------------------------------------------- @Alias:# I_MPI_COMPATIBILITY @Default:# 5
20220327 015824.562 INFO             PET5 index= 607                          MPIR_CVAR_IMPI_PROGRESS_SPIN_COUNT : @Alias:# I_MPI_SPIN_COUNT
20220327 015824.562 INFO             PET5 index= 608                         MPIR_CVAR_IMPI_PROGRESS_PAUSE_COUNT : @Alias:# I_MPI_PAUSE_COUNT
20220327 015824.562 INFO             PET5 index= 609                                 MPIR_CVAR_IMPI_THREAD_YIELD : @Alias:# I_MPI_THREAD_YIELD
20220327 015824.562 INFO             PET5 index= 610                                      MPIR_CVAR_SILENT_ABORT : Do not print abort warning message @Alias:# I_MPI_SILENT_ABORT
20220327 015824.562 INFO             PET5 index= 611                                  MPIR_CVAR_JOB_IDLE_TIMEOUT : Abort job if idle time is larger than the threshold in seconds. @Alias:# I_MPI_JOB_IDLE_TIMEOUT
20220327 015824.562 INFO             PET5 index= 612                              MPIR_CVAR_PMI_VALUE_LENGTH_MAX : Set PMI buffer length as minimum of variable value and PMI_KVS_Get_value_length_max(). @Alias:# I_MPI_PMI_VALUE_LENGTH_MAX
20220327 015824.562 INFO             PET5 index= 613                                       MPIR_CVAR_PMI_LIBRARY : Specify the name to third party implementation of the PMI library. @Alias:# I_MPI_PMI_LIBRARY
20220327 015824.562 INFO             PET5 index= 614                                               MPIR_CVAR_PMI : Select PMI version. Choices: auto, pmi1, pmi2, pmix.$ By default pmi version will be chosen automatically. @Alias:# I_MPI_PMI
20220327 015824.562 INFO             PET5 index= 615                                 MPIR_CVAR_NODEMAP_ALGORITHM : Select algorithm for nodemap creation. Choices: pmi_process_mapping, slurm, pmi_alltoall, auto. @Alias:# I_MPI_NODEMAP_ALGORITHM
20220327 015824.562 INFO             PET5 index= 616                                      MPIR_CVAR_ASYNC_REDUCE : @Alias:# I_MPI_ASYNC_REDUCE @Verbosity:# hidden
20220327 015824.562 INFO             PET5 index= 617                            MPIR_CVAR_CH4_MAX_REDUCE_THREADS : @Alias:# I_MPI_ASYNC_REDUCE_THREADS @Verbosity:# hidden
20220327 015824.562 INFO             PET5 index= 618                      MPIR_CVAR_ASYNC_REDUCE_COUNT_THRESHOLD : @Alias:# I_MPI_ASYNC_REDUCE_COUNT_THRESHOLD @Verbosity:# hidden
20220327 015824.562 INFO             PET5 index= 619                        MPIR_CVAR_CH4_REDUCE_THREAD_AFFINITY : @Alias:# I_MPI_ASYNC_REDUCE_PIN @Verbosity:# hidden
20220327 015824.562 INFO             PET5 --- VMK::logSystem() end ---------------------------------
20220327 015824.562 INFO             PET5 main: --- VMK::log() start -------------------------------------
20220327 015824.562 INFO             PET5 main: vm located at: 0x87ae60
20220327 015824.562 INFO             PET5 main: petCount=6 localPet=5 mypthid=140737352203136 currentSsiPe=20
20220327 015824.562 INFO             PET5 main: Current system level affinity pinning for local PET:
20220327 015824.562 INFO             PET5 main:  SSIPE=20
20220327 015824.562 INFO             PET5 main:  SSIPE=21
20220327 015824.562 INFO             PET5 main:  SSIPE=22
20220327 015824.562 INFO             PET5 main:  SSIPE=23
20220327 015824.562 INFO             PET5 main: Current system level OMP_NUM_THREADS setting for local PET: 1
20220327 015824.562 INFO             PET5 main: ssiCount=1 localSsi=0
20220327 015824.562 INFO             PET5 main: mpionly=1 threadsflag=0
20220327 015824.562 INFO             PET5 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220327 015824.562 INFO             PET5 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220327 015824.562 INFO             PET5 main:  PE=0 SSI=0 SSIPE=0
20220327 015824.562 INFO             PET5 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220327 015824.562 INFO             PET5 main:  PE=1 SSI=0 SSIPE=1
20220327 015824.562 INFO             PET5 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220327 015824.562 INFO             PET5 main:  PE=2 SSI=0 SSIPE=2
20220327 015824.562 INFO             PET5 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220327 015824.562 INFO             PET5 main:  PE=3 SSI=0 SSIPE=3
20220327 015824.562 INFO             PET5 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220327 015824.562 INFO             PET5 main:  PE=4 SSI=0 SSIPE=4
20220327 015824.562 INFO             PET5 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220327 015824.562 INFO             PET5 main:  PE=5 SSI=0 SSIPE=5
20220327 015824.562 INFO             PET5 main: --- VMK::log() end ---------------------------------------
20220327 015824.565 INFO             PET5 Executing 'userm1_setvm'
20220327 015824.566 INFO             PET5 Executing 'userm1_register'
20220327 015824.566 INFO             PET5 Executing 'userm2_setvm'
20220327 015824.566 DEBUG            PET5 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220327 015824.566 DEBUG            PET5 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220327 015824.649 INFO             PET5 Entering 'user1_run'
20220327 015824.649 INFO             PET5 model1: --- VMK::log() start -------------------------------------
20220327 015824.649 INFO             PET5 model1: vm located at: 0x8bf230
20220327 015824.649 INFO             PET5 model1: petCount=6 localPet=5 mypthid=140737352203136 currentSsiPe=5
20220327 015824.649 INFO             PET5 model1: Current system level affinity pinning for local PET:
20220327 015824.649 INFO             PET5 model1:  SSIPE=5
20220327 015824.649 INFO             PET5 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220327 015824.649 INFO             PET5 model1: ssiCount=1 localSsi=0
20220327 015824.649 INFO             PET5 model1: mpionly=1 threadsflag=0
20220327 015824.649 INFO             PET5 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220327 015824.649 INFO             PET5 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220327 015824.649 INFO             PET5 model1:  PE=0 SSI=0 SSIPE=0
20220327 015824.649 INFO             PET5 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220327 015824.649 INFO             PET5 model1:  PE=1 SSI=0 SSIPE=1
20220327 015824.649 INFO             PET5 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220327 015824.649 INFO             PET5 model1:  PE=2 SSI=0 SSIPE=2
20220327 015824.649 INFO             PET5 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220327 015824.649 INFO             PET5 model1:  PE=3 SSI=0 SSIPE=3
20220327 015824.649 INFO             PET5 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220327 015824.649 INFO             PET5 model1:  PE=4 SSI=0 SSIPE=4
20220327 015824.649 INFO             PET5 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220327 015824.649 INFO             PET5 model1:  PE=5 SSI=0 SSIPE=5
20220327 015824.649 INFO             PET5 model1: --- VMK::log() end ---------------------------------------
20220327 015824.649 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220327 015824.774 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220327 015824.899 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220327 015825.024 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220327 015825.149 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220327 015825.274 INFO             PET5 Exiting 'user1_run'
20220327 015826.769 INFO             PET5 Entering 'user1_run'
20220327 015826.769 INFO             PET5 model1: --- VMK::log() start -------------------------------------
20220327 015826.769 INFO             PET5 model1: vm located at: 0x8bf230
20220327 015826.770 INFO             PET5 model1: petCount=6 localPet=5 mypthid=140737352203136 currentSsiPe=5
20220327 015826.770 INFO             PET5 model1: Current system level affinity pinning for local PET:
20220327 015826.770 INFO             PET5 model1:  SSIPE=5
20220327 015826.770 INFO             PET5 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220327 015826.770 INFO             PET5 model1: ssiCount=1 localSsi=0
20220327 015826.770 INFO             PET5 model1: mpionly=1 threadsflag=0
20220327 015826.770 INFO             PET5 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220327 015826.770 INFO             PET5 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220327 015826.770 INFO             PET5 model1:  PE=0 SSI=0 SSIPE=0
20220327 015826.770 INFO             PET5 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220327 015826.770 INFO             PET5 model1:  PE=1 SSI=0 SSIPE=1
20220327 015826.770 INFO             PET5 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220327 015826.770 INFO             PET5 model1:  PE=2 SSI=0 SSIPE=2
20220327 015826.770 INFO             PET5 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220327 015826.770 INFO             PET5 model1:  PE=3 SSI=0 SSIPE=3
20220327 015826.770 INFO             PET5 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220327 015826.770 INFO             PET5 model1:  PE=4 SSI=0 SSIPE=4
20220327 015826.770 INFO             PET5 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220327 015826.770 INFO             PET5 model1:  PE=5 SSI=0 SSIPE=5
20220327 015826.770 INFO             PET5 model1: --- VMK::log() end ---------------------------------------
20220327 015826.770 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220327 015826.895 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220327 015827.020 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220327 015827.144 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220327 015827.272 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220327 015827.397 INFO             PET5 Exiting 'user1_run'
20220327 015828.883 INFO             PET5  NUMBER_OF_PROCESSORS           6
20220327 015828.883 INFO             PET5  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220327 015828.883 INFO             PET5 Finalizing ESMF
