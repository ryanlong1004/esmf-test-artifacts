build time -- 2022-03-17 01:28:56
20220317 014102.897 INFO             PET0 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220317 014102.897 INFO             PET0 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220317 014102.897 INFO             PET0 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220317 014102.897 INFO             PET0 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220317 014102.897 INFO             PET0 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220317 014102.897 INFO             PET0 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220317 014102.897 INFO             PET0 Running with ESMF Version   : v8.3.0b09-100-g5ff85a963f
20220317 014102.897 INFO             PET0 ESMF library build date/time: "Mar 17 2022" "01:27:56"
20220317 014102.897 INFO             PET0 ESMF library build location : /lfs/h1/emc/ptmp/Mark.Potts/gfortran_10.3.0_mpi_O_jedwards_pio_update2
20220317 014102.897 INFO             PET0 ESMF_COMM                   : mpi
20220317 014102.897 INFO             PET0 ESMF_MOAB                   : enabled
20220317 014102.897 INFO             PET0 ESMF_LAPACK                 : enabled
20220317 014102.897 INFO             PET0 ESMF_NETCDF                 : enabled
20220317 014102.897 INFO             PET0 ESMF_PNETCDF                : disabled
20220317 014102.897 INFO             PET0 ESMF_PIO                    : enabled
20220317 014102.897 INFO             PET0 ESMF_YAMLCPP                : enabled
20220317 014102.898 INFO             PET0 --- VMK::logSystem() start -------------------------------
20220317 014102.898 INFO             PET0 esmfComm=mpi
20220317 014102.898 INFO             PET0 isPthreadsEnabled=1
20220317 014102.898 INFO             PET0 isOpenMPEnabled=1
20220317 014102.898 INFO             PET0 isOpenACCEnabled=0
20220317 014102.898 INFO             PET0 isSsiSharedMemoryEnabled=1
20220317 014102.898 INFO             PET0 ssiCount=1 peCount=6
20220317 014102.898 INFO             PET0 PE=0 SSI=0 SSIPE=0
20220317 014102.898 INFO             PET0 PE=1 SSI=0 SSIPE=1
20220317 014102.898 INFO             PET0 PE=2 SSI=0 SSIPE=2
20220317 014102.898 INFO             PET0 PE=3 SSI=0 SSIPE=3
20220317 014102.898 INFO             PET0 PE=4 SSI=0 SSIPE=4
20220317 014102.898 INFO             PET0 PE=5 SSI=0 SSIPE=5
20220317 014102.898 INFO             PET0 --- VMK::logSystem() MPI Control Variables ---------------
20220317 014102.898 INFO             PET0 index=   0          MPIR_CVAR_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE : specifies the cutoff size of the send buffer (in bytes) above which the reduce_scatter functions attempt to use the pairwise exchange algorithm.  In addition, the op must be commutative and the communicator size < MPIR_CVAR_REDUCE_SCATTER_MAX_COMMSIZE for the pairwise exchange algorithm to be used.
20220317 014102.898 INFO             PET0 index=   1                       MPIR_CVAR_REDUCE_SCATTER_MAX_COMMSIZE : specifies the max communicator size that will trigger use of the pairwise exchange algorithm, provided the op is commutative.  The pairwise exchange algorithm is not well suited for scaling to high process counts, so for larger communicators, a recursive halving algorithm is used instead.
20220317 014102.898 INFO             PET0 index=   2                           MPIR_CVAR_NETWORK_BUFFER_COLL_OPT : If set to 1, the MPICH library will use the optimized shared- memory based "network buffer" design for collective operations.  This feature is closely tied to the shared-memory collective optimization available in Cray MPICH. If enabled, the shared-memory buffer is also registered with the NIC and can be used directly to  perform off-node transfers, bypassing the Nemesis channel layer.  This feature is disabled if MPICH_SHARED_MEM_COLL_OPT  is set to 0. Currently, this optimization is only available  for the MPI_Bcast collective operation. To disable this feature,  set MPICH_NETWORK_BUFFER_COLL_OPT to 0.  Default: 0
20220317 014102.898 INFO             PET0 index=   3                                MPIR_CVAR_ALLTOALL_SYNC_FREQ : Adjusts the number of outstanding messages each Alltoall process  will allow.  Default is variable.
20220317 014102.898 INFO             PET0 index=   4                                 MPIR_CVAR_ALLTOALL_BLK_SIZE : The transfer size in bytes for the Alltoall chunking algorithm.  Default is 16384.
20220317 014102.898 INFO             PET0 index=   5                       MPIR_CVAR_ALLTOALL_CHUNKING_MAX_NODES : The maximum number of nodes to use the alltoall chunking algorithm. Above this value, the throttled algorithm will be used.  This is only applicable to SS-11.
20220317 014102.898 INFO             PET0 index=   6                              MPIR_CVAR_ALLGATHER_VSHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized gather/bcast algorithm is used instead of the optimized ring algorithm for MPI_Allgather. The gather/bcast algorithm is better suited for small messages. Defaults: For communicator sizes of <=512 ranks, 1024 bytes. For communicator sizes of >512 ranks, 4096 bytes.
20220317 014102.898 INFO             PET0 index=   7                             MPIR_CVAR_ALLGATHERV_VSHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized gatherv/bcast algorithm is used instead of the optimized ring algorithm for MPI_Allgatherv. The gatherv/bcast algorithm is better suited for small messages. Defaults: For communicator sizes of <=512 ranks, 1024 bytes. For communicator sizes of >512 ranks, 4096 bytes.
20220317 014102.898 INFO             PET0 index=   8                                  MPIR_CVAR_ALLREDUCE_NO_SMP : If set, MPI_Allreduce uses an algorithm that is not smp- aware. This provides a consistent ordering of the specified allreduce operation regardless of system configuration. Note:  This algorithm may not perform as well as the default smp-aware algorithms as it does not take advantage of rank topology. Default: not set
20220317 014102.898 INFO             PET0 index=   9                                MPIR_CVAR_ALLTOALL_SHORT_MSG : Adjusts the cut-off points at and below which the store and forward Alltoall algorithm is used for short messages. The default value is dependent upon the total number of ranks in the MPI communicator used for the MPI_Alltoall call and the Alltoall algorithm being selected. Defaults: If using one of the non-default send/recv algorithms on Aries, the defaults are: if communicator size <= 512, 2048 bytes if communicator size > 512 and <= 1024, 1024 bytes if communicator size > 1024 and <= 65536, 128 bytes if communicator size > 65536 and <= 131072, 64 bytes if communicator size > 131072 , 32 bytes
20220317 014102.898 INFO             PET0 index=  10                                MPIR_CVAR_ALLTOALLV_THROTTLE : Sets the per-process maximum number of outstanding Isends and Irecvs that can be posted concurrently for the optimized send/recv MPI_Alltoallv algorithm. For large messages, consider decreasing the throttle to 1 or 2 to improve performance. Defaults: 8
20220317 014102.898 INFO             PET0 index=  11                                   MPIR_CVAR_BCAST_ONLY_TREE : If set to 1, MPI_Bcast uses an smp-aware tree algorithm regardless of data size. The tree algorithm generally scales well to high processor counts on Cray XE systems. If set to 0, MPI_Bcast uses a variety of algorithms (tree, scatter, or ring) depending on message size and other factors. These other algorithms generally do not scale well when using more than 512 processors on Cray XE systems. Default: 1
20220317 014102.898 INFO             PET0 index=  12                             MPIR_CVAR_BCAST_INTERNODE_RADIX : Used to set the radix of the inter-node tree. This can be set to any integer value greater than or equal to 2. Default: 4
20220317 014102.898 INFO             PET0 index=  13                             MPIR_CVAR_BCAST_INTRANODE_RADIX : Used to set the radix of the intra-node tree. This can be set to any integer value greater than or equal to 2. Default: 4
20220317 014102.898 INFO             PET0 index=  14                                      MPIR_CVAR_COLL_OPT_OFF : If set, disables collective optimizations which use nondefault, architecture-specific algorithms for some MPI collective operations. By default, all collective optimized algorithms are enabled. To disable all collective optimized algorithms, set MPICH_COLL_OPT_OFF to 1. To disable optimized algorithms for selected MPI collectives, set the value to a comma-separated list of the desired collective names. Names are not case-sensitive. Any unrecognizable name is flagged with a warning message and ignored. For example, to disable the MPI_Allgather optimized collective algorithm, set MPICH_COLL_OPT_OFF=mpi_allgather. The following collective names are recognized: MPI_Allgather, MPI_Allgatherv, MPI_Allreduce, MPI_Alltoall, MPI_Alltoallv, MPI_Bcast, MPI_Gatherv, MPI_Scatterv, MPI_Igatherv, and MPI_Iallreduce. Default: Not enabled.
20220317 014102.898 INFO             PET0 index=  15                                         MPIR_CVAR_COLL_SYNC : If set, a Barrier is performed at the beginning of each specified MPI collective function. This forces all processes participating in that collective to sync up before the collective can begin. To disable this feature for all MPI collectives, set the value to 0. This is the default. To enable this feature for all MPI collectives, set the value to 1. To enable this feature for selected MPI collectives, set the value to a comma-separated list of the desired collective names. Names are not case-sensitive. Any unrecognizable name is flagged with a warning message and ignored. The following collective names are recognized: MPI_Allgather, MPI_Allgatherv, MPI_Allreduce, MPI_Alltoall, MPI_Alltoallv, MPI_Alltoallw, MPI_Bcast, MPI_Exscan, MPI_Gather, MPI_Gatherv, MPI_Reduce, MPI_Reduce_scatter, MPI_Scan, MPI_Scatter, and MPI_Scatterv. Default: Not enabled.
20220317 014102.898 INFO             PET0 index=  16                                 MPIR_CVAR_GATHERV_SHORT_MSG : Adjusts the cutoff point at and below which the optimized tree MPI_Gatherv algorithm is used instead of the optimized permission-to-send MPI_Gatherv algorithm. The cutoff is based on the average size of the variable MPI_Gatherv message sizes.  Default: 131072 bytes
20220317 014102.898 INFO             PET0 index=  17                                     MPIR_CVAR_REDUCE_NO_SMP : If set, MPI_Reduce uses an algorithm that is not smp-aware. This provides a consistent ordering of the specified reduce operation regardless of system configuration. Note:  This algorithm may not perform as well as the default smp-aware algorithms as it does not take advantage of rank topology. Default: not set
20220317 014102.898 INFO             PET0 index=  18                              MPIR_CVAR_SCATTERV_SYNCHRONOUS : The default, non-optimized ANL MPI_Scatterv algorithm uses asynchronous sends by default for communicator sizes less than 200,000 ranks. If set, this environment variable causes MPI_Scatterv to switch to using blocking sends, which may be beneficial in certain cases involving large data sizes or high process counts. For communicator sizes equal to or greater than 200,000 ranks, the blocking send algorithm is used by default. Default: not enabled
20220317 014102.898 INFO             PET0 index=  19                               MPIR_CVAR_SHARED_MEM_COLL_OPT : If set, the MPICH library will use the optimized shared- memory based design for collective operations. On Gemini and Aries systems, the supported collective operations are: MPI_Allreduce, MPI_Iallreduce, MPI_Barrier, and MPI_Bcast. To enable all available shared-memory optimizations, set MPICH_SHARED_MEM_COLL_OPT to 1. To enable this feature for a specific set of collective operations, set MPICH_SHARED_MEM_COLL_OPT to a comma- separated list of collective names. For example, to enable this optimization for MPI_Bcast only, set MPICH_SHARED_MEM_COLL_OPT=MPI_Bcast. To enable this optimization for MPI_Allreduce only, set MPICH_SHARED_MEM_COLL_OPT=MPI_Allreduce. Unsupported names are flagged with a warning message and ignored. Default: set
20220317 014102.898 INFO             PET0 index=  20                   MPIR_CVAR_ALLREDUCE_GPU_STAGING_THRESHOLD : If the sendbuf/recvbuf of an Allreduce operation is on GPU-resident memory regions, for payload sizes larger than this variable,  the Cray optimized staging implementation will be used.  By default, the staging implementation is used for all payload sizes.  This variable has no effect if MPICH_GPU_COLL_STAGING_AREA_OPT is set to 0. Default: 0
20220317 014102.898 INFO             PET0 index=  21                         MPIR_CVAR_GPU_COLL_STAGING_BUF_SIZE : This variable determines the size of the staging buffers used for some collectives (such as MPI_Allreduce) when sendbuf/recvbuf on GPU-resident buffers. This variable has no effect if  MPICH_GPU_COLL_STAGING_AREA_OPT is set to 0.   Default: 262144
20220317 014102.898 INFO             PET0 index=  22                                 MPIR_CVAR_GATHERV_SYNC_FREQ : Only applicable to the Gatherv permission-to-send algorithm.  Adjusts  the number of outstanding receives the root for Gatherv will allow.   Default is 16.
20220317 014102.898 INFO             PET0 index=  23                              MPIR_CVAR_GATHERV_MAX_TMP_SIZE : Only applicable to the Gatherv tree algorithm.  Sets the maximum amount of temporary memory GAtherv will allow a rank to allocate when using the tree-based algorithm.  Each rank allocates a different amount, with many allocating no extra memory.  If any rank requires more than this amount of temporary buffer space, a different algorithm is used. Default is 512MB.
20220317 014102.898 INFO             PET0 index=  24                             MPIR_CVAR_GATHERV_MIN_COMM_SIZE : Cray MPI offers two optimized Gatherv algorithms.  A tree algorithm for small messages and a permission-to-send send algorithm for larger messages.  Set  this value to the minimum communicator size to attempt use of either of the  Cray optimized Gatherv algorithms. Default is 64
20220317 014102.898 INFO             PET0 index=  25                                MPIR_CVAR_SCATTERV_SHORT_MSG : Adjusts the cutoff point at and below which the optimized tree MPI_Scatterv algorithm is used instead of the optimized staggered send algorithm.  The cutoff is in bytes, based on the average size of the variable MPI_Scatterv message sizes. Default behavior if unset is: For communicator sizes of <= 512 ranks, 2048 bytes For communicator sizes of > 512 ranks, 8192 bytes
20220317 014102.898 INFO             PET0 index=  26                                MPIR_CVAR_SCATTERV_SYNC_FREQ : Only applicable to the Scatterv staggered send algorithm.  Adjusts the number of outstanding sends the root for Scatterv will use. Default is 16.
20220317 014102.898 INFO             PET0 index=  27                             MPIR_CVAR_SCATTERV_MAX_TMP_SIZE : Only applicable to the Scatterv tree algorithm.  Sets the maximum amount of temporary memory Scatterv will allow a rank to allocate when using the tree-based algorithm.  Each rank allocates a different amount, with many allocating no extra memory.  If any rank requires more than this amount of temporary buffer space, a different algorithm is used. Default is 512MB.
20220317 014102.898 INFO             PET0 index=  28                            MPIR_CVAR_SCATTERV_MIN_COMM_SIZE : Cray MPI offers two optimized Scatterv algorithms.  A tree algorithm for small messages and a staggered send algorithm for larger messages.  Set this value to the minimum communicator size to attempt use of either of the Cray optimized Scatterv algorithms. Default is 64
20220317 014102.898 INFO             PET0 index=  29                           MPIR_CVAR_MPIIO_ABORT_ON_RW_ERROR : If set to enable, causes MPI-IO to abort immediately after issuing an error message if an I/O error occurs during a system read() or write() call. This applies only to I/O errors for system read() and write() calls made as a result of MPI I/O calls. It does not apply to I/O errors for other MPI I/O calls such as MPI_File_open(), nor does it apply to read() and write() calls made by means other than MPI I/O calls. Abort on error is not standard behavior. The MPI Standard specifies that the default error handling for MPI I/O calls is to return an error code to the application rather than aborting the application, but since errors on write or read are almost always unexpected and usually not recoverable, it may be preferable to abort as soon as the error is detected. Doing so does not allow any recovery, but does provide the most information about the error and terminates the job quickly. If the Cray Abnormal Termination Processing (ATP) feature is enabled, the abort will result in a full stack backtrace writte
20220317 014102.898 INFO             PET0 index=  30                MPIR_CVAR_MPIIO_AGGREGATOR_PLACEMENT_DISPLAY : This variable controls whether the placement of the aggregators will be displayed when a file is opened. The placement can be  controlled on a per file basis with the aggregator_placement_stride  hint. If set, displays the assignment of MPIIO collective buffering aggregators for reads/writes of a shared file, showing rank and node ID (nid). For example: Aggregator Placement for /lus/scratch/myfile RankReorderMethod=3  AggPlacementStride=-1 AGG    Rank       nid ----  ------  -------- 0       0  nid00578 1       4  nid00579 2       1  nid00606 3       5  nid00607 4       2  nid00578 5       6  nid00579 6       3  nid00606 7       7  nid00607 Default: not set
20220317 014102.898 INFO             PET0 index=  31                 MPIR_CVAR_MPIIO_AGGREGATOR_PLACEMENT_STRIDE : Partially controls to which nodes MPIIO collective buffering aggregators are assigned. See the notes below on the order of nodes. Network traffic and resulting I/O performance may be affected by the assignments. If set to 1, consecutive nodes are used. The number of aggregators assigned per node is controlled by the cb_config_list hint. By default, no more than one aggregator per node will be assigned if there are at least as many nodes as aggregators. If set to a value greater than 1, node selection is strided across the available nodes by this value. If the stride times the number of aggregators exceeds the number of nodes, the assignments will wrap around, which is usually not optimal for performance. If set to -1, node selection is strided across available nodes by the value of the number of nodes divided by the number of aggregators (integer division, minimum value of 1). The purpose is to spread out the nodes to reduce network congestion. Note:  The order of nodes can be shown by setting the MPICH_RANK
20220317 014102.898 INFO             PET0 index=  32                                    MPIR_CVAR_MPIIO_CB_ALIGN : Sets the default value for the cb_align hint. Files opened with MPI_File_open wil have this value for the cb_align hint unless the hint is set on a per file basis with either the MPICH_MPIIO_HINTS environment variable or from within a program with the MPI_Info_set() call. Note:  Only MPICH_MPIIO_CB_ALIGN == 2 is fully supported. Other values are for internal testing only. Default: 2
20220317 014102.898 INFO             PET0 index=  33                                MPIR_CVAR_MPIIO_DVS_MAXNODES : Note:  This environment variable in relevant only for file systems accessed from Cray system compute nodes via DVS server nodes; e.g. GPFS or PANFS. As described in the dvs(5) man page, the environment variable DVS_MAXNODES can be used to set the stripe width— that is, the number of DVS server nodes—used to access a file in "stripe parallel mode." For most files, and especially for small files, setting DVS_MAXNODES to 1 ("cluster parallel mode") is preferred. The MPICH_MPIIO_DVS_MAXNODES environment variable enables you to leave DVS_MAXNODES set to 1 and then use MPICH_MPIIO_DVS_MAXNODES to temporarily override DVS_MAXNODES when it is advantageous to specify wider striping for files being opened by the MPI_File_open() call. The range of values accepted by MPICH_MPIIO_DVS_MAXNODES goes from 1 to the number of server nodes specified on the mount with the nnodes mount option. DVS_MAXNODES is not set by default. Therefore, for MPICH_MPIIO_DVS_MAXNODES to have any effect, DVS_MAXNODES must be defined before p
20220317 014102.898 INFO             PET0 index=  34                                       MPIR_CVAR_MPIIO_HINTS : If set, override the default value of one or more MPI I/O hints. This also overrides any values that were set by using calls to MPI_Info_set in the application code. The new values apply to the file the next time it is opened using an MPI_File_open() call. After the MPI_File_open() call, subsequent MPI_Info_set calls can be used to pass new MPI I/O hints that take precedence over some of the environment variable values. Other MPI I/O hints such as striping_factor, striping_unit, cb_nodes, and cb_config_list cannot be changed after the MPI_File_open() call, as these are evaluated and applied only during the file open process. An MPI_File_close call followed by an MPI_File_open call can be used to restart the MPI I/O hint evaluation process. The syntax for this environment variable is a comma- separated list of specifications. Each individual specification is a pathname_pattern followed by a colon- separated list of one or more key=value pairs. In each key=value pair, the key is the MPI-IO hint name, and the v
20220317 014102.898 INFO             PET0 index=  35                               MPIR_CVAR_MPIIO_HINTS_DISPLAY : If set, causes rank 0 in the participating communicator to display the names and values of all MPI-IO hints that are set for the file being opened with the MPI_File_open call. It also displays relevant environment variables whether or not MPICH_ENV_DISPLAY is set. Default: not enabled.
20220317 014102.898 INFO             PET0 index=  36                               MPIR_CVAR_MPIIO_MAX_NUM_IRECV : When MPIIO collective buffering is used, this environment variable controls the number of outstanding MPI_Irecv calls allowed before an MPI_Waitall is done. Default: 50
20220317 014102.898 INFO             PET0 index=  37                               MPIR_CVAR_MPIIO_MAX_NUM_ISEND : When MPIIO collective buffering is used, this environment variable controls the number of outstanding MPI_Isend calls allowed before an MPI_Waitall is done. Default: 50
20220317 014102.898 INFO             PET0 index=  38                              MPIR_CVAR_MPIIO_MAX_SIZE_ISEND : When MPIIO collective buffering is used, this environment variable limits MPI_Isend by the amount of data being sent rather than by the number of calls. Default: 10485760 bytes
20220317 014102.898 INFO             PET0 index=  39                                       MPIR_CVAR_MPIIO_STATS : If set to 1, a summary of file write and read access patterns is written by rank 0 to stderr. This information provides some insight into how I/O performance may be improved. The information is provided on a per-file basis and is written when the file is closed. It does not provide any timing information. If set to 2, a set of data files are written to the working directory, one file for each rank, with the filename prefix specified by the MPICH_MPIIO_STATS_FILE environment variable. The data is in comma-separated values (CSV) format, which can be summarized with the cray_mpiio_summary script in the /opt/cray/mpt/version/gni/bin directory. Additional example scripts are provided in that directory to further process and display the data. Only enabled if MPIR_CVAR_MPIIO_CB_ALIGN=2. Default: not set
20220317 014102.898 INFO             PET0 index=  40                                  MPIR_CVAR_MPIIO_STATS_FILE : Specifies the filename prefix for the set of data files written when MPICH_MPIIO_STATS is set to 2. The filename prefix may be a full absolute pathname or a relative pathname. Summary plots of these files can be generated using the cray_mpiio_summary script from the /opt/cray/mpt/version/gni/bin directory. Other example scripts for post-processing this data can also be found in /opt/cray/mpt/version/gni/bin. Default: _cray_mpiio_stats_
20220317 014102.898 INFO             PET0 index=  41                         MPIR_CVAR_MPIIO_STATS_INTERVAL_MSEC : Specifies the time interval in milliseconds for each MPICH_MPIIO_STATS data point. Default: 250
20220317 014102.898 INFO             PET0 index=  42                                      MPIR_CVAR_MPIIO_TIMERS : If set to 0, or not set at all, no timing data is collected. If set to 1, timing data for different phases in MPI-IO is collected locally by each MPI process and then during MPI_File_close the data is consolidated and printed. Some timing data is displayed in seconds, other data is displayed in clock ticks, possibly scaled down. Also see MPICH_MPIIO_TIMERS_SCALE The relative values of the reported times are more important to the analysis than the absolute time. More detailed information about MPI-IO performance can be obtained by using the MPICH_MPIIO_STATS feature and by using the CrayPat and Apprentice2 Timeline Report of I/O bandwidth. Only enabled if MPIR_CVAR_MPIIO_CB_ALIGN=2. Default: 0
20220317 014102.898 INFO             PET0 index=  43                                MPIR_CVAR_MPIIO_TIMERS_SCALE : Specifies the power of 2 to use to scale the times reported by MPICH_MPIIO_TIMERS.  The raw times are collected in clock ticks. This generally is a very large number and reducing all the times by the same scaling factor makes for a more compact display. If set to 0, or not set at all, MPI-IO automatically determines a scaling factor to limit the report times to 9 or fewer digits. This auto-determined value is displayed.  To make run to run comparisons, you can set the scaling factor to your preferred value. Default: 0
20220317 014102.898 INFO             PET0 index=  44                                  MPIR_CVAR_MPIIO_TIME_WAITS : If set to non-zero, time how long this rank has to wait for other ranks to catch up.  This separates true metadata time from imbalance time. This is disabled when MPICH_MPIIO_TIMERS is not set.  Otherwise it defaults to 1. Default: 1
20220317 014102.898 INFO             PET0 index=  45                          MPIR_CVAR_MPIIO_WRITE_EXIT_BARRIER : If set to non-zero, collective write's will barrier on exit Default: 1
20220317 014102.898 INFO             PET0 index=  46                               MPIR_CVAR_MPIIO_DS_WRITE_CRAY : If set to non-zero, collective write's with data sieving will be optimized  Default: 1
20220317 014102.899 INFO             PET0 index=  47                         MPIR_CVAR_MPIIO_OFI_STARTUP_CONNECT : If set to enable, causes MPI-IO to establish connections between ranks and aggregators during the openning of the file to be used for MPIIO collective operations
20220317 014102.899 INFO             PET0 index=  48                MPIR_CVAR_MPIIO_OFI_STARTUP_NODES_AGGREGATOR : If MPIIO_OFI_STARTUP_CONNECT is enabled, this specifies the number of nodes that will concurrently attempt to connext to each aggrgator. Each rank on each node will establish its own connection to the aggregator. Default: 2
20220317 014102.899 INFO             PET0 index=  49                                           MPIR_CVAR_DPM_DIR : Sets the directory to use for MPI port name publishing in the  file-based nameserv implementation, as well as publishing the  credential obtained from libdrc.
20220317 014102.899 INFO             PET0 index=  50                               MPIR_CVAR_SINGLE_HOST_ENABLED : If true and if running as a single node, then inter-node communications is never initialized. The helps prevent the use of scarce hardware resources.
20220317 014102.899 INFO             PET0 index=  51                                       MPIR_CVAR_OFI_VERBOSE : If set to 1, enable verbose output about the CH4 OFI device. If set to 2 or higher, enable more verbose output.
20220317 014102.899 INFO             PET0 index=  52                                 MPIR_CVAR_OFI_XRCD_BASE_DIR : Specifies the base directory for the XRCD file.  This is only  used if the job is not launched via PALS launcher.  If launched  via PALS, we use the private, temporary PALS_SPOOL_DIR directory. Default: /tmp
20220317 014102.899 INFO             PET0 index=  53                                   MPIR_CVAR_OFI_NIC_VERBOSE : If set to 1 or higher, enables verbose output about the CH4 OFI NIC selection.   Non-fatal warnings are also displayed.  If set to 2 or higher, each rank in  the job will display it's NIC selection.  If set to 3 or higher, more verbose  output is enabled (for debug purposes).
20220317 014102.899 INFO             PET0 index=  54                                    MPIR_CVAR_OFI_NIC_POLICY : Specifies the rank-to-nic policy.  The supported options are:  NUMA | GPU | BLOCK | ROUND-ROBIN | USER
20220317 014102.899 INFO             PET0 index=  55                                   MPIR_CVAR_OFI_NIC_MAPPING : Specifies the precise rank-to-NIC mapping to use on each node.  This is  required if the NIC_POLICY is set to USER.  Each local rank must have a NIC mapping assigned by this env variable. If there are fewer MPI ranks on any  node, that part of the MAPPING string will be ignored.  For example: To assign local ranks 0,16,32,48 to NIC 0, and remaining ranks to NIC 1 MPICH_OFI_NIC_MAPPING="0:0,16,32,48; 1:1-15,17-31,33-47,49-63"
20220317 014102.899 INFO             PET0 index=  56                                      MPIR_CVAR_OFI_NUM_NICS : If set, specifies the number of NICs the job can use on each node. By default, when multiple NICs per node are available, MPI attempts to use them all.  If using fewer NICs is desired, this env variable  can be set to indicate the maximum number of NICs per node MPI will  use. By default, consecutive NICs are used, starting with index 0.   To limit use to 1 NIC/node, use : export MPICH_OFI_NUM_NICS=1 To limit use to 2 NICs/node, use: export MPICH_OFI_NUM_NICS=2 To specify alternative NIC index values, you may indicate the desired index values by adding a colon, followed by the NIC indexes.  Use  quotes so the shell doesn't interfere.  For example: To limit use to 1 NIC/node, index 1    : export MPICH_OFI_NUM_NICS="1:1" To limit use to 2 NICs/node, index 1&3 : export MPICH_OFI_NUM_NICS="2:1,3"
20220317 014102.899 INFO             PET0 index=  57                        MPIR_CVAR_OFI_SKIP_NIC_SYMMETRY_TEST : If set to 1, CrayMPICH will bypass it's check for NIC symmetry on all nodes in the job. By default, this check is run during MPI_Init to  make sure all the nodes in the job have the same number of NICs available.
20220317 014102.899 INFO             PET0 index=  58                               MPIR_CVAR_OFI_STARTUP_CONNECT : If set to 1, CrayMPICH will create COMM_WORLD static OFI connections to every other rank during MPI_Init.
20220317 014102.899 INFO             PET0 index=  59                           MPIR_CVAR_OFI_RMA_STARTUP_CONNECT : If set to 1, CrayMPICH RMA will create static OFI connections to every other rank on the same node during MPI_Init. This will also enable MPIR_CVAR_OFI_STARTUP_CONNECT.
20220317 014102.899 INFO             PET0 index=  60                                MPIR_CVAR_OFI_DEFAULT_TCLASS : If set, CrayMPICH will assign the requested default TC (traffic class) to the job domain.  By default, all endpoints inherit this TC.
20220317 014102.899 INFO             PET0 index=  61                                 MPIR_CVAR_OFI_TCLASS_ERRORS : Determines how CrayMPICH responds to traffic class errors.  Valid values are "warn", "silent" or "error".
20220317 014102.899 INFO             PET0 index=  62                                  MPIR_CVAR_OFI_CXI_PID_BASE : Set to the base value that MPI will use to assign portal IDs for the CXI provider.  Max PID value is 510.  MPI uses PID_BASE+lrank as the unique value per rank per node.  This is only applicable to SS11.
20220317 014102.899 INFO             PET0 index=  63                          MPIR_CVAR_OFI_USE_SCALABLE_STARTUP : Set to false (0) to bypass the scalable startup feature for CXI. This is only applicable to SS11.
20220317 014102.899 INFO             PET0 index=  64                                       MPIR_CVAR_UCX_VERBOSE : If set to 1, enable verbose output about the CH4 UCX device. If set to 2 or higher, enable more verbose output.
20220317 014102.899 INFO             PET0 index=  65                                  MPIR_CVAR_UCX_RC_MAX_RANKS : By default, CrayMPI selects either the UCX RC or UD protocols for inter-node messaging.  If a job is launched with MPICH_UCX_RC_MAX_RANKS  ranks or fewer, the RC protocol is selected.  If more ranks are launched,  the UCX UD protocol is chosen.  The RC protocol does not scale well due to the resource requirements.  Note this default can be overridden by setting the UCX_TLS environment variable.
20220317 014102.899 INFO             PET0 index=  66                              MPIR_CVAR_SMP_SINGLE_COPY_MODE : If non-null, choose an on-node copy mode for large messages. This variable can be set to XPMEM, CMA, or NONE. By default, Cray MPICH will look to use XPMEM. If XPMEM is requested, but not available, Cray MPICH will attempt to use CMA. (NOTE: Cray MPICH does not support CMA currently. The env. variables and CVARs are set up to allow CMA usage with future versions of Cray MPICH) If both XPMEM and CMA cannot be used, Cray MPICH will fall back to using the ANL shared memory implementation (2-copy) Default: NULL
20220317 014102.899 INFO             PET0 index=  67                              MPIR_CVAR_SMP_SINGLE_COPY_SIZE : Specifies the minimum message size in bytes to consider for single-copy transfers for on-node messages. This applies only to the SMP (on-node shared memory) device. The value is interpreted as bytes, unless the string ends in a K, which indicates kilobytes, or M, which indicates megabytes. Default: 8192
20220317 014102.899 INFO             PET0 index=  68                       MPIR_CVAR_SHM_PROGRESS_MAX_BATCH_SIZE : Adjusts the maximum number of on-node requests that can be processed in a single batch. Higher values for the maximum batch size can lower the overhead due to entering the progress engine, but can also  delay the processing of off-node message requests. Default is 8.
20220317 014102.899 INFO             PET0 index=  69                                MPIR_CVAR_CH4_RMA_THREAD_HOT : If true, the big lock is disabled for certain RMA operations
20220317 014102.899 INFO             PET0 index=  70                                    MPIR_CVAR_ABORT_ON_ERROR : If set, causes MPICH to abort and produce a core dump when MPICH detects an internal error. Note that the core dump size limit (usually 0 bytes by default) must be reset to an appropriate value in order to enable coredumps. Default: Not enabled.
20220317 014102.899 INFO             PET0 index=  71                                   MPIR_CVAR_CPUMASK_DISPLAY : If set, causes each MPI rank in the job to display its CPU affinity bitmask. Note that this reports only the CPU affinity masks for the MPI ranks; if you have a hybrid program, it does not provide any thread information. The bitmask is read from right to left, meaning the value in the rightmost position corresponds to CPU 0 on the node.
20220317 014102.899 INFO             PET0 index=  72                                       MPIR_CVAR_ENV_DISPLAY : If set, causes rank 0 to display all MPICH environment variables and their current settings at MPI initialization time. If two or more nodes are used, MPICH/GNI environment settings are also included in the listing. Default: Not enabled.
20220317 014102.899 INFO             PET0 index=  73                                  MPIR_CVAR_OPTIMIZED_MEMCPY : Specifies which version of memcpy to use. Valid values are: 0         Use the system (glibc) version of memcpy. 1         Use an optimized version of memcpy if one is available for the processor being used. In this release, an optimized version of memcpy() is available only for Intel processors. 2         Use a highly optimized version of memcpy that provides better performance in some areas but may have performance regressions in other areas, if one is available for the processor being used. In this release, a highly optimized version of memcpy() is available only for Intel Haswell processors. MPICH_OPTIMIZED_MEMCPY is overridden by MPICH_USE_SYSTEM_MEMCPY. If MPICH_USE_SYSTEM_MEMCPY is set, MPICH_OPTIMIZED_MEMCPY is ignored and the system (glibc) version of memcpy() is used. Default: 1
20220317 014102.899 INFO             PET0 index=  74                                     MPIR_CVAR_STATS_DISPLAY : If set to 1, a summary of MPI statistics, also available through the MPI Tools Interface, will be written by rank 0 to stderr. If set to 2, all ranks will produce an individualized statistics summary and write to file on a per-rank basis. The MPICH_STATS_FILE determines the prefix of the file to be used. This information may provide insight into how MPI performance may be improved. Default: 0
20220317 014102.899 INFO             PET0 index=  75                                   MPIR_CVAR_STATS_VERBOSITY : Specifies the verbosity of the MPI statistics summary. This information may provide insight into how MPI performance may be improved. Increase the value for more detailed summary. Default: 1 1        USER_BASIC  (default) 2        USER_DETAIL 3        USER_ALL 4        TUNER_BASIC 5        TUNER_DETAIL 6        TUNER_ALL 7        MPIDEV_BASIC 8        MPIDEV_DETAIL 9        MPIDEV_ALL
20220317 014102.899 INFO             PET0 index=  76                                        MPIR_CVAR_STATS_FILE : Specifies the filename prefix for the set of data files written when MPICH_STATS_DISPLAY is set to 2. The filename prefix may be a full absolute pathname or a relative pathname. Default: _cray_stats_
20220317 014102.899 INFO             PET0 index=  77                              MPIR_CVAR_RANK_REORDER_DISPLAY : If set, causes rank 0 to display which node each MPI rank resides in. The rank order can be manipulated via the MPICH_RANK_REORDER_METHOD environment variable or MPIR_CVAR_RANK_REORDER_METHOD control variable. Default: Not set
20220317 014102.899 INFO             PET0 index=  78                               MPIR_CVAR_RANK_REORDER_METHOD : Overrides the default MPI rank placement scheme. If this variable is not set, the default aprun launcher placement policy is used. The default policy for aprun is SMP-style placement. To display the MPI rank placement information, set MPICH_RANK_REORDER_DISPLAY. See manpage for more details. Default: 1, for SMP-style placement.
20220317 014102.899 INFO             PET0 index=  79                                 MPIR_CVAR_USE_SYSTEM_MEMCPY : Note:  This environment variable is deprecated and scheduled to be removed in a future release. Use MPICH_OPTIMIZED_MEMCPY instead. If set, use the system (glibc) version of memcpy(); otherwise, an optimized version of memcpy() may be used. Currently, an optimized version of memcpy() is available only for Intel processors. Default: Not set
20220317 014102.899 INFO             PET0 index=  80                                   MPIR_CVAR_VERSION_DISPLAY : If set, causes MPICH to display the CRAY MPICH version number as well as build date information. Default: Not enabled
20220317 014102.899 INFO             PET0 index=  81                          MPIR_CVAR_USE_GPU_STREAM_TRIGGERED : If set, causes MPICH to allow using stream triggered GPU communication operations. Default: Not enabled
20220317 014102.899 INFO             PET0 index=  82                               MPIR_CVAR_NUM_MAX_GPU_STREAMS : If set, causes MPICH to allow using the set maximum number of streams concurrently in the stream triggered GPU communication operations. Default: 26
20220317 014102.899 INFO             PET0 index=  83                                  MPIR_CVAR_MEMCPY_MEM_CHECK : If set, enables a check of the memcpy() source and destination areas. If they overlap, the application asserts with an error message listing the file, line, and memory range overlap. If this error is found, correct it either by changing the memory ranges or possibly by using MPI_IN_PLACE. Default: not set (off)
20220317 014102.899 INFO             PET0 index=  84                                     MPIR_CVAR_MSG_QUEUE_DBG : If set, turns on TotalView Message Queue Debugging support so that message queues are tracked in the TotalView debugger and a message queue graph can be generated. Enabling this feature degrades performance. Default: not enabled.
20220317 014102.899 INFO             PET0 index=  85                             MPIR_CVAR_NO_BUFFER_ALIAS_CHECK : If set, the buffer alias error check for collectives is disabled. The MPI standard does not allow aliasing of type OUT or INOUT parameters on the same collective function call. The use of MPI_IN_PLACE is required in these scenarios. A new check was added in MPT 5.2 to detect this condition and report the error. To bypass this check, set MPICH_NO_BUFFER_ALIAS_CHECK to any value. Default: not set
20220317 014102.899 INFO             PET0 index=  86                                MPIR_CVAR_ALLOC_MEM_AFFINITY : Controls the affinity of the memory region allocated by the MPI_Alloc_mem() or MPI_Win_allocate() operations. On systems that do not offer High Bandwidth Memory capabilities, (such as Intel Haswell or Broadwell processors), this env. variable is ignored and memory affinity is set to DDR. On Phi processors, such as KNL (and KNH, KNP in the future), this env. variable allows users to specifically request the memory returned by MPI_Alloc_mem() and MPI_Win_allocate() to be bound to either DDR, or the MCDRAM. Users can request a specific page size or memory binding policy via the MPICH_ALLOC_MEM_POLICY and MPICH_ALLOC_MEM_PG_SZ env. variables. Default: SYS_DEFAULT
20220317 014102.899 INFO             PET0 index=  87                             MPIR_CVAR_INTERNAL_MEM_AFFINITY : Controls the affinity of internal memory regions allocated by the MPI library. This variable currently affects the memory affinity of the mail-boxes used for off-node communication, and the shared-memory regions that are used for on-node pt2pt and collective ops. On systems that do not offer High Bandwidth Memory capabilities, (such as Intel Haswell or Broadwell processors), this env. variable is ignored and memory affinity is set to DDR. On Phi processors, such as KNL (and KNH, KNP in the future), this env. variable allows users to specifically request the internal memory regions used by the MPI library to be bound to either DDR, or the MCDRAM. The default affinity settings will be governed by the system defaults. For example, on a KNL system configured in the Quad/Flat mode, if the job is run with numactl --membind=1, all of MPI's internal memory will be bound to MCDRAM if this variable is not set. Default: SYS_DEFAULT
20220317 014102.899 INFO             PET0 index=  88                                  MPIR_CVAR_ALLOC_MEM_POLICY : Controls the memory affinity policy on systems with specialized memory hardware. By default, the memory policy is set to "{P}referred". Other accepted values are "{M}andatory" and "{I}"nterleave. Default: Preferred
20220317 014102.899 INFO             PET0 index=  89                                   MPIR_CVAR_ALLOC_MEM_PG_SZ : Controls the page size for the MPI_Alloc_mem() and MPI_Win_allocate() operations. This parameters defaults to 4KB base pages. The supported values are 2M, 4M, 8M, 16M, 32M, 64M, 128M, 256M, 512M, 1G and 2G. Default: 4096
20220317 014102.899 INFO             PET0 index=  90                                   MPIR_CVAR_OPT_THREAD_SYNC : Controls the mechanism used to implement thread-synchronization inside the Cray MPICH library. If set to 1, an optimized synchronization implementation is used. If set to 0, Cray MPICH falls back to using a pthread_mutex-based thread-synchronization implementation. Default: 1
20220317 014102.899 INFO             PET0 index=  91                                 MPIR_CVAR_THREAD_YIELD_FREQ : Determines how often a thread yields while waiting to acquire a lock in the new Cray optimized locking impl. This variable has no effect if MPICH_CRAY_OPT_THREAD_SYNC is 0. Default: 10000
20220317 014102.899 INFO             PET0 index=  92                                   MPIR_CVAR_MEM_DEBUG_FNAME : If set, the MPI library creates new files with the user specified name and writes various important memory statistics into these files. This information can be useful for post-processing. Users can set MPICH_MEM_DEBUG_FNAME to any suitable string. The resulting files are named as "string".<pid>.<MPI-rank>. For example, if MPICH_MEM_DEBUG_FNAME is set to "MEM_DBG_MSGS" the debug file for rank0 will be named "MEM_DBG_MSGS.<pid>.0 and written to the user's current working directory. If this flag is not set, the MPI library will redirect all the debug messages to stderr. Default: unset (disabled)
20220317 014102.899 INFO             PET0 index=  93                                   MPIR_CVAR_MALLOC_FALLBACK : Set the policy for fallback behavior when attempting to allocate large pages for internal buffers and insufficient large pages are available to satisfy the request. If set to enabled, MPICH falls back to using malloc in such cases. Default: not enabled (process fails and job terminates if insufficient large pages are available to satisfy the request)
20220317 014102.899 INFO             PET0 index=  94                              MPIR_CVAR_NO_DIRECT_GPU_ACCESS : If true, IPC and NIC-GPU Direct Access capabilities are not used for GPU-to-GPU transfers. This is mainly used for debugging.  Default: 0
20220317 014102.899 INFO             PET0 index=  95                                      MPIR_CVAR_G2G_PIPELINE : If nonzero, the device-host and network transfers will be overlapped to pipeline GPU-to-GPU transfers. Setting MPICH_G2G_PIPELINE to N will allow N GPU-to-GPU messages to be efficiently in-flight at any one time. If MPICH_G2G_PIPELINE is nonzero but MPICH_GPU_SUPPORT_ENABLED is disabled, MPICH_G2G_PIPELINE will be turned off. If MPICH_GPU_SUPPORT_ENABLED is enabled but MPICH_G2G_PIPELINE is 0, the default value is set to 8.  The pipeline-based implementation is being carried over from  CH3/Aries implementation. We think the pipelined-based implementation will not be necessary on SS-11 systems.  This logic may get dropped in the future.  Default: not set
20220317 014102.899 INFO             PET0 index=  96                               MPIR_CVAR_GPU_SUPPORT_ENABLED : If set, allows the MPI application to pass GPU pointers directly to MPI communication functions.  Default: not set
20220317 014102.899 INFO             PET0 index=  97                MPIR_CVAR_GPU_MANAGED_MEMORY_SUPPORT_ENABLED : If set, allows the MPI application to pass pointers allocated via  GPU managed memory allocators to MPI communication functions. Default: not set
20220317 014102.899 INFO             PET0 index=  98                         MPIR_CVAR_GPU_COLL_STAGING_AREA_OPT : If set, allows Cray MPI to use pre-registered CPU-attached memory to optimize collective ops.  Default: set
20220317 014102.899 INFO             PET0 index=  99               MPIR_CVAR_GPU_COLL_REGISTER_SHARED_MEM_REGION : If set, allows Cray MPI to registere CPU-attached shared memory regions  used to optimize collective ops.  Default: set
20220317 014102.899 INFO             PET0 index= 100                                   MPIR_CVAR_GPU_IPC_ENABLED : If set to 1, Cray MPICH will use GPU IPC to move data between GPU devices that are within the same node. If set to 0, Cray MPICH will use staging buffers on the host to implement data transfer between GPU devices that are within the same node. If MPIR_CVAR_GPU_SUPPORT_ENABLED is set, Cray MPICH  automatically attempts to enable the IPC optimizations.   If MPIR_CVAR_NO_DIRECT_GPU_ACCESS is set, IPC optimizations are disabled.  Default: -1
20220317 014102.899 INFO             PET0 index= 101                                 MPIR_CVAR_GPU_IPC_THRESHOLD : Specifies the minimum message size in bytes to consider for single-copy transfers between GPU devices that are within the same compute node. The value is interpreted as bytes, unless the string ends in a K, which indicates kilobytes, or M, which indicates megabytes.
20220317 014102.899 INFO             PET0 index= 102                                  MPIR_CVAR_GPU_IPC_PROTOCOL : Specifies the protocol used for large msg on-node, inter-GPU  data transfers via GPU IPC.  Valid options are RPUT or RGET.  Default: RGET
20220317 014102.899 INFO             PET0 index= 103                                 MPIR_CVAR_GPU_NO_ASYNC_COPY : If set, disables the use of asynchronous memcpy's for on-node GPU-aware transfers. Default: 0
20220317 014102.899 INFO             PET0 index= 104                                      MPIR_CVAR_ENABLE_YAKSA : If set, enables the use of the YAKSA datatype engine for packing and unpacking non-contiguous buffers in both CPU- and GPU-attached memory. Default: 0
20220317 014102.899 INFO             PET0 index= 105                              MPIR_CVAR_GPU_EAGER_DEVICE_MEM : If set to 1, GPU-attached memory will be used for the eager staging buffers for short on-node messages where at least the sending buffer is GPU-attached. If set to 0, this optimization is disabled and we fall-back to using staging buffers on the CPU for small message  intra-node device-host and device-device MPI ops. Default: 1
20220317 014102.899 INFO             PET0 index= 106                       MPIR_CVAR_GPU_EAGER_REGISTER_HOST_MEM : If set to 1, enables MPI to request POSIX  eager memory to be registered with the GPU runtime.  This is an optimization to improve the performance of  small message intra-node, inter-GPU MPI ops that use  POSIX shared memory as bounce buffers.  If set to 0, this optimization is disabled and we rely on GTL to lock/unlock host memory regions for a given on-node, small msg inter-GPU transfer.  Default: 1
20220317 014102.899 INFO             PET0 index= 107                          MPIR_CVAR_GPU_ALLREDUCE_USE_KERNEL : If set, adds a hint that the use of device kernels for reduction operations is desired. MPI is not guaranteed to use a device kernel for all reduction operations. Default: 0
20220317 014102.899 INFO             PET0 index= 108                                   MPIR_CVAR_RMA_MAX_PENDING : Determines how many RMA operation may be outstanding at any time over libfabrics. RMA operations beyond this max will be queued and only issued as pending operations complete. Default: 64
20220317 014102.899 INFO             PET0 index= 109                                MPIR_CVAR_RMA_SHM_ACCUMULATE : If set to 1, enables SHM accumulate operations. If set to 0, disables SHM accumulate operations. It also sets the default for the window hint "disable_shm_accumulate".  Default: 1
20220317 014102.899 INFO             PET0 --- VMK::logSystem() end ---------------------------------
20220317 014102.899 INFO             PET0 main: --- VMK::log() start -------------------------------------
20220317 014102.899 INFO             PET0 main: vm located at: 0x1bbfae0
20220317 014102.899 INFO             PET0 main: petCount=6 localPet=0 mypthid=23297668698240 currentSsiPe=0
20220317 014102.899 INFO             PET0 main: Current system level affinity pinning for local PET:
20220317 014102.899 INFO             PET0 main:  SSIPE=0
20220317 014102.899 INFO             PET0 main: Current system level OMP_NUM_THREADS setting for local PET: 128
20220317 014102.899 INFO             PET0 main: ssiCount=1 localSsi=0
20220317 014102.899 INFO             PET0 main: mpionly=1 threadsflag=0
20220317 014102.899 INFO             PET0 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220317 014102.899 INFO             PET0 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220317 014102.899 INFO             PET0 main:  PE=0 SSI=0 SSIPE=0
20220317 014102.899 INFO             PET0 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220317 014102.899 INFO             PET0 main:  PE=1 SSI=0 SSIPE=1
20220317 014102.899 INFO             PET0 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220317 014102.899 INFO             PET0 main:  PE=2 SSI=0 SSIPE=2
20220317 014102.899 INFO             PET0 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220317 014102.899 INFO             PET0 main:  PE=3 SSI=0 SSIPE=3
20220317 014102.899 INFO             PET0 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220317 014102.899 INFO             PET0 main:  PE=4 SSI=0 SSIPE=4
20220317 014102.899 INFO             PET0 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220317 014102.899 INFO             PET0 main:  PE=5 SSI=0 SSIPE=5
20220317 014102.899 INFO             PET0 main: --- VMK::log() end ---------------------------------------
20220317 014102.899 INFO             PET0 Executing 'userm1_setvm'
20220317 014102.900 INFO             PET0 Executing 'userm1_register'
20220317 014102.900 INFO             PET0 Executing 'userm2_setvm'
20220317 014102.900 INFO             PET0 Executing 'userm2_register'
20220317 014102.901 INFO             PET0 Entering 'user1_run'
20220317 014102.901 INFO             PET0 model1: --- VMK::log() start -------------------------------------
20220317 014102.901 INFO             PET0 model1: vm located at: 0x1c1b840
20220317 014102.901 INFO             PET0 model1: petCount=6 localPet=0 mypthid=23297668698240 currentSsiPe=0
20220317 014102.901 INFO             PET0 model1: Current system level affinity pinning for local PET:
20220317 014102.901 INFO             PET0 model1:  SSIPE=0
20220317 014102.901 INFO             PET0 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220317 014102.901 INFO             PET0 model1: ssiCount=1 localSsi=0
20220317 014102.901 INFO             PET0 model1: mpionly=1 threadsflag=0
20220317 014102.901 INFO             PET0 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220317 014102.901 INFO             PET0 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220317 014102.901 INFO             PET0 model1:  PE=0 SSI=0 SSIPE=0
20220317 014102.901 INFO             PET0 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220317 014102.901 INFO             PET0 model1:  PE=1 SSI=0 SSIPE=1
20220317 014102.901 INFO             PET0 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220317 014102.901 INFO             PET0 model1:  PE=2 SSI=0 SSIPE=2
20220317 014102.901 INFO             PET0 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220317 014102.901 INFO             PET0 model1:  PE=3 SSI=0 SSIPE=3
20220317 014102.901 INFO             PET0 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220317 014102.901 INFO             PET0 model1:  PE=4 SSI=0 SSIPE=4
20220317 014102.901 INFO             PET0 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220317 014102.901 INFO             PET0 model1:  PE=5 SSI=0 SSIPE=5
20220317 014102.901 INFO             PET0 model1: --- VMK::log() end ---------------------------------------
20220317 014102.901 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220317 014103.265 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220317 014103.568 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220317 014103.872 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220317 014104.175 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220317 014104.478 INFO             PET0 Exiting 'user1_run'
20220317 014104.499 INFO             PET0 Entering 'user2_run'
20220317 014104.499 INFO             PET0 model2: --- VMK::log() start -------------------------------------
20220317 014104.499 INFO             PET0 model2: vm located at: 0x1c1b9c0
20220317 014104.499 INFO             PET0 model2: petCount=2 localPet=0 mypthid=23297668698240 currentSsiPe=0
20220317 014104.499 INFO             PET0 model2: Current system level affinity pinning for local PET:
20220317 014104.499 INFO             PET0 model2:  SSIPE=0
20220317 014104.499 INFO             PET0 model2: Current system level OMP_NUM_THREADS setting for local PET: 3
20220317 014104.499 INFO             PET0 model2: ssiCount=1 localSsi=0
20220317 014104.499 INFO             PET0 model2: mpionly=1 threadsflag=0
20220317 014104.499 INFO             PET0 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220317 014104.499 INFO             PET0 model2: PET=0 lpid=0 tid=0 pid=0 peCount=3 accCount=0
20220317 014104.499 INFO             PET0 model2:  PE=0 SSI=0 SSIPE=0
20220317 014104.499 INFO             PET0 model2:  PE=1 SSI=0 SSIPE=1
20220317 014104.499 INFO             PET0 model2:  PE=2 SSI=0 SSIPE=2
20220317 014104.499 INFO             PET0 model2: PET=1 lpid=1 tid=0 pid=3 peCount=3 accCount=0
20220317 014104.499 INFO             PET0 model2:  PE=3 SSI=0 SSIPE=3
20220317 014104.499 INFO             PET0 model2:  PE=4 SSI=0 SSIPE=4
20220317 014104.499 INFO             PET0 model2:  PE=5 SSI=0 SSIPE=5
20220317 014104.499 INFO             PET0 model2: --- VMK::log() end ---------------------------------------
20220317 014104.499 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220317 014104.499 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220317 014104.499 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220317 014104.851 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220317 014104.851 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220317 014104.851 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220317 014105.173 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220317 014105.173 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220317 014105.173 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220317 014105.496 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220317 014105.496 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220317 014105.496 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220317 014105.818 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220317 014105.818 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220317 014105.818 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220317 014106.141 INFO             PET0  user2_run: All data correct.
20220317 014106.141 INFO             PET0 Exiting 'user2_run'
20220317 014106.141 INFO             PET0 Entering 'user1_run'
20220317 014106.141 INFO             PET0 model1: --- VMK::log() start -------------------------------------
20220317 014106.141 INFO             PET0 model1: vm located at: 0x1c1b840
20220317 014106.141 INFO             PET0 model1: petCount=6 localPet=0 mypthid=23297668698240 currentSsiPe=0
20220317 014106.141 INFO             PET0 model1: Current system level affinity pinning for local PET:
20220317 014106.141 INFO             PET0 model1:  SSIPE=0
20220317 014106.141 INFO             PET0 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220317 014106.141 INFO             PET0 model1: ssiCount=1 localSsi=0
20220317 014106.141 INFO             PET0 model1: mpionly=1 threadsflag=0
20220317 014106.141 INFO             PET0 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220317 014106.141 INFO             PET0 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220317 014106.141 INFO             PET0 model1:  PE=0 SSI=0 SSIPE=0
20220317 014106.141 INFO             PET0 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220317 014106.141 INFO             PET0 model1:  PE=1 SSI=0 SSIPE=1
20220317 014106.141 INFO             PET0 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220317 014106.141 INFO             PET0 model1:  PE=2 SSI=0 SSIPE=2
20220317 014106.141 INFO             PET0 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220317 014106.141 INFO             PET0 model1:  PE=3 SSI=0 SSIPE=3
20220317 014106.141 INFO             PET0 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220317 014106.141 INFO             PET0 model1:  PE=4 SSI=0 SSIPE=4
20220317 014106.141 INFO             PET0 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220317 014106.141 INFO             PET0 model1:  PE=5 SSI=0 SSIPE=5
20220317 014106.141 INFO             PET0 model1: --- VMK::log() end ---------------------------------------
20220317 014106.141 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220317 014106.444 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220317 014106.747 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220317 014107.051 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220317 014107.354 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220317 014107.657 INFO             PET0 Exiting 'user1_run'
20220317 014107.678 INFO             PET0 Entering 'user2_run'
20220317 014107.678 INFO             PET0 model2: --- VMK::log() start -------------------------------------
20220317 014107.678 INFO             PET0 model2: vm located at: 0x1c1b9c0
20220317 014107.678 INFO             PET0 model2: petCount=2 localPet=0 mypthid=23297668698240 currentSsiPe=0
20220317 014107.678 INFO             PET0 model2: Current system level affinity pinning for local PET:
20220317 014107.678 INFO             PET0 model2:  SSIPE=0
20220317 014107.678 INFO             PET0 model2: Current system level OMP_NUM_THREADS setting for local PET: 3
20220317 014107.678 INFO             PET0 model2: ssiCount=1 localSsi=0
20220317 014107.678 INFO             PET0 model2: mpionly=1 threadsflag=0
20220317 014107.678 INFO             PET0 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220317 014107.678 INFO             PET0 model2: PET=0 lpid=0 tid=0 pid=0 peCount=3 accCount=0
20220317 014107.678 INFO             PET0 model2:  PE=0 SSI=0 SSIPE=0
20220317 014107.678 INFO             PET0 model2:  PE=1 SSI=0 SSIPE=1
20220317 014107.678 INFO             PET0 model2:  PE=2 SSI=0 SSIPE=2
20220317 014107.678 INFO             PET0 model2: PET=1 lpid=1 tid=0 pid=3 peCount=3 accCount=0
20220317 014107.678 INFO             PET0 model2:  PE=3 SSI=0 SSIPE=3
20220317 014107.678 INFO             PET0 model2:  PE=4 SSI=0 SSIPE=4
20220317 014107.678 INFO             PET0 model2:  PE=5 SSI=0 SSIPE=5
20220317 014107.678 INFO             PET0 model2: --- VMK::log() end ---------------------------------------
20220317 014107.678 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220317 014107.678 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220317 014107.678 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220317 014108.001 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220317 014108.001 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220317 014108.001 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220317 014108.323 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220317 014108.323 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220317 014108.323 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220317 014108.645 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220317 014108.646 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220317 014108.646 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220317 014108.968 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220317 014108.968 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220317 014108.968 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220317 014109.290 INFO             PET0  user2_run: All data correct.
20220317 014109.290 INFO             PET0 Exiting 'user2_run'
20220317 014109.291 INFO             PET0  NUMBER_OF_PROCESSORS           6
20220317 014109.291 INFO             PET0  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220317 014109.291 INFO             PET0 Finalizing ESMF
20220317 014102.897 INFO             PET1 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220317 014102.897 INFO             PET1 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220317 014102.897 INFO             PET1 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220317 014102.897 INFO             PET1 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220317 014102.897 INFO             PET1 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220317 014102.897 INFO             PET1 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220317 014102.897 INFO             PET1 Running with ESMF Version   : v8.3.0b09-100-g5ff85a963f
20220317 014102.897 INFO             PET1 ESMF library build date/time: "Mar 17 2022" "01:27:56"
20220317 014102.897 INFO             PET1 ESMF library build location : /lfs/h1/emc/ptmp/Mark.Potts/gfortran_10.3.0_mpi_O_jedwards_pio_update2
20220317 014102.897 INFO             PET1 ESMF_COMM                   : mpi
20220317 014102.897 INFO             PET1 ESMF_MOAB                   : enabled
20220317 014102.897 INFO             PET1 ESMF_LAPACK                 : enabled
20220317 014102.897 INFO             PET1 ESMF_NETCDF                 : enabled
20220317 014102.897 INFO             PET1 ESMF_PNETCDF                : disabled
20220317 014102.897 INFO             PET1 ESMF_PIO                    : enabled
20220317 014102.897 INFO             PET1 ESMF_YAMLCPP                : enabled
20220317 014102.898 INFO             PET1 --- VMK::logSystem() start -------------------------------
20220317 014102.898 INFO             PET1 esmfComm=mpi
20220317 014102.898 INFO             PET1 isPthreadsEnabled=1
20220317 014102.898 INFO             PET1 isOpenMPEnabled=1
20220317 014102.898 INFO             PET1 isOpenACCEnabled=0
20220317 014102.898 INFO             PET1 isSsiSharedMemoryEnabled=1
20220317 014102.898 INFO             PET1 ssiCount=1 peCount=6
20220317 014102.898 INFO             PET1 PE=0 SSI=0 SSIPE=0
20220317 014102.898 INFO             PET1 PE=1 SSI=0 SSIPE=1
20220317 014102.898 INFO             PET1 PE=2 SSI=0 SSIPE=2
20220317 014102.898 INFO             PET1 PE=3 SSI=0 SSIPE=3
20220317 014102.898 INFO             PET1 PE=4 SSI=0 SSIPE=4
20220317 014102.898 INFO             PET1 PE=5 SSI=0 SSIPE=5
20220317 014102.898 INFO             PET1 --- VMK::logSystem() MPI Control Variables ---------------
20220317 014102.898 INFO             PET1 index=   0          MPIR_CVAR_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE : specifies the cutoff size of the send buffer (in bytes) above which the reduce_scatter functions attempt to use the pairwise exchange algorithm.  In addition, the op must be commutative and the communicator size < MPIR_CVAR_REDUCE_SCATTER_MAX_COMMSIZE for the pairwise exchange algorithm to be used.
20220317 014102.898 INFO             PET1 index=   1                       MPIR_CVAR_REDUCE_SCATTER_MAX_COMMSIZE : specifies the max communicator size that will trigger use of the pairwise exchange algorithm, provided the op is commutative.  The pairwise exchange algorithm is not well suited for scaling to high process counts, so for larger communicators, a recursive halving algorithm is used instead.
20220317 014102.898 INFO             PET1 index=   2                           MPIR_CVAR_NETWORK_BUFFER_COLL_OPT : If set to 1, the MPICH library will use the optimized shared- memory based "network buffer" design for collective operations.  This feature is closely tied to the shared-memory collective optimization available in Cray MPICH. If enabled, the shared-memory buffer is also registered with the NIC and can be used directly to  perform off-node transfers, bypassing the Nemesis channel layer.  This feature is disabled if MPICH_SHARED_MEM_COLL_OPT  is set to 0. Currently, this optimization is only available  for the MPI_Bcast collective operation. To disable this feature,  set MPICH_NETWORK_BUFFER_COLL_OPT to 0.  Default: 0
20220317 014102.898 INFO             PET1 index=   3                                MPIR_CVAR_ALLTOALL_SYNC_FREQ : Adjusts the number of outstanding messages each Alltoall process  will allow.  Default is variable.
20220317 014102.898 INFO             PET1 index=   4                                 MPIR_CVAR_ALLTOALL_BLK_SIZE : The transfer size in bytes for the Alltoall chunking algorithm.  Default is 16384.
20220317 014102.898 INFO             PET1 index=   5                       MPIR_CVAR_ALLTOALL_CHUNKING_MAX_NODES : The maximum number of nodes to use the alltoall chunking algorithm. Above this value, the throttled algorithm will be used.  This is only applicable to SS-11.
20220317 014102.898 INFO             PET1 index=   6                              MPIR_CVAR_ALLGATHER_VSHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized gather/bcast algorithm is used instead of the optimized ring algorithm for MPI_Allgather. The gather/bcast algorithm is better suited for small messages. Defaults: For communicator sizes of <=512 ranks, 1024 bytes. For communicator sizes of >512 ranks, 4096 bytes.
20220317 014102.898 INFO             PET1 index=   7                             MPIR_CVAR_ALLGATHERV_VSHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized gatherv/bcast algorithm is used instead of the optimized ring algorithm for MPI_Allgatherv. The gatherv/bcast algorithm is better suited for small messages. Defaults: For communicator sizes of <=512 ranks, 1024 bytes. For communicator sizes of >512 ranks, 4096 bytes.
20220317 014102.898 INFO             PET1 index=   8                                  MPIR_CVAR_ALLREDUCE_NO_SMP : If set, MPI_Allreduce uses an algorithm that is not smp- aware. This provides a consistent ordering of the specified allreduce operation regardless of system configuration. Note:  This algorithm may not perform as well as the default smp-aware algorithms as it does not take advantage of rank topology. Default: not set
20220317 014102.898 INFO             PET1 index=   9                                MPIR_CVAR_ALLTOALL_SHORT_MSG : Adjusts the cut-off points at and below which the store and forward Alltoall algorithm is used for short messages. The default value is dependent upon the total number of ranks in the MPI communicator used for the MPI_Alltoall call and the Alltoall algorithm being selected. Defaults: If using one of the non-default send/recv algorithms on Aries, the defaults are: if communicator size <= 512, 2048 bytes if communicator size > 512 and <= 1024, 1024 bytes if communicator size > 1024 and <= 65536, 128 bytes if communicator size > 65536 and <= 131072, 64 bytes if communicator size > 131072 , 32 bytes
20220317 014102.898 INFO             PET1 index=  10                                MPIR_CVAR_ALLTOALLV_THROTTLE : Sets the per-process maximum number of outstanding Isends and Irecvs that can be posted concurrently for the optimized send/recv MPI_Alltoallv algorithm. For large messages, consider decreasing the throttle to 1 or 2 to improve performance. Defaults: 8
20220317 014102.898 INFO             PET1 index=  11                                   MPIR_CVAR_BCAST_ONLY_TREE : If set to 1, MPI_Bcast uses an smp-aware tree algorithm regardless of data size. The tree algorithm generally scales well to high processor counts on Cray XE systems. If set to 0, MPI_Bcast uses a variety of algorithms (tree, scatter, or ring) depending on message size and other factors. These other algorithms generally do not scale well when using more than 512 processors on Cray XE systems. Default: 1
20220317 014102.898 INFO             PET1 index=  12                             MPIR_CVAR_BCAST_INTERNODE_RADIX : Used to set the radix of the inter-node tree. This can be set to any integer value greater than or equal to 2. Default: 4
20220317 014102.898 INFO             PET1 index=  13                             MPIR_CVAR_BCAST_INTRANODE_RADIX : Used to set the radix of the intra-node tree. This can be set to any integer value greater than or equal to 2. Default: 4
20220317 014102.898 INFO             PET1 index=  14                                      MPIR_CVAR_COLL_OPT_OFF : If set, disables collective optimizations which use nondefault, architecture-specific algorithms for some MPI collective operations. By default, all collective optimized algorithms are enabled. To disable all collective optimized algorithms, set MPICH_COLL_OPT_OFF to 1. To disable optimized algorithms for selected MPI collectives, set the value to a comma-separated list of the desired collective names. Names are not case-sensitive. Any unrecognizable name is flagged with a warning message and ignored. For example, to disable the MPI_Allgather optimized collective algorithm, set MPICH_COLL_OPT_OFF=mpi_allgather. The following collective names are recognized: MPI_Allgather, MPI_Allgatherv, MPI_Allreduce, MPI_Alltoall, MPI_Alltoallv, MPI_Bcast, MPI_Gatherv, MPI_Scatterv, MPI_Igatherv, and MPI_Iallreduce. Default: Not enabled.
20220317 014102.898 INFO             PET1 index=  15                                         MPIR_CVAR_COLL_SYNC : If set, a Barrier is performed at the beginning of each specified MPI collective function. This forces all processes participating in that collective to sync up before the collective can begin. To disable this feature for all MPI collectives, set the value to 0. This is the default. To enable this feature for all MPI collectives, set the value to 1. To enable this feature for selected MPI collectives, set the value to a comma-separated list of the desired collective names. Names are not case-sensitive. Any unrecognizable name is flagged with a warning message and ignored. The following collective names are recognized: MPI_Allgather, MPI_Allgatherv, MPI_Allreduce, MPI_Alltoall, MPI_Alltoallv, MPI_Alltoallw, MPI_Bcast, MPI_Exscan, MPI_Gather, MPI_Gatherv, MPI_Reduce, MPI_Reduce_scatter, MPI_Scan, MPI_Scatter, and MPI_Scatterv. Default: Not enabled.
20220317 014102.898 INFO             PET1 index=  16                                 MPIR_CVAR_GATHERV_SHORT_MSG : Adjusts the cutoff point at and below which the optimized tree MPI_Gatherv algorithm is used instead of the optimized permission-to-send MPI_Gatherv algorithm. The cutoff is based on the average size of the variable MPI_Gatherv message sizes.  Default: 131072 bytes
20220317 014102.898 INFO             PET1 index=  17                                     MPIR_CVAR_REDUCE_NO_SMP : If set, MPI_Reduce uses an algorithm that is not smp-aware. This provides a consistent ordering of the specified reduce operation regardless of system configuration. Note:  This algorithm may not perform as well as the default smp-aware algorithms as it does not take advantage of rank topology. Default: not set
20220317 014102.898 INFO             PET1 index=  18                              MPIR_CVAR_SCATTERV_SYNCHRONOUS : The default, non-optimized ANL MPI_Scatterv algorithm uses asynchronous sends by default for communicator sizes less than 200,000 ranks. If set, this environment variable causes MPI_Scatterv to switch to using blocking sends, which may be beneficial in certain cases involving large data sizes or high process counts. For communicator sizes equal to or greater than 200,000 ranks, the blocking send algorithm is used by default. Default: not enabled
20220317 014102.898 INFO             PET1 index=  19                               MPIR_CVAR_SHARED_MEM_COLL_OPT : If set, the MPICH library will use the optimized shared- memory based design for collective operations. On Gemini and Aries systems, the supported collective operations are: MPI_Allreduce, MPI_Iallreduce, MPI_Barrier, and MPI_Bcast. To enable all available shared-memory optimizations, set MPICH_SHARED_MEM_COLL_OPT to 1. To enable this feature for a specific set of collective operations, set MPICH_SHARED_MEM_COLL_OPT to a comma- separated list of collective names. For example, to enable this optimization for MPI_Bcast only, set MPICH_SHARED_MEM_COLL_OPT=MPI_Bcast. To enable this optimization for MPI_Allreduce only, set MPICH_SHARED_MEM_COLL_OPT=MPI_Allreduce. Unsupported names are flagged with a warning message and ignored. Default: set
20220317 014102.898 INFO             PET1 index=  20                   MPIR_CVAR_ALLREDUCE_GPU_STAGING_THRESHOLD : If the sendbuf/recvbuf of an Allreduce operation is on GPU-resident memory regions, for payload sizes larger than this variable,  the Cray optimized staging implementation will be used.  By default, the staging implementation is used for all payload sizes.  This variable has no effect if MPICH_GPU_COLL_STAGING_AREA_OPT is set to 0. Default: 0
20220317 014102.898 INFO             PET1 index=  21                         MPIR_CVAR_GPU_COLL_STAGING_BUF_SIZE : This variable determines the size of the staging buffers used for some collectives (such as MPI_Allreduce) when sendbuf/recvbuf on GPU-resident buffers. This variable has no effect if  MPICH_GPU_COLL_STAGING_AREA_OPT is set to 0.   Default: 262144
20220317 014102.898 INFO             PET1 index=  22                                 MPIR_CVAR_GATHERV_SYNC_FREQ : Only applicable to the Gatherv permission-to-send algorithm.  Adjusts  the number of outstanding receives the root for Gatherv will allow.   Default is 16.
20220317 014102.898 INFO             PET1 index=  23                              MPIR_CVAR_GATHERV_MAX_TMP_SIZE : Only applicable to the Gatherv tree algorithm.  Sets the maximum amount of temporary memory GAtherv will allow a rank to allocate when using the tree-based algorithm.  Each rank allocates a different amount, with many allocating no extra memory.  If any rank requires more than this amount of temporary buffer space, a different algorithm is used. Default is 512MB.
20220317 014102.898 INFO             PET1 index=  24                             MPIR_CVAR_GATHERV_MIN_COMM_SIZE : Cray MPI offers two optimized Gatherv algorithms.  A tree algorithm for small messages and a permission-to-send send algorithm for larger messages.  Set  this value to the minimum communicator size to attempt use of either of the  Cray optimized Gatherv algorithms. Default is 64
20220317 014102.898 INFO             PET1 index=  25                                MPIR_CVAR_SCATTERV_SHORT_MSG : Adjusts the cutoff point at and below which the optimized tree MPI_Scatterv algorithm is used instead of the optimized staggered send algorithm.  The cutoff is in bytes, based on the average size of the variable MPI_Scatterv message sizes. Default behavior if unset is: For communicator sizes of <= 512 ranks, 2048 bytes For communicator sizes of > 512 ranks, 8192 bytes
20220317 014102.898 INFO             PET1 index=  26                                MPIR_CVAR_SCATTERV_SYNC_FREQ : Only applicable to the Scatterv staggered send algorithm.  Adjusts the number of outstanding sends the root for Scatterv will use. Default is 16.
20220317 014102.898 INFO             PET1 index=  27                             MPIR_CVAR_SCATTERV_MAX_TMP_SIZE : Only applicable to the Scatterv tree algorithm.  Sets the maximum amount of temporary memory Scatterv will allow a rank to allocate when using the tree-based algorithm.  Each rank allocates a different amount, with many allocating no extra memory.  If any rank requires more than this amount of temporary buffer space, a different algorithm is used. Default is 512MB.
20220317 014102.898 INFO             PET1 index=  28                            MPIR_CVAR_SCATTERV_MIN_COMM_SIZE : Cray MPI offers two optimized Scatterv algorithms.  A tree algorithm for small messages and a staggered send algorithm for larger messages.  Set this value to the minimum communicator size to attempt use of either of the Cray optimized Scatterv algorithms. Default is 64
20220317 014102.898 INFO             PET1 index=  29                           MPIR_CVAR_MPIIO_ABORT_ON_RW_ERROR : If set to enable, causes MPI-IO to abort immediately after issuing an error message if an I/O error occurs during a system read() or write() call. This applies only to I/O errors for system read() and write() calls made as a result of MPI I/O calls. It does not apply to I/O errors for other MPI I/O calls such as MPI_File_open(), nor does it apply to read() and write() calls made by means other than MPI I/O calls. Abort on error is not standard behavior. The MPI Standard specifies that the default error handling for MPI I/O calls is to return an error code to the application rather than aborting the application, but since errors on write or read are almost always unexpected and usually not recoverable, it may be preferable to abort as soon as the error is detected. Doing so does not allow any recovery, but does provide the most information about the error and terminates the job quickly. If the Cray Abnormal Termination Processing (ATP) feature is enabled, the abort will result in a full stack backtrace writte
20220317 014102.898 INFO             PET1 index=  30                MPIR_CVAR_MPIIO_AGGREGATOR_PLACEMENT_DISPLAY : This variable controls whether the placement of the aggregators will be displayed when a file is opened. The placement can be  controlled on a per file basis with the aggregator_placement_stride  hint. If set, displays the assignment of MPIIO collective buffering aggregators for reads/writes of a shared file, showing rank and node ID (nid). For example: Aggregator Placement for /lus/scratch/myfile RankReorderMethod=3  AggPlacementStride=-1 AGG    Rank       nid ----  ------  -------- 0       0  nid00578 1       4  nid00579 2       1  nid00606 3       5  nid00607 4       2  nid00578 5       6  nid00579 6       3  nid00606 7       7  nid00607 Default: not set
20220317 014102.898 INFO             PET1 index=  31                 MPIR_CVAR_MPIIO_AGGREGATOR_PLACEMENT_STRIDE : Partially controls to which nodes MPIIO collective buffering aggregators are assigned. See the notes below on the order of nodes. Network traffic and resulting I/O performance may be affected by the assignments. If set to 1, consecutive nodes are used. The number of aggregators assigned per node is controlled by the cb_config_list hint. By default, no more than one aggregator per node will be assigned if there are at least as many nodes as aggregators. If set to a value greater than 1, node selection is strided across the available nodes by this value. If the stride times the number of aggregators exceeds the number of nodes, the assignments will wrap around, which is usually not optimal for performance. If set to -1, node selection is strided across available nodes by the value of the number of nodes divided by the number of aggregators (integer division, minimum value of 1). The purpose is to spread out the nodes to reduce network congestion. Note:  The order of nodes can be shown by setting the MPICH_RANK
20220317 014102.898 INFO             PET1 index=  32                                    MPIR_CVAR_MPIIO_CB_ALIGN : Sets the default value for the cb_align hint. Files opened with MPI_File_open wil have this value for the cb_align hint unless the hint is set on a per file basis with either the MPICH_MPIIO_HINTS environment variable or from within a program with the MPI_Info_set() call. Note:  Only MPICH_MPIIO_CB_ALIGN == 2 is fully supported. Other values are for internal testing only. Default: 2
20220317 014102.898 INFO             PET1 index=  33                                MPIR_CVAR_MPIIO_DVS_MAXNODES : Note:  This environment variable in relevant only for file systems accessed from Cray system compute nodes via DVS server nodes; e.g. GPFS or PANFS. As described in the dvs(5) man page, the environment variable DVS_MAXNODES can be used to set the stripe width— that is, the number of DVS server nodes—used to access a file in "stripe parallel mode." For most files, and especially for small files, setting DVS_MAXNODES to 1 ("cluster parallel mode") is preferred. The MPICH_MPIIO_DVS_MAXNODES environment variable enables you to leave DVS_MAXNODES set to 1 and then use MPICH_MPIIO_DVS_MAXNODES to temporarily override DVS_MAXNODES when it is advantageous to specify wider striping for files being opened by the MPI_File_open() call. The range of values accepted by MPICH_MPIIO_DVS_MAXNODES goes from 1 to the number of server nodes specified on the mount with the nnodes mount option. DVS_MAXNODES is not set by default. Therefore, for MPICH_MPIIO_DVS_MAXNODES to have any effect, DVS_MAXNODES must be defined before p
20220317 014102.898 INFO             PET1 index=  34                                       MPIR_CVAR_MPIIO_HINTS : If set, override the default value of one or more MPI I/O hints. This also overrides any values that were set by using calls to MPI_Info_set in the application code. The new values apply to the file the next time it is opened using an MPI_File_open() call. After the MPI_File_open() call, subsequent MPI_Info_set calls can be used to pass new MPI I/O hints that take precedence over some of the environment variable values. Other MPI I/O hints such as striping_factor, striping_unit, cb_nodes, and cb_config_list cannot be changed after the MPI_File_open() call, as these are evaluated and applied only during the file open process. An MPI_File_close call followed by an MPI_File_open call can be used to restart the MPI I/O hint evaluation process. The syntax for this environment variable is a comma- separated list of specifications. Each individual specification is a pathname_pattern followed by a colon- separated list of one or more key=value pairs. In each key=value pair, the key is the MPI-IO hint name, and the v
20220317 014102.898 INFO             PET1 index=  35                               MPIR_CVAR_MPIIO_HINTS_DISPLAY : If set, causes rank 0 in the participating communicator to display the names and values of all MPI-IO hints that are set for the file being opened with the MPI_File_open call. It also displays relevant environment variables whether or not MPICH_ENV_DISPLAY is set. Default: not enabled.
20220317 014102.898 INFO             PET1 index=  36                               MPIR_CVAR_MPIIO_MAX_NUM_IRECV : When MPIIO collective buffering is used, this environment variable controls the number of outstanding MPI_Irecv calls allowed before an MPI_Waitall is done. Default: 50
20220317 014102.898 INFO             PET1 index=  37                               MPIR_CVAR_MPIIO_MAX_NUM_ISEND : When MPIIO collective buffering is used, this environment variable controls the number of outstanding MPI_Isend calls allowed before an MPI_Waitall is done. Default: 50
20220317 014102.898 INFO             PET1 index=  38                              MPIR_CVAR_MPIIO_MAX_SIZE_ISEND : When MPIIO collective buffering is used, this environment variable limits MPI_Isend by the amount of data being sent rather than by the number of calls. Default: 10485760 bytes
20220317 014102.898 INFO             PET1 index=  39                                       MPIR_CVAR_MPIIO_STATS : If set to 1, a summary of file write and read access patterns is written by rank 0 to stderr. This information provides some insight into how I/O performance may be improved. The information is provided on a per-file basis and is written when the file is closed. It does not provide any timing information. If set to 2, a set of data files are written to the working directory, one file for each rank, with the filename prefix specified by the MPICH_MPIIO_STATS_FILE environment variable. The data is in comma-separated values (CSV) format, which can be summarized with the cray_mpiio_summary script in the /opt/cray/mpt/version/gni/bin directory. Additional example scripts are provided in that directory to further process and display the data. Only enabled if MPIR_CVAR_MPIIO_CB_ALIGN=2. Default: not set
20220317 014102.898 INFO             PET1 index=  40                                  MPIR_CVAR_MPIIO_STATS_FILE : Specifies the filename prefix for the set of data files written when MPICH_MPIIO_STATS is set to 2. The filename prefix may be a full absolute pathname or a relative pathname. Summary plots of these files can be generated using the cray_mpiio_summary script from the /opt/cray/mpt/version/gni/bin directory. Other example scripts for post-processing this data can also be found in /opt/cray/mpt/version/gni/bin. Default: _cray_mpiio_stats_
20220317 014102.898 INFO             PET1 index=  41                         MPIR_CVAR_MPIIO_STATS_INTERVAL_MSEC : Specifies the time interval in milliseconds for each MPICH_MPIIO_STATS data point. Default: 250
20220317 014102.898 INFO             PET1 index=  42                                      MPIR_CVAR_MPIIO_TIMERS : If set to 0, or not set at all, no timing data is collected. If set to 1, timing data for different phases in MPI-IO is collected locally by each MPI process and then during MPI_File_close the data is consolidated and printed. Some timing data is displayed in seconds, other data is displayed in clock ticks, possibly scaled down. Also see MPICH_MPIIO_TIMERS_SCALE The relative values of the reported times are more important to the analysis than the absolute time. More detailed information about MPI-IO performance can be obtained by using the MPICH_MPIIO_STATS feature and by using the CrayPat and Apprentice2 Timeline Report of I/O bandwidth. Only enabled if MPIR_CVAR_MPIIO_CB_ALIGN=2. Default: 0
20220317 014102.898 INFO             PET1 index=  43                                MPIR_CVAR_MPIIO_TIMERS_SCALE : Specifies the power of 2 to use to scale the times reported by MPICH_MPIIO_TIMERS.  The raw times are collected in clock ticks. This generally is a very large number and reducing all the times by the same scaling factor makes for a more compact display. If set to 0, or not set at all, MPI-IO automatically determines a scaling factor to limit the report times to 9 or fewer digits. This auto-determined value is displayed.  To make run to run comparisons, you can set the scaling factor to your preferred value. Default: 0
20220317 014102.898 INFO             PET1 index=  44                                  MPIR_CVAR_MPIIO_TIME_WAITS : If set to non-zero, time how long this rank has to wait for other ranks to catch up.  This separates true metadata time from imbalance time. This is disabled when MPICH_MPIIO_TIMERS is not set.  Otherwise it defaults to 1. Default: 1
20220317 014102.898 INFO             PET1 index=  45                          MPIR_CVAR_MPIIO_WRITE_EXIT_BARRIER : If set to non-zero, collective write's will barrier on exit Default: 1
20220317 014102.898 INFO             PET1 index=  46                               MPIR_CVAR_MPIIO_DS_WRITE_CRAY : If set to non-zero, collective write's with data sieving will be optimized  Default: 1
20220317 014102.898 INFO             PET1 index=  47                         MPIR_CVAR_MPIIO_OFI_STARTUP_CONNECT : If set to enable, causes MPI-IO to establish connections between ranks and aggregators during the openning of the file to be used for MPIIO collective operations
20220317 014102.899 INFO             PET1 index=  48                MPIR_CVAR_MPIIO_OFI_STARTUP_NODES_AGGREGATOR : If MPIIO_OFI_STARTUP_CONNECT is enabled, this specifies the number of nodes that will concurrently attempt to connext to each aggrgator. Each rank on each node will establish its own connection to the aggregator. Default: 2
20220317 014102.899 INFO             PET1 index=  49                                           MPIR_CVAR_DPM_DIR : Sets the directory to use for MPI port name publishing in the  file-based nameserv implementation, as well as publishing the  credential obtained from libdrc.
20220317 014102.899 INFO             PET1 index=  50                               MPIR_CVAR_SINGLE_HOST_ENABLED : If true and if running as a single node, then inter-node communications is never initialized. The helps prevent the use of scarce hardware resources.
20220317 014102.899 INFO             PET1 index=  51                                       MPIR_CVAR_OFI_VERBOSE : If set to 1, enable verbose output about the CH4 OFI device. If set to 2 or higher, enable more verbose output.
20220317 014102.899 INFO             PET1 index=  52                                 MPIR_CVAR_OFI_XRCD_BASE_DIR : Specifies the base directory for the XRCD file.  This is only  used if the job is not launched via PALS launcher.  If launched  via PALS, we use the private, temporary PALS_SPOOL_DIR directory. Default: /tmp
20220317 014102.899 INFO             PET1 index=  53                                   MPIR_CVAR_OFI_NIC_VERBOSE : If set to 1 or higher, enables verbose output about the CH4 OFI NIC selection.   Non-fatal warnings are also displayed.  If set to 2 or higher, each rank in  the job will display it's NIC selection.  If set to 3 or higher, more verbose  output is enabled (for debug purposes).
20220317 014102.899 INFO             PET1 index=  54                                    MPIR_CVAR_OFI_NIC_POLICY : Specifies the rank-to-nic policy.  The supported options are:  NUMA | GPU | BLOCK | ROUND-ROBIN | USER
20220317 014102.899 INFO             PET1 index=  55                                   MPIR_CVAR_OFI_NIC_MAPPING : Specifies the precise rank-to-NIC mapping to use on each node.  This is  required if the NIC_POLICY is set to USER.  Each local rank must have a NIC mapping assigned by this env variable. If there are fewer MPI ranks on any  node, that part of the MAPPING string will be ignored.  For example: To assign local ranks 0,16,32,48 to NIC 0, and remaining ranks to NIC 1 MPICH_OFI_NIC_MAPPING="0:0,16,32,48; 1:1-15,17-31,33-47,49-63"
20220317 014102.899 INFO             PET1 index=  56                                      MPIR_CVAR_OFI_NUM_NICS : If set, specifies the number of NICs the job can use on each node. By default, when multiple NICs per node are available, MPI attempts to use them all.  If using fewer NICs is desired, this env variable  can be set to indicate the maximum number of NICs per node MPI will  use. By default, consecutive NICs are used, starting with index 0.   To limit use to 1 NIC/node, use : export MPICH_OFI_NUM_NICS=1 To limit use to 2 NICs/node, use: export MPICH_OFI_NUM_NICS=2 To specify alternative NIC index values, you may indicate the desired index values by adding a colon, followed by the NIC indexes.  Use  quotes so the shell doesn't interfere.  For example: To limit use to 1 NIC/node, index 1    : export MPICH_OFI_NUM_NICS="1:1" To limit use to 2 NICs/node, index 1&3 : export MPICH_OFI_NUM_NICS="2:1,3"
20220317 014102.899 INFO             PET1 index=  57                        MPIR_CVAR_OFI_SKIP_NIC_SYMMETRY_TEST : If set to 1, CrayMPICH will bypass it's check for NIC symmetry on all nodes in the job. By default, this check is run during MPI_Init to  make sure all the nodes in the job have the same number of NICs available.
20220317 014102.899 INFO             PET1 index=  58                               MPIR_CVAR_OFI_STARTUP_CONNECT : If set to 1, CrayMPICH will create COMM_WORLD static OFI connections to every other rank during MPI_Init.
20220317 014102.899 INFO             PET1 index=  59                           MPIR_CVAR_OFI_RMA_STARTUP_CONNECT : If set to 1, CrayMPICH RMA will create static OFI connections to every other rank on the same node during MPI_Init. This will also enable MPIR_CVAR_OFI_STARTUP_CONNECT.
20220317 014102.899 INFO             PET1 index=  60                                MPIR_CVAR_OFI_DEFAULT_TCLASS : If set, CrayMPICH will assign the requested default TC (traffic class) to the job domain.  By default, all endpoints inherit this TC.
20220317 014102.899 INFO             PET1 index=  61                                 MPIR_CVAR_OFI_TCLASS_ERRORS : Determines how CrayMPICH responds to traffic class errors.  Valid values are "warn", "silent" or "error".
20220317 014102.899 INFO             PET1 index=  62                                  MPIR_CVAR_OFI_CXI_PID_BASE : Set to the base value that MPI will use to assign portal IDs for the CXI provider.  Max PID value is 510.  MPI uses PID_BASE+lrank as the unique value per rank per node.  This is only applicable to SS11.
20220317 014102.899 INFO             PET1 index=  63                          MPIR_CVAR_OFI_USE_SCALABLE_STARTUP : Set to false (0) to bypass the scalable startup feature for CXI. This is only applicable to SS11.
20220317 014102.899 INFO             PET1 index=  64                                       MPIR_CVAR_UCX_VERBOSE : If set to 1, enable verbose output about the CH4 UCX device. If set to 2 or higher, enable more verbose output.
20220317 014102.899 INFO             PET1 index=  65                                  MPIR_CVAR_UCX_RC_MAX_RANKS : By default, CrayMPI selects either the UCX RC or UD protocols for inter-node messaging.  If a job is launched with MPICH_UCX_RC_MAX_RANKS  ranks or fewer, the RC protocol is selected.  If more ranks are launched,  the UCX UD protocol is chosen.  The RC protocol does not scale well due to the resource requirements.  Note this default can be overridden by setting the UCX_TLS environment variable.
20220317 014102.899 INFO             PET1 index=  66                              MPIR_CVAR_SMP_SINGLE_COPY_MODE : If non-null, choose an on-node copy mode for large messages. This variable can be set to XPMEM, CMA, or NONE. By default, Cray MPICH will look to use XPMEM. If XPMEM is requested, but not available, Cray MPICH will attempt to use CMA. (NOTE: Cray MPICH does not support CMA currently. The env. variables and CVARs are set up to allow CMA usage with future versions of Cray MPICH) If both XPMEM and CMA cannot be used, Cray MPICH will fall back to using the ANL shared memory implementation (2-copy) Default: NULL
20220317 014102.899 INFO             PET1 index=  67                              MPIR_CVAR_SMP_SINGLE_COPY_SIZE : Specifies the minimum message size in bytes to consider for single-copy transfers for on-node messages. This applies only to the SMP (on-node shared memory) device. The value is interpreted as bytes, unless the string ends in a K, which indicates kilobytes, or M, which indicates megabytes. Default: 8192
20220317 014102.899 INFO             PET1 index=  68                       MPIR_CVAR_SHM_PROGRESS_MAX_BATCH_SIZE : Adjusts the maximum number of on-node requests that can be processed in a single batch. Higher values for the maximum batch size can lower the overhead due to entering the progress engine, but can also  delay the processing of off-node message requests. Default is 8.
20220317 014102.899 INFO             PET1 index=  69                                MPIR_CVAR_CH4_RMA_THREAD_HOT : If true, the big lock is disabled for certain RMA operations
20220317 014102.899 INFO             PET1 index=  70                                    MPIR_CVAR_ABORT_ON_ERROR : If set, causes MPICH to abort and produce a core dump when MPICH detects an internal error. Note that the core dump size limit (usually 0 bytes by default) must be reset to an appropriate value in order to enable coredumps. Default: Not enabled.
20220317 014102.899 INFO             PET1 index=  71                                   MPIR_CVAR_CPUMASK_DISPLAY : If set, causes each MPI rank in the job to display its CPU affinity bitmask. Note that this reports only the CPU affinity masks for the MPI ranks; if you have a hybrid program, it does not provide any thread information. The bitmask is read from right to left, meaning the value in the rightmost position corresponds to CPU 0 on the node.
20220317 014102.899 INFO             PET1 index=  72                                       MPIR_CVAR_ENV_DISPLAY : If set, causes rank 0 to display all MPICH environment variables and their current settings at MPI initialization time. If two or more nodes are used, MPICH/GNI environment settings are also included in the listing. Default: Not enabled.
20220317 014102.899 INFO             PET1 index=  73                                  MPIR_CVAR_OPTIMIZED_MEMCPY : Specifies which version of memcpy to use. Valid values are: 0         Use the system (glibc) version of memcpy. 1         Use an optimized version of memcpy if one is available for the processor being used. In this release, an optimized version of memcpy() is available only for Intel processors. 2         Use a highly optimized version of memcpy that provides better performance in some areas but may have performance regressions in other areas, if one is available for the processor being used. In this release, a highly optimized version of memcpy() is available only for Intel Haswell processors. MPICH_OPTIMIZED_MEMCPY is overridden by MPICH_USE_SYSTEM_MEMCPY. If MPICH_USE_SYSTEM_MEMCPY is set, MPICH_OPTIMIZED_MEMCPY is ignored and the system (glibc) version of memcpy() is used. Default: 1
20220317 014102.899 INFO             PET1 index=  74                                     MPIR_CVAR_STATS_DISPLAY : If set to 1, a summary of MPI statistics, also available through the MPI Tools Interface, will be written by rank 0 to stderr. If set to 2, all ranks will produce an individualized statistics summary and write to file on a per-rank basis. The MPICH_STATS_FILE determines the prefix of the file to be used. This information may provide insight into how MPI performance may be improved. Default: 0
20220317 014102.899 INFO             PET1 index=  75                                   MPIR_CVAR_STATS_VERBOSITY : Specifies the verbosity of the MPI statistics summary. This information may provide insight into how MPI performance may be improved. Increase the value for more detailed summary. Default: 1 1        USER_BASIC  (default) 2        USER_DETAIL 3        USER_ALL 4        TUNER_BASIC 5        TUNER_DETAIL 6        TUNER_ALL 7        MPIDEV_BASIC 8        MPIDEV_DETAIL 9        MPIDEV_ALL
20220317 014102.899 INFO             PET1 index=  76                                        MPIR_CVAR_STATS_FILE : Specifies the filename prefix for the set of data files written when MPICH_STATS_DISPLAY is set to 2. The filename prefix may be a full absolute pathname or a relative pathname. Default: _cray_stats_
20220317 014102.899 INFO             PET1 index=  77                              MPIR_CVAR_RANK_REORDER_DISPLAY : If set, causes rank 0 to display which node each MPI rank resides in. The rank order can be manipulated via the MPICH_RANK_REORDER_METHOD environment variable or MPIR_CVAR_RANK_REORDER_METHOD control variable. Default: Not set
20220317 014102.899 INFO             PET1 index=  78                               MPIR_CVAR_RANK_REORDER_METHOD : Overrides the default MPI rank placement scheme. If this variable is not set, the default aprun launcher placement policy is used. The default policy for aprun is SMP-style placement. To display the MPI rank placement information, set MPICH_RANK_REORDER_DISPLAY. See manpage for more details. Default: 1, for SMP-style placement.
20220317 014102.899 INFO             PET1 index=  79                                 MPIR_CVAR_USE_SYSTEM_MEMCPY : Note:  This environment variable is deprecated and scheduled to be removed in a future release. Use MPICH_OPTIMIZED_MEMCPY instead. If set, use the system (glibc) version of memcpy(); otherwise, an optimized version of memcpy() may be used. Currently, an optimized version of memcpy() is available only for Intel processors. Default: Not set
20220317 014102.899 INFO             PET1 index=  80                                   MPIR_CVAR_VERSION_DISPLAY : If set, causes MPICH to display the CRAY MPICH version number as well as build date information. Default: Not enabled
20220317 014102.899 INFO             PET1 index=  81                          MPIR_CVAR_USE_GPU_STREAM_TRIGGERED : If set, causes MPICH to allow using stream triggered GPU communication operations. Default: Not enabled
20220317 014102.899 INFO             PET1 index=  82                               MPIR_CVAR_NUM_MAX_GPU_STREAMS : If set, causes MPICH to allow using the set maximum number of streams concurrently in the stream triggered GPU communication operations. Default: 26
20220317 014102.899 INFO             PET1 index=  83                                  MPIR_CVAR_MEMCPY_MEM_CHECK : If set, enables a check of the memcpy() source and destination areas. If they overlap, the application asserts with an error message listing the file, line, and memory range overlap. If this error is found, correct it either by changing the memory ranges or possibly by using MPI_IN_PLACE. Default: not set (off)
20220317 014102.899 INFO             PET1 index=  84                                     MPIR_CVAR_MSG_QUEUE_DBG : If set, turns on TotalView Message Queue Debugging support so that message queues are tracked in the TotalView debugger and a message queue graph can be generated. Enabling this feature degrades performance. Default: not enabled.
20220317 014102.899 INFO             PET1 index=  85                             MPIR_CVAR_NO_BUFFER_ALIAS_CHECK : If set, the buffer alias error check for collectives is disabled. The MPI standard does not allow aliasing of type OUT or INOUT parameters on the same collective function call. The use of MPI_IN_PLACE is required in these scenarios. A new check was added in MPT 5.2 to detect this condition and report the error. To bypass this check, set MPICH_NO_BUFFER_ALIAS_CHECK to any value. Default: not set
20220317 014102.899 INFO             PET1 index=  86                                MPIR_CVAR_ALLOC_MEM_AFFINITY : Controls the affinity of the memory region allocated by the MPI_Alloc_mem() or MPI_Win_allocate() operations. On systems that do not offer High Bandwidth Memory capabilities, (such as Intel Haswell or Broadwell processors), this env. variable is ignored and memory affinity is set to DDR. On Phi processors, such as KNL (and KNH, KNP in the future), this env. variable allows users to specifically request the memory returned by MPI_Alloc_mem() and MPI_Win_allocate() to be bound to either DDR, or the MCDRAM. Users can request a specific page size or memory binding policy via the MPICH_ALLOC_MEM_POLICY and MPICH_ALLOC_MEM_PG_SZ env. variables. Default: SYS_DEFAULT
20220317 014102.899 INFO             PET1 index=  87                             MPIR_CVAR_INTERNAL_MEM_AFFINITY : Controls the affinity of internal memory regions allocated by the MPI library. This variable currently affects the memory affinity of the mail-boxes used for off-node communication, and the shared-memory regions that are used for on-node pt2pt and collective ops. On systems that do not offer High Bandwidth Memory capabilities, (such as Intel Haswell or Broadwell processors), this env. variable is ignored and memory affinity is set to DDR. On Phi processors, such as KNL (and KNH, KNP in the future), this env. variable allows users to specifically request the internal memory regions used by the MPI library to be bound to either DDR, or the MCDRAM. The default affinity settings will be governed by the system defaults. For example, on a KNL system configured in the Quad/Flat mode, if the job is run with numactl --membind=1, all of MPI's internal memory will be bound to MCDRAM if this variable is not set. Default: SYS_DEFAULT
20220317 014102.899 INFO             PET1 index=  88                                  MPIR_CVAR_ALLOC_MEM_POLICY : Controls the memory affinity policy on systems with specialized memory hardware. By default, the memory policy is set to "{P}referred". Other accepted values are "{M}andatory" and "{I}"nterleave. Default: Preferred
20220317 014102.899 INFO             PET1 index=  89                                   MPIR_CVAR_ALLOC_MEM_PG_SZ : Controls the page size for the MPI_Alloc_mem() and MPI_Win_allocate() operations. This parameters defaults to 4KB base pages. The supported values are 2M, 4M, 8M, 16M, 32M, 64M, 128M, 256M, 512M, 1G and 2G. Default: 4096
20220317 014102.899 INFO             PET1 index=  90                                   MPIR_CVAR_OPT_THREAD_SYNC : Controls the mechanism used to implement thread-synchronization inside the Cray MPICH library. If set to 1, an optimized synchronization implementation is used. If set to 0, Cray MPICH falls back to using a pthread_mutex-based thread-synchronization implementation. Default: 1
20220317 014102.899 INFO             PET1 index=  91                                 MPIR_CVAR_THREAD_YIELD_FREQ : Determines how often a thread yields while waiting to acquire a lock in the new Cray optimized locking impl. This variable has no effect if MPICH_CRAY_OPT_THREAD_SYNC is 0. Default: 10000
20220317 014102.899 INFO             PET1 index=  92                                   MPIR_CVAR_MEM_DEBUG_FNAME : If set, the MPI library creates new files with the user specified name and writes various important memory statistics into these files. This information can be useful for post-processing. Users can set MPICH_MEM_DEBUG_FNAME to any suitable string. The resulting files are named as "string".<pid>.<MPI-rank>. For example, if MPICH_MEM_DEBUG_FNAME is set to "MEM_DBG_MSGS" the debug file for rank0 will be named "MEM_DBG_MSGS.<pid>.0 and written to the user's current working directory. If this flag is not set, the MPI library will redirect all the debug messages to stderr. Default: unset (disabled)
20220317 014102.899 INFO             PET1 index=  93                                   MPIR_CVAR_MALLOC_FALLBACK : Set the policy for fallback behavior when attempting to allocate large pages for internal buffers and insufficient large pages are available to satisfy the request. If set to enabled, MPICH falls back to using malloc in such cases. Default: not enabled (process fails and job terminates if insufficient large pages are available to satisfy the request)
20220317 014102.899 INFO             PET1 index=  94                              MPIR_CVAR_NO_DIRECT_GPU_ACCESS : If true, IPC and NIC-GPU Direct Access capabilities are not used for GPU-to-GPU transfers. This is mainly used for debugging.  Default: 0
20220317 014102.899 INFO             PET1 index=  95                                      MPIR_CVAR_G2G_PIPELINE : If nonzero, the device-host and network transfers will be overlapped to pipeline GPU-to-GPU transfers. Setting MPICH_G2G_PIPELINE to N will allow N GPU-to-GPU messages to be efficiently in-flight at any one time. If MPICH_G2G_PIPELINE is nonzero but MPICH_GPU_SUPPORT_ENABLED is disabled, MPICH_G2G_PIPELINE will be turned off. If MPICH_GPU_SUPPORT_ENABLED is enabled but MPICH_G2G_PIPELINE is 0, the default value is set to 8.  The pipeline-based implementation is being carried over from  CH3/Aries implementation. We think the pipelined-based implementation will not be necessary on SS-11 systems.  This logic may get dropped in the future.  Default: not set
20220317 014102.899 INFO             PET1 index=  96                               MPIR_CVAR_GPU_SUPPORT_ENABLED : If set, allows the MPI application to pass GPU pointers directly to MPI communication functions.  Default: not set
20220317 014102.899 INFO             PET1 index=  97                MPIR_CVAR_GPU_MANAGED_MEMORY_SUPPORT_ENABLED : If set, allows the MPI application to pass pointers allocated via  GPU managed memory allocators to MPI communication functions. Default: not set
20220317 014102.899 INFO             PET1 index=  98                         MPIR_CVAR_GPU_COLL_STAGING_AREA_OPT : If set, allows Cray MPI to use pre-registered CPU-attached memory to optimize collective ops.  Default: set
20220317 014102.899 INFO             PET1 index=  99               MPIR_CVAR_GPU_COLL_REGISTER_SHARED_MEM_REGION : If set, allows Cray MPI to registere CPU-attached shared memory regions  used to optimize collective ops.  Default: set
20220317 014102.899 INFO             PET1 index= 100                                   MPIR_CVAR_GPU_IPC_ENABLED : If set to 1, Cray MPICH will use GPU IPC to move data between GPU devices that are within the same node. If set to 0, Cray MPICH will use staging buffers on the host to implement data transfer between GPU devices that are within the same node. If MPIR_CVAR_GPU_SUPPORT_ENABLED is set, Cray MPICH  automatically attempts to enable the IPC optimizations.   If MPIR_CVAR_NO_DIRECT_GPU_ACCESS is set, IPC optimizations are disabled.  Default: -1
20220317 014102.899 INFO             PET1 index= 101                                 MPIR_CVAR_GPU_IPC_THRESHOLD : Specifies the minimum message size in bytes to consider for single-copy transfers between GPU devices that are within the same compute node. The value is interpreted as bytes, unless the string ends in a K, which indicates kilobytes, or M, which indicates megabytes.
20220317 014102.899 INFO             PET1 index= 102                                  MPIR_CVAR_GPU_IPC_PROTOCOL : Specifies the protocol used for large msg on-node, inter-GPU  data transfers via GPU IPC.  Valid options are RPUT or RGET.  Default: RGET
20220317 014102.899 INFO             PET1 index= 103                                 MPIR_CVAR_GPU_NO_ASYNC_COPY : If set, disables the use of asynchronous memcpy's for on-node GPU-aware transfers. Default: 0
20220317 014102.899 INFO             PET1 index= 104                                      MPIR_CVAR_ENABLE_YAKSA : If set, enables the use of the YAKSA datatype engine for packing and unpacking non-contiguous buffers in both CPU- and GPU-attached memory. Default: 0
20220317 014102.899 INFO             PET1 index= 105                              MPIR_CVAR_GPU_EAGER_DEVICE_MEM : If set to 1, GPU-attached memory will be used for the eager staging buffers for short on-node messages where at least the sending buffer is GPU-attached. If set to 0, this optimization is disabled and we fall-back to using staging buffers on the CPU for small message  intra-node device-host and device-device MPI ops. Default: 1
20220317 014102.899 INFO             PET1 index= 106                       MPIR_CVAR_GPU_EAGER_REGISTER_HOST_MEM : If set to 1, enables MPI to request POSIX  eager memory to be registered with the GPU runtime.  This is an optimization to improve the performance of  small message intra-node, inter-GPU MPI ops that use  POSIX shared memory as bounce buffers.  If set to 0, this optimization is disabled and we rely on GTL to lock/unlock host memory regions for a given on-node, small msg inter-GPU transfer.  Default: 1
20220317 014102.899 INFO             PET1 index= 107                          MPIR_CVAR_GPU_ALLREDUCE_USE_KERNEL : If set, adds a hint that the use of device kernels for reduction operations is desired. MPI is not guaranteed to use a device kernel for all reduction operations. Default: 0
20220317 014102.899 INFO             PET1 index= 108                                   MPIR_CVAR_RMA_MAX_PENDING : Determines how many RMA operation may be outstanding at any time over libfabrics. RMA operations beyond this max will be queued and only issued as pending operations complete. Default: 64
20220317 014102.899 INFO             PET1 index= 109                                MPIR_CVAR_RMA_SHM_ACCUMULATE : If set to 1, enables SHM accumulate operations. If set to 0, disables SHM accumulate operations. It also sets the default for the window hint "disable_shm_accumulate".  Default: 1
20220317 014102.899 INFO             PET1 --- VMK::logSystem() end ---------------------------------
20220317 014102.899 INFO             PET1 main: --- VMK::log() start -------------------------------------
20220317 014102.899 INFO             PET1 main: vm located at: 0x1b35ae0
20220317 014102.899 INFO             PET1 main: petCount=6 localPet=1 mypthid=22834890383488 currentSsiPe=1
20220317 014102.899 INFO             PET1 main: Current system level affinity pinning for local PET:
20220317 014102.899 INFO             PET1 main:  SSIPE=1
20220317 014102.899 INFO             PET1 main: Current system level OMP_NUM_THREADS setting for local PET: 128
20220317 014102.899 INFO             PET1 main: ssiCount=1 localSsi=0
20220317 014102.899 INFO             PET1 main: mpionly=1 threadsflag=0
20220317 014102.899 INFO             PET1 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220317 014102.899 INFO             PET1 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220317 014102.899 INFO             PET1 main:  PE=0 SSI=0 SSIPE=0
20220317 014102.899 INFO             PET1 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220317 014102.899 INFO             PET1 main:  PE=1 SSI=0 SSIPE=1
20220317 014102.899 INFO             PET1 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220317 014102.899 INFO             PET1 main:  PE=2 SSI=0 SSIPE=2
20220317 014102.899 INFO             PET1 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220317 014102.899 INFO             PET1 main:  PE=3 SSI=0 SSIPE=3
20220317 014102.899 INFO             PET1 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220317 014102.899 INFO             PET1 main:  PE=4 SSI=0 SSIPE=4
20220317 014102.899 INFO             PET1 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220317 014102.899 INFO             PET1 main:  PE=5 SSI=0 SSIPE=5
20220317 014102.899 INFO             PET1 main: --- VMK::log() end ---------------------------------------
20220317 014102.899 INFO             PET1 Executing 'userm1_setvm'
20220317 014102.900 INFO             PET1 Executing 'userm1_register'
20220317 014102.900 INFO             PET1 Executing 'userm2_setvm'
20220317 014102.900 DEBUG            PET1 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220317 014102.900 DEBUG            PET1 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220317 014102.901 INFO             PET1 Entering 'user1_run'
20220317 014102.901 INFO             PET1 model1: --- VMK::log() start -------------------------------------
20220317 014102.901 INFO             PET1 model1: vm located at: 0x1b913c0
20220317 014102.901 INFO             PET1 model1: petCount=6 localPet=1 mypthid=22834890383488 currentSsiPe=1
20220317 014102.901 INFO             PET1 model1: Current system level affinity pinning for local PET:
20220317 014102.901 INFO             PET1 model1:  SSIPE=1
20220317 014102.901 INFO             PET1 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220317 014102.901 INFO             PET1 model1: ssiCount=1 localSsi=0
20220317 014102.901 INFO             PET1 model1: mpionly=1 threadsflag=0
20220317 014102.901 INFO             PET1 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220317 014102.901 INFO             PET1 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220317 014102.901 INFO             PET1 model1:  PE=0 SSI=0 SSIPE=0
20220317 014102.901 INFO             PET1 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220317 014102.901 INFO             PET1 model1:  PE=1 SSI=0 SSIPE=1
20220317 014102.901 INFO             PET1 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220317 014102.901 INFO             PET1 model1:  PE=2 SSI=0 SSIPE=2
20220317 014102.901 INFO             PET1 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220317 014102.901 INFO             PET1 model1:  PE=3 SSI=0 SSIPE=3
20220317 014102.901 INFO             PET1 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220317 014102.901 INFO             PET1 model1:  PE=4 SSI=0 SSIPE=4
20220317 014102.901 INFO             PET1 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220317 014102.901 INFO             PET1 model1:  PE=5 SSI=0 SSIPE=5
20220317 014102.901 INFO             PET1 model1: --- VMK::log() end ---------------------------------------
20220317 014102.901 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220317 014103.268 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220317 014103.575 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220317 014103.882 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220317 014104.190 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220317 014104.497 INFO             PET1 Exiting 'user1_run'
20220317 014106.141 INFO             PET1 Entering 'user1_run'
20220317 014106.141 INFO             PET1 model1: --- VMK::log() start -------------------------------------
20220317 014106.141 INFO             PET1 model1: vm located at: 0x1b913c0
20220317 014106.141 INFO             PET1 model1: petCount=6 localPet=1 mypthid=22834890383488 currentSsiPe=1
20220317 014106.141 INFO             PET1 model1: Current system level affinity pinning for local PET:
20220317 014106.141 INFO             PET1 model1:  SSIPE=1
20220317 014106.141 INFO             PET1 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220317 014106.141 INFO             PET1 model1: ssiCount=1 localSsi=0
20220317 014106.141 INFO             PET1 model1: mpionly=1 threadsflag=0
20220317 014106.141 INFO             PET1 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220317 014106.141 INFO             PET1 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220317 014106.141 INFO             PET1 model1:  PE=0 SSI=0 SSIPE=0
20220317 014106.141 INFO             PET1 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220317 014106.141 INFO             PET1 model1:  PE=1 SSI=0 SSIPE=1
20220317 014106.141 INFO             PET1 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220317 014106.141 INFO             PET1 model1:  PE=2 SSI=0 SSIPE=2
20220317 014106.141 INFO             PET1 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220317 014106.141 INFO             PET1 model1:  PE=3 SSI=0 SSIPE=3
20220317 014106.141 INFO             PET1 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220317 014106.141 INFO             PET1 model1:  PE=4 SSI=0 SSIPE=4
20220317 014106.141 INFO             PET1 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220317 014106.141 INFO             PET1 model1:  PE=5 SSI=0 SSIPE=5
20220317 014106.141 INFO             PET1 model1: --- VMK::log() end ---------------------------------------
20220317 014106.141 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220317 014106.449 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220317 014106.756 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220317 014107.063 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220317 014107.371 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220317 014107.678 INFO             PET1 Exiting 'user1_run'
20220317 014109.291 INFO             PET1  NUMBER_OF_PROCESSORS           6
20220317 014109.291 INFO             PET1  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220317 014109.291 INFO             PET1 Finalizing ESMF
20220317 014102.897 INFO             PET2 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220317 014102.897 INFO             PET2 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220317 014102.897 INFO             PET2 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220317 014102.897 INFO             PET2 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220317 014102.897 INFO             PET2 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220317 014102.897 INFO             PET2 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220317 014102.897 INFO             PET2 Running with ESMF Version   : v8.3.0b09-100-g5ff85a963f
20220317 014102.897 INFO             PET2 ESMF library build date/time: "Mar 17 2022" "01:27:56"
20220317 014102.897 INFO             PET2 ESMF library build location : /lfs/h1/emc/ptmp/Mark.Potts/gfortran_10.3.0_mpi_O_jedwards_pio_update2
20220317 014102.897 INFO             PET2 ESMF_COMM                   : mpi
20220317 014102.898 INFO             PET2 ESMF_MOAB                   : enabled
20220317 014102.898 INFO             PET2 ESMF_LAPACK                 : enabled
20220317 014102.898 INFO             PET2 ESMF_NETCDF                 : enabled
20220317 014102.898 INFO             PET2 ESMF_PNETCDF                : disabled
20220317 014102.898 INFO             PET2 ESMF_PIO                    : enabled
20220317 014102.898 INFO             PET2 ESMF_YAMLCPP                : enabled
20220317 014102.898 INFO             PET2 --- VMK::logSystem() start -------------------------------
20220317 014102.898 INFO             PET2 esmfComm=mpi
20220317 014102.898 INFO             PET2 isPthreadsEnabled=1
20220317 014102.898 INFO             PET2 isOpenMPEnabled=1
20220317 014102.898 INFO             PET2 isOpenACCEnabled=0
20220317 014102.898 INFO             PET2 isSsiSharedMemoryEnabled=1
20220317 014102.898 INFO             PET2 ssiCount=1 peCount=6
20220317 014102.898 INFO             PET2 PE=0 SSI=0 SSIPE=0
20220317 014102.898 INFO             PET2 PE=1 SSI=0 SSIPE=1
20220317 014102.898 INFO             PET2 PE=2 SSI=0 SSIPE=2
20220317 014102.898 INFO             PET2 PE=3 SSI=0 SSIPE=3
20220317 014102.898 INFO             PET2 PE=4 SSI=0 SSIPE=4
20220317 014102.898 INFO             PET2 PE=5 SSI=0 SSIPE=5
20220317 014102.898 INFO             PET2 --- VMK::logSystem() MPI Control Variables ---------------
20220317 014102.898 INFO             PET2 index=   0          MPIR_CVAR_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE : specifies the cutoff size of the send buffer (in bytes) above which the reduce_scatter functions attempt to use the pairwise exchange algorithm.  In addition, the op must be commutative and the communicator size < MPIR_CVAR_REDUCE_SCATTER_MAX_COMMSIZE for the pairwise exchange algorithm to be used.
20220317 014102.898 INFO             PET2 index=   1                       MPIR_CVAR_REDUCE_SCATTER_MAX_COMMSIZE : specifies the max communicator size that will trigger use of the pairwise exchange algorithm, provided the op is commutative.  The pairwise exchange algorithm is not well suited for scaling to high process counts, so for larger communicators, a recursive halving algorithm is used instead.
20220317 014102.898 INFO             PET2 index=   2                           MPIR_CVAR_NETWORK_BUFFER_COLL_OPT : If set to 1, the MPICH library will use the optimized shared- memory based "network buffer" design for collective operations.  This feature is closely tied to the shared-memory collective optimization available in Cray MPICH. If enabled, the shared-memory buffer is also registered with the NIC and can be used directly to  perform off-node transfers, bypassing the Nemesis channel layer.  This feature is disabled if MPICH_SHARED_MEM_COLL_OPT  is set to 0. Currently, this optimization is only available  for the MPI_Bcast collective operation. To disable this feature,  set MPICH_NETWORK_BUFFER_COLL_OPT to 0.  Default: 0
20220317 014102.898 INFO             PET2 index=   3                                MPIR_CVAR_ALLTOALL_SYNC_FREQ : Adjusts the number of outstanding messages each Alltoall process  will allow.  Default is variable.
20220317 014102.898 INFO             PET2 index=   4                                 MPIR_CVAR_ALLTOALL_BLK_SIZE : The transfer size in bytes for the Alltoall chunking algorithm.  Default is 16384.
20220317 014102.898 INFO             PET2 index=   5                       MPIR_CVAR_ALLTOALL_CHUNKING_MAX_NODES : The maximum number of nodes to use the alltoall chunking algorithm. Above this value, the throttled algorithm will be used.  This is only applicable to SS-11.
20220317 014102.898 INFO             PET2 index=   6                              MPIR_CVAR_ALLGATHER_VSHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized gather/bcast algorithm is used instead of the optimized ring algorithm for MPI_Allgather. The gather/bcast algorithm is better suited for small messages. Defaults: For communicator sizes of <=512 ranks, 1024 bytes. For communicator sizes of >512 ranks, 4096 bytes.
20220317 014102.898 INFO             PET2 index=   7                             MPIR_CVAR_ALLGATHERV_VSHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized gatherv/bcast algorithm is used instead of the optimized ring algorithm for MPI_Allgatherv. The gatherv/bcast algorithm is better suited for small messages. Defaults: For communicator sizes of <=512 ranks, 1024 bytes. For communicator sizes of >512 ranks, 4096 bytes.
20220317 014102.898 INFO             PET2 index=   8                                  MPIR_CVAR_ALLREDUCE_NO_SMP : If set, MPI_Allreduce uses an algorithm that is not smp- aware. This provides a consistent ordering of the specified allreduce operation regardless of system configuration. Note:  This algorithm may not perform as well as the default smp-aware algorithms as it does not take advantage of rank topology. Default: not set
20220317 014102.898 INFO             PET2 index=   9                                MPIR_CVAR_ALLTOALL_SHORT_MSG : Adjusts the cut-off points at and below which the store and forward Alltoall algorithm is used for short messages. The default value is dependent upon the total number of ranks in the MPI communicator used for the MPI_Alltoall call and the Alltoall algorithm being selected. Defaults: If using one of the non-default send/recv algorithms on Aries, the defaults are: if communicator size <= 512, 2048 bytes if communicator size > 512 and <= 1024, 1024 bytes if communicator size > 1024 and <= 65536, 128 bytes if communicator size > 65536 and <= 131072, 64 bytes if communicator size > 131072 , 32 bytes
20220317 014102.898 INFO             PET2 index=  10                                MPIR_CVAR_ALLTOALLV_THROTTLE : Sets the per-process maximum number of outstanding Isends and Irecvs that can be posted concurrently for the optimized send/recv MPI_Alltoallv algorithm. For large messages, consider decreasing the throttle to 1 or 2 to improve performance. Defaults: 8
20220317 014102.898 INFO             PET2 index=  11                                   MPIR_CVAR_BCAST_ONLY_TREE : If set to 1, MPI_Bcast uses an smp-aware tree algorithm regardless of data size. The tree algorithm generally scales well to high processor counts on Cray XE systems. If set to 0, MPI_Bcast uses a variety of algorithms (tree, scatter, or ring) depending on message size and other factors. These other algorithms generally do not scale well when using more than 512 processors on Cray XE systems. Default: 1
20220317 014102.898 INFO             PET2 index=  12                             MPIR_CVAR_BCAST_INTERNODE_RADIX : Used to set the radix of the inter-node tree. This can be set to any integer value greater than or equal to 2. Default: 4
20220317 014102.898 INFO             PET2 index=  13                             MPIR_CVAR_BCAST_INTRANODE_RADIX : Used to set the radix of the intra-node tree. This can be set to any integer value greater than or equal to 2. Default: 4
20220317 014102.898 INFO             PET2 index=  14                                      MPIR_CVAR_COLL_OPT_OFF : If set, disables collective optimizations which use nondefault, architecture-specific algorithms for some MPI collective operations. By default, all collective optimized algorithms are enabled. To disable all collective optimized algorithms, set MPICH_COLL_OPT_OFF to 1. To disable optimized algorithms for selected MPI collectives, set the value to a comma-separated list of the desired collective names. Names are not case-sensitive. Any unrecognizable name is flagged with a warning message and ignored. For example, to disable the MPI_Allgather optimized collective algorithm, set MPICH_COLL_OPT_OFF=mpi_allgather. The following collective names are recognized: MPI_Allgather, MPI_Allgatherv, MPI_Allreduce, MPI_Alltoall, MPI_Alltoallv, MPI_Bcast, MPI_Gatherv, MPI_Scatterv, MPI_Igatherv, and MPI_Iallreduce. Default: Not enabled.
20220317 014102.898 INFO             PET2 index=  15                                         MPIR_CVAR_COLL_SYNC : If set, a Barrier is performed at the beginning of each specified MPI collective function. This forces all processes participating in that collective to sync up before the collective can begin. To disable this feature for all MPI collectives, set the value to 0. This is the default. To enable this feature for all MPI collectives, set the value to 1. To enable this feature for selected MPI collectives, set the value to a comma-separated list of the desired collective names. Names are not case-sensitive. Any unrecognizable name is flagged with a warning message and ignored. The following collective names are recognized: MPI_Allgather, MPI_Allgatherv, MPI_Allreduce, MPI_Alltoall, MPI_Alltoallv, MPI_Alltoallw, MPI_Bcast, MPI_Exscan, MPI_Gather, MPI_Gatherv, MPI_Reduce, MPI_Reduce_scatter, MPI_Scan, MPI_Scatter, and MPI_Scatterv. Default: Not enabled.
20220317 014102.898 INFO             PET2 index=  16                                 MPIR_CVAR_GATHERV_SHORT_MSG : Adjusts the cutoff point at and below which the optimized tree MPI_Gatherv algorithm is used instead of the optimized permission-to-send MPI_Gatherv algorithm. The cutoff is based on the average size of the variable MPI_Gatherv message sizes.  Default: 131072 bytes
20220317 014102.898 INFO             PET2 index=  17                                     MPIR_CVAR_REDUCE_NO_SMP : If set, MPI_Reduce uses an algorithm that is not smp-aware. This provides a consistent ordering of the specified reduce operation regardless of system configuration. Note:  This algorithm may not perform as well as the default smp-aware algorithms as it does not take advantage of rank topology. Default: not set
20220317 014102.898 INFO             PET2 index=  18                              MPIR_CVAR_SCATTERV_SYNCHRONOUS : The default, non-optimized ANL MPI_Scatterv algorithm uses asynchronous sends by default for communicator sizes less than 200,000 ranks. If set, this environment variable causes MPI_Scatterv to switch to using blocking sends, which may be beneficial in certain cases involving large data sizes or high process counts. For communicator sizes equal to or greater than 200,000 ranks, the blocking send algorithm is used by default. Default: not enabled
20220317 014102.898 INFO             PET2 index=  19                               MPIR_CVAR_SHARED_MEM_COLL_OPT : If set, the MPICH library will use the optimized shared- memory based design for collective operations. On Gemini and Aries systems, the supported collective operations are: MPI_Allreduce, MPI_Iallreduce, MPI_Barrier, and MPI_Bcast. To enable all available shared-memory optimizations, set MPICH_SHARED_MEM_COLL_OPT to 1. To enable this feature for a specific set of collective operations, set MPICH_SHARED_MEM_COLL_OPT to a comma- separated list of collective names. For example, to enable this optimization for MPI_Bcast only, set MPICH_SHARED_MEM_COLL_OPT=MPI_Bcast. To enable this optimization for MPI_Allreduce only, set MPICH_SHARED_MEM_COLL_OPT=MPI_Allreduce. Unsupported names are flagged with a warning message and ignored. Default: set
20220317 014102.898 INFO             PET2 index=  20                   MPIR_CVAR_ALLREDUCE_GPU_STAGING_THRESHOLD : If the sendbuf/recvbuf of an Allreduce operation is on GPU-resident memory regions, for payload sizes larger than this variable,  the Cray optimized staging implementation will be used.  By default, the staging implementation is used for all payload sizes.  This variable has no effect if MPICH_GPU_COLL_STAGING_AREA_OPT is set to 0. Default: 0
20220317 014102.898 INFO             PET2 index=  21                         MPIR_CVAR_GPU_COLL_STAGING_BUF_SIZE : This variable determines the size of the staging buffers used for some collectives (such as MPI_Allreduce) when sendbuf/recvbuf on GPU-resident buffers. This variable has no effect if  MPICH_GPU_COLL_STAGING_AREA_OPT is set to 0.   Default: 262144
20220317 014102.898 INFO             PET2 index=  22                                 MPIR_CVAR_GATHERV_SYNC_FREQ : Only applicable to the Gatherv permission-to-send algorithm.  Adjusts  the number of outstanding receives the root for Gatherv will allow.   Default is 16.
20220317 014102.898 INFO             PET2 index=  23                              MPIR_CVAR_GATHERV_MAX_TMP_SIZE : Only applicable to the Gatherv tree algorithm.  Sets the maximum amount of temporary memory GAtherv will allow a rank to allocate when using the tree-based algorithm.  Each rank allocates a different amount, with many allocating no extra memory.  If any rank requires more than this amount of temporary buffer space, a different algorithm is used. Default is 512MB.
20220317 014102.898 INFO             PET2 index=  24                             MPIR_CVAR_GATHERV_MIN_COMM_SIZE : Cray MPI offers two optimized Gatherv algorithms.  A tree algorithm for small messages and a permission-to-send send algorithm for larger messages.  Set  this value to the minimum communicator size to attempt use of either of the  Cray optimized Gatherv algorithms. Default is 64
20220317 014102.898 INFO             PET2 index=  25                                MPIR_CVAR_SCATTERV_SHORT_MSG : Adjusts the cutoff point at and below which the optimized tree MPI_Scatterv algorithm is used instead of the optimized staggered send algorithm.  The cutoff is in bytes, based on the average size of the variable MPI_Scatterv message sizes. Default behavior if unset is: For communicator sizes of <= 512 ranks, 2048 bytes For communicator sizes of > 512 ranks, 8192 bytes
20220317 014102.898 INFO             PET2 index=  26                                MPIR_CVAR_SCATTERV_SYNC_FREQ : Only applicable to the Scatterv staggered send algorithm.  Adjusts the number of outstanding sends the root for Scatterv will use. Default is 16.
20220317 014102.898 INFO             PET2 index=  27                             MPIR_CVAR_SCATTERV_MAX_TMP_SIZE : Only applicable to the Scatterv tree algorithm.  Sets the maximum amount of temporary memory Scatterv will allow a rank to allocate when using the tree-based algorithm.  Each rank allocates a different amount, with many allocating no extra memory.  If any rank requires more than this amount of temporary buffer space, a different algorithm is used. Default is 512MB.
20220317 014102.898 INFO             PET2 index=  28                            MPIR_CVAR_SCATTERV_MIN_COMM_SIZE : Cray MPI offers two optimized Scatterv algorithms.  A tree algorithm for small messages and a staggered send algorithm for larger messages.  Set this value to the minimum communicator size to attempt use of either of the Cray optimized Scatterv algorithms. Default is 64
20220317 014102.898 INFO             PET2 index=  29                           MPIR_CVAR_MPIIO_ABORT_ON_RW_ERROR : If set to enable, causes MPI-IO to abort immediately after issuing an error message if an I/O error occurs during a system read() or write() call. This applies only to I/O errors for system read() and write() calls made as a result of MPI I/O calls. It does not apply to I/O errors for other MPI I/O calls such as MPI_File_open(), nor does it apply to read() and write() calls made by means other than MPI I/O calls. Abort on error is not standard behavior. The MPI Standard specifies that the default error handling for MPI I/O calls is to return an error code to the application rather than aborting the application, but since errors on write or read are almost always unexpected and usually not recoverable, it may be preferable to abort as soon as the error is detected. Doing so does not allow any recovery, but does provide the most information about the error and terminates the job quickly. If the Cray Abnormal Termination Processing (ATP) feature is enabled, the abort will result in a full stack backtrace writte
20220317 014102.898 INFO             PET2 index=  30                MPIR_CVAR_MPIIO_AGGREGATOR_PLACEMENT_DISPLAY : This variable controls whether the placement of the aggregators will be displayed when a file is opened. The placement can be  controlled on a per file basis with the aggregator_placement_stride  hint. If set, displays the assignment of MPIIO collective buffering aggregators for reads/writes of a shared file, showing rank and node ID (nid). For example: Aggregator Placement for /lus/scratch/myfile RankReorderMethod=3  AggPlacementStride=-1 AGG    Rank       nid ----  ------  -------- 0       0  nid00578 1       4  nid00579 2       1  nid00606 3       5  nid00607 4       2  nid00578 5       6  nid00579 6       3  nid00606 7       7  nid00607 Default: not set
20220317 014102.898 INFO             PET2 index=  31                 MPIR_CVAR_MPIIO_AGGREGATOR_PLACEMENT_STRIDE : Partially controls to which nodes MPIIO collective buffering aggregators are assigned. See the notes below on the order of nodes. Network traffic and resulting I/O performance may be affected by the assignments. If set to 1, consecutive nodes are used. The number of aggregators assigned per node is controlled by the cb_config_list hint. By default, no more than one aggregator per node will be assigned if there are at least as many nodes as aggregators. If set to a value greater than 1, node selection is strided across the available nodes by this value. If the stride times the number of aggregators exceeds the number of nodes, the assignments will wrap around, which is usually not optimal for performance. If set to -1, node selection is strided across available nodes by the value of the number of nodes divided by the number of aggregators (integer division, minimum value of 1). The purpose is to spread out the nodes to reduce network congestion. Note:  The order of nodes can be shown by setting the MPICH_RANK
20220317 014102.898 INFO             PET2 index=  32                                    MPIR_CVAR_MPIIO_CB_ALIGN : Sets the default value for the cb_align hint. Files opened with MPI_File_open wil have this value for the cb_align hint unless the hint is set on a per file basis with either the MPICH_MPIIO_HINTS environment variable or from within a program with the MPI_Info_set() call. Note:  Only MPICH_MPIIO_CB_ALIGN == 2 is fully supported. Other values are for internal testing only. Default: 2
20220317 014102.898 INFO             PET2 index=  33                                MPIR_CVAR_MPIIO_DVS_MAXNODES : Note:  This environment variable in relevant only for file systems accessed from Cray system compute nodes via DVS server nodes; e.g. GPFS or PANFS. As described in the dvs(5) man page, the environment variable DVS_MAXNODES can be used to set the stripe width— that is, the number of DVS server nodes—used to access a file in "stripe parallel mode." For most files, and especially for small files, setting DVS_MAXNODES to 1 ("cluster parallel mode") is preferred. The MPICH_MPIIO_DVS_MAXNODES environment variable enables you to leave DVS_MAXNODES set to 1 and then use MPICH_MPIIO_DVS_MAXNODES to temporarily override DVS_MAXNODES when it is advantageous to specify wider striping for files being opened by the MPI_File_open() call. The range of values accepted by MPICH_MPIIO_DVS_MAXNODES goes from 1 to the number of server nodes specified on the mount with the nnodes mount option. DVS_MAXNODES is not set by default. Therefore, for MPICH_MPIIO_DVS_MAXNODES to have any effect, DVS_MAXNODES must be defined before p
20220317 014102.898 INFO             PET2 index=  34                                       MPIR_CVAR_MPIIO_HINTS : If set, override the default value of one or more MPI I/O hints. This also overrides any values that were set by using calls to MPI_Info_set in the application code. The new values apply to the file the next time it is opened using an MPI_File_open() call. After the MPI_File_open() call, subsequent MPI_Info_set calls can be used to pass new MPI I/O hints that take precedence over some of the environment variable values. Other MPI I/O hints such as striping_factor, striping_unit, cb_nodes, and cb_config_list cannot be changed after the MPI_File_open() call, as these are evaluated and applied only during the file open process. An MPI_File_close call followed by an MPI_File_open call can be used to restart the MPI I/O hint evaluation process. The syntax for this environment variable is a comma- separated list of specifications. Each individual specification is a pathname_pattern followed by a colon- separated list of one or more key=value pairs. In each key=value pair, the key is the MPI-IO hint name, and the v
20220317 014102.898 INFO             PET2 index=  35                               MPIR_CVAR_MPIIO_HINTS_DISPLAY : If set, causes rank 0 in the participating communicator to display the names and values of all MPI-IO hints that are set for the file being opened with the MPI_File_open call. It also displays relevant environment variables whether or not MPICH_ENV_DISPLAY is set. Default: not enabled.
20220317 014102.898 INFO             PET2 index=  36                               MPIR_CVAR_MPIIO_MAX_NUM_IRECV : When MPIIO collective buffering is used, this environment variable controls the number of outstanding MPI_Irecv calls allowed before an MPI_Waitall is done. Default: 50
20220317 014102.898 INFO             PET2 index=  37                               MPIR_CVAR_MPIIO_MAX_NUM_ISEND : When MPIIO collective buffering is used, this environment variable controls the number of outstanding MPI_Isend calls allowed before an MPI_Waitall is done. Default: 50
20220317 014102.898 INFO             PET2 index=  38                              MPIR_CVAR_MPIIO_MAX_SIZE_ISEND : When MPIIO collective buffering is used, this environment variable limits MPI_Isend by the amount of data being sent rather than by the number of calls. Default: 10485760 bytes
20220317 014102.898 INFO             PET2 index=  39                                       MPIR_CVAR_MPIIO_STATS : If set to 1, a summary of file write and read access patterns is written by rank 0 to stderr. This information provides some insight into how I/O performance may be improved. The information is provided on a per-file basis and is written when the file is closed. It does not provide any timing information. If set to 2, a set of data files are written to the working directory, one file for each rank, with the filename prefix specified by the MPICH_MPIIO_STATS_FILE environment variable. The data is in comma-separated values (CSV) format, which can be summarized with the cray_mpiio_summary script in the /opt/cray/mpt/version/gni/bin directory. Additional example scripts are provided in that directory to further process and display the data. Only enabled if MPIR_CVAR_MPIIO_CB_ALIGN=2. Default: not set
20220317 014102.898 INFO             PET2 index=  40                                  MPIR_CVAR_MPIIO_STATS_FILE : Specifies the filename prefix for the set of data files written when MPICH_MPIIO_STATS is set to 2. The filename prefix may be a full absolute pathname or a relative pathname. Summary plots of these files can be generated using the cray_mpiio_summary script from the /opt/cray/mpt/version/gni/bin directory. Other example scripts for post-processing this data can also be found in /opt/cray/mpt/version/gni/bin. Default: _cray_mpiio_stats_
20220317 014102.898 INFO             PET2 index=  41                         MPIR_CVAR_MPIIO_STATS_INTERVAL_MSEC : Specifies the time interval in milliseconds for each MPICH_MPIIO_STATS data point. Default: 250
20220317 014102.898 INFO             PET2 index=  42                                      MPIR_CVAR_MPIIO_TIMERS : If set to 0, or not set at all, no timing data is collected. If set to 1, timing data for different phases in MPI-IO is collected locally by each MPI process and then during MPI_File_close the data is consolidated and printed. Some timing data is displayed in seconds, other data is displayed in clock ticks, possibly scaled down. Also see MPICH_MPIIO_TIMERS_SCALE The relative values of the reported times are more important to the analysis than the absolute time. More detailed information about MPI-IO performance can be obtained by using the MPICH_MPIIO_STATS feature and by using the CrayPat and Apprentice2 Timeline Report of I/O bandwidth. Only enabled if MPIR_CVAR_MPIIO_CB_ALIGN=2. Default: 0
20220317 014102.898 INFO             PET2 index=  43                                MPIR_CVAR_MPIIO_TIMERS_SCALE : Specifies the power of 2 to use to scale the times reported by MPICH_MPIIO_TIMERS.  The raw times are collected in clock ticks. This generally is a very large number and reducing all the times by the same scaling factor makes for a more compact display. If set to 0, or not set at all, MPI-IO automatically determines a scaling factor to limit the report times to 9 or fewer digits. This auto-determined value is displayed.  To make run to run comparisons, you can set the scaling factor to your preferred value. Default: 0
20220317 014102.898 INFO             PET2 index=  44                                  MPIR_CVAR_MPIIO_TIME_WAITS : If set to non-zero, time how long this rank has to wait for other ranks to catch up.  This separates true metadata time from imbalance time. This is disabled when MPICH_MPIIO_TIMERS is not set.  Otherwise it defaults to 1. Default: 1
20220317 014102.898 INFO             PET2 index=  45                          MPIR_CVAR_MPIIO_WRITE_EXIT_BARRIER : If set to non-zero, collective write's will barrier on exit Default: 1
20220317 014102.898 INFO             PET2 index=  46                               MPIR_CVAR_MPIIO_DS_WRITE_CRAY : If set to non-zero, collective write's with data sieving will be optimized  Default: 1
20220317 014102.899 INFO             PET2 index=  47                         MPIR_CVAR_MPIIO_OFI_STARTUP_CONNECT : If set to enable, causes MPI-IO to establish connections between ranks and aggregators during the openning of the file to be used for MPIIO collective operations
20220317 014102.899 INFO             PET2 index=  48                MPIR_CVAR_MPIIO_OFI_STARTUP_NODES_AGGREGATOR : If MPIIO_OFI_STARTUP_CONNECT is enabled, this specifies the number of nodes that will concurrently attempt to connext to each aggrgator. Each rank on each node will establish its own connection to the aggregator. Default: 2
20220317 014102.899 INFO             PET2 index=  49                                           MPIR_CVAR_DPM_DIR : Sets the directory to use for MPI port name publishing in the  file-based nameserv implementation, as well as publishing the  credential obtained from libdrc.
20220317 014102.899 INFO             PET2 index=  50                               MPIR_CVAR_SINGLE_HOST_ENABLED : If true and if running as a single node, then inter-node communications is never initialized. The helps prevent the use of scarce hardware resources.
20220317 014102.899 INFO             PET2 index=  51                                       MPIR_CVAR_OFI_VERBOSE : If set to 1, enable verbose output about the CH4 OFI device. If set to 2 or higher, enable more verbose output.
20220317 014102.899 INFO             PET2 index=  52                                 MPIR_CVAR_OFI_XRCD_BASE_DIR : Specifies the base directory for the XRCD file.  This is only  used if the job is not launched via PALS launcher.  If launched  via PALS, we use the private, temporary PALS_SPOOL_DIR directory. Default: /tmp
20220317 014102.899 INFO             PET2 index=  53                                   MPIR_CVAR_OFI_NIC_VERBOSE : If set to 1 or higher, enables verbose output about the CH4 OFI NIC selection.   Non-fatal warnings are also displayed.  If set to 2 or higher, each rank in  the job will display it's NIC selection.  If set to 3 or higher, more verbose  output is enabled (for debug purposes).
20220317 014102.899 INFO             PET2 index=  54                                    MPIR_CVAR_OFI_NIC_POLICY : Specifies the rank-to-nic policy.  The supported options are:  NUMA | GPU | BLOCK | ROUND-ROBIN | USER
20220317 014102.899 INFO             PET2 index=  55                                   MPIR_CVAR_OFI_NIC_MAPPING : Specifies the precise rank-to-NIC mapping to use on each node.  This is  required if the NIC_POLICY is set to USER.  Each local rank must have a NIC mapping assigned by this env variable. If there are fewer MPI ranks on any  node, that part of the MAPPING string will be ignored.  For example: To assign local ranks 0,16,32,48 to NIC 0, and remaining ranks to NIC 1 MPICH_OFI_NIC_MAPPING="0:0,16,32,48; 1:1-15,17-31,33-47,49-63"
20220317 014102.899 INFO             PET2 index=  56                                      MPIR_CVAR_OFI_NUM_NICS : If set, specifies the number of NICs the job can use on each node. By default, when multiple NICs per node are available, MPI attempts to use them all.  If using fewer NICs is desired, this env variable  can be set to indicate the maximum number of NICs per node MPI will  use. By default, consecutive NICs are used, starting with index 0.   To limit use to 1 NIC/node, use : export MPICH_OFI_NUM_NICS=1 To limit use to 2 NICs/node, use: export MPICH_OFI_NUM_NICS=2 To specify alternative NIC index values, you may indicate the desired index values by adding a colon, followed by the NIC indexes.  Use  quotes so the shell doesn't interfere.  For example: To limit use to 1 NIC/node, index 1    : export MPICH_OFI_NUM_NICS="1:1" To limit use to 2 NICs/node, index 1&3 : export MPICH_OFI_NUM_NICS="2:1,3"
20220317 014102.899 INFO             PET2 index=  57                        MPIR_CVAR_OFI_SKIP_NIC_SYMMETRY_TEST : If set to 1, CrayMPICH will bypass it's check for NIC symmetry on all nodes in the job. By default, this check is run during MPI_Init to  make sure all the nodes in the job have the same number of NICs available.
20220317 014102.899 INFO             PET2 index=  58                               MPIR_CVAR_OFI_STARTUP_CONNECT : If set to 1, CrayMPICH will create COMM_WORLD static OFI connections to every other rank during MPI_Init.
20220317 014102.899 INFO             PET2 index=  59                           MPIR_CVAR_OFI_RMA_STARTUP_CONNECT : If set to 1, CrayMPICH RMA will create static OFI connections to every other rank on the same node during MPI_Init. This will also enable MPIR_CVAR_OFI_STARTUP_CONNECT.
20220317 014102.899 INFO             PET2 index=  60                                MPIR_CVAR_OFI_DEFAULT_TCLASS : If set, CrayMPICH will assign the requested default TC (traffic class) to the job domain.  By default, all endpoints inherit this TC.
20220317 014102.899 INFO             PET2 index=  61                                 MPIR_CVAR_OFI_TCLASS_ERRORS : Determines how CrayMPICH responds to traffic class errors.  Valid values are "warn", "silent" or "error".
20220317 014102.899 INFO             PET2 index=  62                                  MPIR_CVAR_OFI_CXI_PID_BASE : Set to the base value that MPI will use to assign portal IDs for the CXI provider.  Max PID value is 510.  MPI uses PID_BASE+lrank as the unique value per rank per node.  This is only applicable to SS11.
20220317 014102.899 INFO             PET2 index=  63                          MPIR_CVAR_OFI_USE_SCALABLE_STARTUP : Set to false (0) to bypass the scalable startup feature for CXI. This is only applicable to SS11.
20220317 014102.899 INFO             PET2 index=  64                                       MPIR_CVAR_UCX_VERBOSE : If set to 1, enable verbose output about the CH4 UCX device. If set to 2 or higher, enable more verbose output.
20220317 014102.899 INFO             PET2 index=  65                                  MPIR_CVAR_UCX_RC_MAX_RANKS : By default, CrayMPI selects either the UCX RC or UD protocols for inter-node messaging.  If a job is launched with MPICH_UCX_RC_MAX_RANKS  ranks or fewer, the RC protocol is selected.  If more ranks are launched,  the UCX UD protocol is chosen.  The RC protocol does not scale well due to the resource requirements.  Note this default can be overridden by setting the UCX_TLS environment variable.
20220317 014102.899 INFO             PET2 index=  66                              MPIR_CVAR_SMP_SINGLE_COPY_MODE : If non-null, choose an on-node copy mode for large messages. This variable can be set to XPMEM, CMA, or NONE. By default, Cray MPICH will look to use XPMEM. If XPMEM is requested, but not available, Cray MPICH will attempt to use CMA. (NOTE: Cray MPICH does not support CMA currently. The env. variables and CVARs are set up to allow CMA usage with future versions of Cray MPICH) If both XPMEM and CMA cannot be used, Cray MPICH will fall back to using the ANL shared memory implementation (2-copy) Default: NULL
20220317 014102.899 INFO             PET2 index=  67                              MPIR_CVAR_SMP_SINGLE_COPY_SIZE : Specifies the minimum message size in bytes to consider for single-copy transfers for on-node messages. This applies only to the SMP (on-node shared memory) device. The value is interpreted as bytes, unless the string ends in a K, which indicates kilobytes, or M, which indicates megabytes. Default: 8192
20220317 014102.899 INFO             PET2 index=  68                       MPIR_CVAR_SHM_PROGRESS_MAX_BATCH_SIZE : Adjusts the maximum number of on-node requests that can be processed in a single batch. Higher values for the maximum batch size can lower the overhead due to entering the progress engine, but can also  delay the processing of off-node message requests. Default is 8.
20220317 014102.899 INFO             PET2 index=  69                                MPIR_CVAR_CH4_RMA_THREAD_HOT : If true, the big lock is disabled for certain RMA operations
20220317 014102.899 INFO             PET2 index=  70                                    MPIR_CVAR_ABORT_ON_ERROR : If set, causes MPICH to abort and produce a core dump when MPICH detects an internal error. Note that the core dump size limit (usually 0 bytes by default) must be reset to an appropriate value in order to enable coredumps. Default: Not enabled.
20220317 014102.899 INFO             PET2 index=  71                                   MPIR_CVAR_CPUMASK_DISPLAY : If set, causes each MPI rank in the job to display its CPU affinity bitmask. Note that this reports only the CPU affinity masks for the MPI ranks; if you have a hybrid program, it does not provide any thread information. The bitmask is read from right to left, meaning the value in the rightmost position corresponds to CPU 0 on the node.
20220317 014102.899 INFO             PET2 index=  72                                       MPIR_CVAR_ENV_DISPLAY : If set, causes rank 0 to display all MPICH environment variables and their current settings at MPI initialization time. If two or more nodes are used, MPICH/GNI environment settings are also included in the listing. Default: Not enabled.
20220317 014102.899 INFO             PET2 index=  73                                  MPIR_CVAR_OPTIMIZED_MEMCPY : Specifies which version of memcpy to use. Valid values are: 0         Use the system (glibc) version of memcpy. 1         Use an optimized version of memcpy if one is available for the processor being used. In this release, an optimized version of memcpy() is available only for Intel processors. 2         Use a highly optimized version of memcpy that provides better performance in some areas but may have performance regressions in other areas, if one is available for the processor being used. In this release, a highly optimized version of memcpy() is available only for Intel Haswell processors. MPICH_OPTIMIZED_MEMCPY is overridden by MPICH_USE_SYSTEM_MEMCPY. If MPICH_USE_SYSTEM_MEMCPY is set, MPICH_OPTIMIZED_MEMCPY is ignored and the system (glibc) version of memcpy() is used. Default: 1
20220317 014102.899 INFO             PET2 index=  74                                     MPIR_CVAR_STATS_DISPLAY : If set to 1, a summary of MPI statistics, also available through the MPI Tools Interface, will be written by rank 0 to stderr. If set to 2, all ranks will produce an individualized statistics summary and write to file on a per-rank basis. The MPICH_STATS_FILE determines the prefix of the file to be used. This information may provide insight into how MPI performance may be improved. Default: 0
20220317 014102.899 INFO             PET2 index=  75                                   MPIR_CVAR_STATS_VERBOSITY : Specifies the verbosity of the MPI statistics summary. This information may provide insight into how MPI performance may be improved. Increase the value for more detailed summary. Default: 1 1        USER_BASIC  (default) 2        USER_DETAIL 3        USER_ALL 4        TUNER_BASIC 5        TUNER_DETAIL 6        TUNER_ALL 7        MPIDEV_BASIC 8        MPIDEV_DETAIL 9        MPIDEV_ALL
20220317 014102.899 INFO             PET2 index=  76                                        MPIR_CVAR_STATS_FILE : Specifies the filename prefix for the set of data files written when MPICH_STATS_DISPLAY is set to 2. The filename prefix may be a full absolute pathname or a relative pathname. Default: _cray_stats_
20220317 014102.899 INFO             PET2 index=  77                              MPIR_CVAR_RANK_REORDER_DISPLAY : If set, causes rank 0 to display which node each MPI rank resides in. The rank order can be manipulated via the MPICH_RANK_REORDER_METHOD environment variable or MPIR_CVAR_RANK_REORDER_METHOD control variable. Default: Not set
20220317 014102.899 INFO             PET2 index=  78                               MPIR_CVAR_RANK_REORDER_METHOD : Overrides the default MPI rank placement scheme. If this variable is not set, the default aprun launcher placement policy is used. The default policy for aprun is SMP-style placement. To display the MPI rank placement information, set MPICH_RANK_REORDER_DISPLAY. See manpage for more details. Default: 1, for SMP-style placement.
20220317 014102.899 INFO             PET2 index=  79                                 MPIR_CVAR_USE_SYSTEM_MEMCPY : Note:  This environment variable is deprecated and scheduled to be removed in a future release. Use MPICH_OPTIMIZED_MEMCPY instead. If set, use the system (glibc) version of memcpy(); otherwise, an optimized version of memcpy() may be used. Currently, an optimized version of memcpy() is available only for Intel processors. Default: Not set
20220317 014102.899 INFO             PET2 index=  80                                   MPIR_CVAR_VERSION_DISPLAY : If set, causes MPICH to display the CRAY MPICH version number as well as build date information. Default: Not enabled
20220317 014102.899 INFO             PET2 index=  81                          MPIR_CVAR_USE_GPU_STREAM_TRIGGERED : If set, causes MPICH to allow using stream triggered GPU communication operations. Default: Not enabled
20220317 014102.899 INFO             PET2 index=  82                               MPIR_CVAR_NUM_MAX_GPU_STREAMS : If set, causes MPICH to allow using the set maximum number of streams concurrently in the stream triggered GPU communication operations. Default: 26
20220317 014102.899 INFO             PET2 index=  83                                  MPIR_CVAR_MEMCPY_MEM_CHECK : If set, enables a check of the memcpy() source and destination areas. If they overlap, the application asserts with an error message listing the file, line, and memory range overlap. If this error is found, correct it either by changing the memory ranges or possibly by using MPI_IN_PLACE. Default: not set (off)
20220317 014102.899 INFO             PET2 index=  84                                     MPIR_CVAR_MSG_QUEUE_DBG : If set, turns on TotalView Message Queue Debugging support so that message queues are tracked in the TotalView debugger and a message queue graph can be generated. Enabling this feature degrades performance. Default: not enabled.
20220317 014102.899 INFO             PET2 index=  85                             MPIR_CVAR_NO_BUFFER_ALIAS_CHECK : If set, the buffer alias error check for collectives is disabled. The MPI standard does not allow aliasing of type OUT or INOUT parameters on the same collective function call. The use of MPI_IN_PLACE is required in these scenarios. A new check was added in MPT 5.2 to detect this condition and report the error. To bypass this check, set MPICH_NO_BUFFER_ALIAS_CHECK to any value. Default: not set
20220317 014102.899 INFO             PET2 index=  86                                MPIR_CVAR_ALLOC_MEM_AFFINITY : Controls the affinity of the memory region allocated by the MPI_Alloc_mem() or MPI_Win_allocate() operations. On systems that do not offer High Bandwidth Memory capabilities, (such as Intel Haswell or Broadwell processors), this env. variable is ignored and memory affinity is set to DDR. On Phi processors, such as KNL (and KNH, KNP in the future), this env. variable allows users to specifically request the memory returned by MPI_Alloc_mem() and MPI_Win_allocate() to be bound to either DDR, or the MCDRAM. Users can request a specific page size or memory binding policy via the MPICH_ALLOC_MEM_POLICY and MPICH_ALLOC_MEM_PG_SZ env. variables. Default: SYS_DEFAULT
20220317 014102.899 INFO             PET2 index=  87                             MPIR_CVAR_INTERNAL_MEM_AFFINITY : Controls the affinity of internal memory regions allocated by the MPI library. This variable currently affects the memory affinity of the mail-boxes used for off-node communication, and the shared-memory regions that are used for on-node pt2pt and collective ops. On systems that do not offer High Bandwidth Memory capabilities, (such as Intel Haswell or Broadwell processors), this env. variable is ignored and memory affinity is set to DDR. On Phi processors, such as KNL (and KNH, KNP in the future), this env. variable allows users to specifically request the internal memory regions used by the MPI library to be bound to either DDR, or the MCDRAM. The default affinity settings will be governed by the system defaults. For example, on a KNL system configured in the Quad/Flat mode, if the job is run with numactl --membind=1, all of MPI's internal memory will be bound to MCDRAM if this variable is not set. Default: SYS_DEFAULT
20220317 014102.899 INFO             PET2 index=  88                                  MPIR_CVAR_ALLOC_MEM_POLICY : Controls the memory affinity policy on systems with specialized memory hardware. By default, the memory policy is set to "{P}referred". Other accepted values are "{M}andatory" and "{I}"nterleave. Default: Preferred
20220317 014102.899 INFO             PET2 index=  89                                   MPIR_CVAR_ALLOC_MEM_PG_SZ : Controls the page size for the MPI_Alloc_mem() and MPI_Win_allocate() operations. This parameters defaults to 4KB base pages. The supported values are 2M, 4M, 8M, 16M, 32M, 64M, 128M, 256M, 512M, 1G and 2G. Default: 4096
20220317 014102.899 INFO             PET2 index=  90                                   MPIR_CVAR_OPT_THREAD_SYNC : Controls the mechanism used to implement thread-synchronization inside the Cray MPICH library. If set to 1, an optimized synchronization implementation is used. If set to 0, Cray MPICH falls back to using a pthread_mutex-based thread-synchronization implementation. Default: 1
20220317 014102.899 INFO             PET2 index=  91                                 MPIR_CVAR_THREAD_YIELD_FREQ : Determines how often a thread yields while waiting to acquire a lock in the new Cray optimized locking impl. This variable has no effect if MPICH_CRAY_OPT_THREAD_SYNC is 0. Default: 10000
20220317 014102.899 INFO             PET2 index=  92                                   MPIR_CVAR_MEM_DEBUG_FNAME : If set, the MPI library creates new files with the user specified name and writes various important memory statistics into these files. This information can be useful for post-processing. Users can set MPICH_MEM_DEBUG_FNAME to any suitable string. The resulting files are named as "string".<pid>.<MPI-rank>. For example, if MPICH_MEM_DEBUG_FNAME is set to "MEM_DBG_MSGS" the debug file for rank0 will be named "MEM_DBG_MSGS.<pid>.0 and written to the user's current working directory. If this flag is not set, the MPI library will redirect all the debug messages to stderr. Default: unset (disabled)
20220317 014102.899 INFO             PET2 index=  93                                   MPIR_CVAR_MALLOC_FALLBACK : Set the policy for fallback behavior when attempting to allocate large pages for internal buffers and insufficient large pages are available to satisfy the request. If set to enabled, MPICH falls back to using malloc in such cases. Default: not enabled (process fails and job terminates if insufficient large pages are available to satisfy the request)
20220317 014102.899 INFO             PET2 index=  94                              MPIR_CVAR_NO_DIRECT_GPU_ACCESS : If true, IPC and NIC-GPU Direct Access capabilities are not used for GPU-to-GPU transfers. This is mainly used for debugging.  Default: 0
20220317 014102.899 INFO             PET2 index=  95                                      MPIR_CVAR_G2G_PIPELINE : If nonzero, the device-host and network transfers will be overlapped to pipeline GPU-to-GPU transfers. Setting MPICH_G2G_PIPELINE to N will allow N GPU-to-GPU messages to be efficiently in-flight at any one time. If MPICH_G2G_PIPELINE is nonzero but MPICH_GPU_SUPPORT_ENABLED is disabled, MPICH_G2G_PIPELINE will be turned off. If MPICH_GPU_SUPPORT_ENABLED is enabled but MPICH_G2G_PIPELINE is 0, the default value is set to 8.  The pipeline-based implementation is being carried over from  CH3/Aries implementation. We think the pipelined-based implementation will not be necessary on SS-11 systems.  This logic may get dropped in the future.  Default: not set
20220317 014102.899 INFO             PET2 index=  96                               MPIR_CVAR_GPU_SUPPORT_ENABLED : If set, allows the MPI application to pass GPU pointers directly to MPI communication functions.  Default: not set
20220317 014102.899 INFO             PET2 index=  97                MPIR_CVAR_GPU_MANAGED_MEMORY_SUPPORT_ENABLED : If set, allows the MPI application to pass pointers allocated via  GPU managed memory allocators to MPI communication functions. Default: not set
20220317 014102.899 INFO             PET2 index=  98                         MPIR_CVAR_GPU_COLL_STAGING_AREA_OPT : If set, allows Cray MPI to use pre-registered CPU-attached memory to optimize collective ops.  Default: set
20220317 014102.899 INFO             PET2 index=  99               MPIR_CVAR_GPU_COLL_REGISTER_SHARED_MEM_REGION : If set, allows Cray MPI to registere CPU-attached shared memory regions  used to optimize collective ops.  Default: set
20220317 014102.899 INFO             PET2 index= 100                                   MPIR_CVAR_GPU_IPC_ENABLED : If set to 1, Cray MPICH will use GPU IPC to move data between GPU devices that are within the same node. If set to 0, Cray MPICH will use staging buffers on the host to implement data transfer between GPU devices that are within the same node. If MPIR_CVAR_GPU_SUPPORT_ENABLED is set, Cray MPICH  automatically attempts to enable the IPC optimizations.   If MPIR_CVAR_NO_DIRECT_GPU_ACCESS is set, IPC optimizations are disabled.  Default: -1
20220317 014102.899 INFO             PET2 index= 101                                 MPIR_CVAR_GPU_IPC_THRESHOLD : Specifies the minimum message size in bytes to consider for single-copy transfers between GPU devices that are within the same compute node. The value is interpreted as bytes, unless the string ends in a K, which indicates kilobytes, or M, which indicates megabytes.
20220317 014102.899 INFO             PET2 index= 102                                  MPIR_CVAR_GPU_IPC_PROTOCOL : Specifies the protocol used for large msg on-node, inter-GPU  data transfers via GPU IPC.  Valid options are RPUT or RGET.  Default: RGET
20220317 014102.899 INFO             PET2 index= 103                                 MPIR_CVAR_GPU_NO_ASYNC_COPY : If set, disables the use of asynchronous memcpy's for on-node GPU-aware transfers. Default: 0
20220317 014102.899 INFO             PET2 index= 104                                      MPIR_CVAR_ENABLE_YAKSA : If set, enables the use of the YAKSA datatype engine for packing and unpacking non-contiguous buffers in both CPU- and GPU-attached memory. Default: 0
20220317 014102.899 INFO             PET2 index= 105                              MPIR_CVAR_GPU_EAGER_DEVICE_MEM : If set to 1, GPU-attached memory will be used for the eager staging buffers for short on-node messages where at least the sending buffer is GPU-attached. If set to 0, this optimization is disabled and we fall-back to using staging buffers on the CPU for small message  intra-node device-host and device-device MPI ops. Default: 1
20220317 014102.899 INFO             PET2 index= 106                       MPIR_CVAR_GPU_EAGER_REGISTER_HOST_MEM : If set to 1, enables MPI to request POSIX  eager memory to be registered with the GPU runtime.  This is an optimization to improve the performance of  small message intra-node, inter-GPU MPI ops that use  POSIX shared memory as bounce buffers.  If set to 0, this optimization is disabled and we rely on GTL to lock/unlock host memory regions for a given on-node, small msg inter-GPU transfer.  Default: 1
20220317 014102.899 INFO             PET2 index= 107                          MPIR_CVAR_GPU_ALLREDUCE_USE_KERNEL : If set, adds a hint that the use of device kernels for reduction operations is desired. MPI is not guaranteed to use a device kernel for all reduction operations. Default: 0
20220317 014102.899 INFO             PET2 index= 108                                   MPIR_CVAR_RMA_MAX_PENDING : Determines how many RMA operation may be outstanding at any time over libfabrics. RMA operations beyond this max will be queued and only issued as pending operations complete. Default: 64
20220317 014102.899 INFO             PET2 index= 109                                MPIR_CVAR_RMA_SHM_ACCUMULATE : If set to 1, enables SHM accumulate operations. If set to 0, disables SHM accumulate operations. It also sets the default for the window hint "disable_shm_accumulate".  Default: 1
20220317 014102.899 INFO             PET2 --- VMK::logSystem() end ---------------------------------
20220317 014102.899 INFO             PET2 main: --- VMK::log() start -------------------------------------
20220317 014102.899 INFO             PET2 main: vm located at: 0x1176ae0
20220317 014102.899 INFO             PET2 main: petCount=6 localPet=2 mypthid=23000400322688 currentSsiPe=2
20220317 014102.899 INFO             PET2 main: Current system level affinity pinning for local PET:
20220317 014102.899 INFO             PET2 main:  SSIPE=2
20220317 014102.899 INFO             PET2 main: Current system level OMP_NUM_THREADS setting for local PET: 128
20220317 014102.899 INFO             PET2 main: ssiCount=1 localSsi=0
20220317 014102.899 INFO             PET2 main: mpionly=1 threadsflag=0
20220317 014102.899 INFO             PET2 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220317 014102.899 INFO             PET2 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220317 014102.899 INFO             PET2 main:  PE=0 SSI=0 SSIPE=0
20220317 014102.899 INFO             PET2 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220317 014102.899 INFO             PET2 main:  PE=1 SSI=0 SSIPE=1
20220317 014102.899 INFO             PET2 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220317 014102.899 INFO             PET2 main:  PE=2 SSI=0 SSIPE=2
20220317 014102.899 INFO             PET2 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220317 014102.899 INFO             PET2 main:  PE=3 SSI=0 SSIPE=3
20220317 014102.899 INFO             PET2 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220317 014102.899 INFO             PET2 main:  PE=4 SSI=0 SSIPE=4
20220317 014102.899 INFO             PET2 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220317 014102.899 INFO             PET2 main:  PE=5 SSI=0 SSIPE=5
20220317 014102.899 INFO             PET2 main: --- VMK::log() end ---------------------------------------
20220317 014102.899 INFO             PET2 Executing 'userm1_setvm'
20220317 014102.900 INFO             PET2 Executing 'userm1_register'
20220317 014102.900 INFO             PET2 Executing 'userm2_setvm'
20220317 014102.900 DEBUG            PET2 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220317 014102.900 DEBUG            PET2 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220317 014102.901 INFO             PET2 Entering 'user1_run'
20220317 014102.901 INFO             PET2 model1: --- VMK::log() start -------------------------------------
20220317 014102.901 INFO             PET2 model1: vm located at: 0x11d21a0
20220317 014102.901 INFO             PET2 model1: petCount=6 localPet=2 mypthid=23000400322688 currentSsiPe=2
20220317 014102.901 INFO             PET2 model1: Current system level affinity pinning for local PET:
20220317 014102.901 INFO             PET2 model1:  SSIPE=2
20220317 014102.901 INFO             PET2 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220317 014102.901 INFO             PET2 model1: ssiCount=1 localSsi=0
20220317 014102.901 INFO             PET2 model1: mpionly=1 threadsflag=0
20220317 014102.901 INFO             PET2 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220317 014102.901 INFO             PET2 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220317 014102.901 INFO             PET2 model1:  PE=0 SSI=0 SSIPE=0
20220317 014102.901 INFO             PET2 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220317 014102.901 INFO             PET2 model1:  PE=1 SSI=0 SSIPE=1
20220317 014102.901 INFO             PET2 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220317 014102.901 INFO             PET2 model1:  PE=2 SSI=0 SSIPE=2
20220317 014102.901 INFO             PET2 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220317 014102.901 INFO             PET2 model1:  PE=3 SSI=0 SSIPE=3
20220317 014102.901 INFO             PET2 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220317 014102.901 INFO             PET2 model1:  PE=4 SSI=0 SSIPE=4
20220317 014102.901 INFO             PET2 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220317 014102.901 INFO             PET2 model1:  PE=5 SSI=0 SSIPE=5
20220317 014102.901 INFO             PET2 model1: --- VMK::log() end ---------------------------------------
20220317 014102.901 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220317 014103.268 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220317 014103.575 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220317 014103.882 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220317 014104.189 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220317 014104.496 INFO             PET2 Exiting 'user1_run'
20220317 014106.141 INFO             PET2 Entering 'user1_run'
20220317 014106.141 INFO             PET2 model1: --- VMK::log() start -------------------------------------
20220317 014106.141 INFO             PET2 model1: vm located at: 0x11d21a0
20220317 014106.141 INFO             PET2 model1: petCount=6 localPet=2 mypthid=23000400322688 currentSsiPe=2
20220317 014106.141 INFO             PET2 model1: Current system level affinity pinning for local PET:
20220317 014106.141 INFO             PET2 model1:  SSIPE=2
20220317 014106.141 INFO             PET2 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220317 014106.141 INFO             PET2 model1: ssiCount=1 localSsi=0
20220317 014106.141 INFO             PET2 model1: mpionly=1 threadsflag=0
20220317 014106.141 INFO             PET2 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220317 014106.141 INFO             PET2 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220317 014106.141 INFO             PET2 model1:  PE=0 SSI=0 SSIPE=0
20220317 014106.141 INFO             PET2 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220317 014106.141 INFO             PET2 model1:  PE=1 SSI=0 SSIPE=1
20220317 014106.141 INFO             PET2 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220317 014106.141 INFO             PET2 model1:  PE=2 SSI=0 SSIPE=2
20220317 014106.141 INFO             PET2 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220317 014106.141 INFO             PET2 model1:  PE=3 SSI=0 SSIPE=3
20220317 014106.141 INFO             PET2 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220317 014106.141 INFO             PET2 model1:  PE=4 SSI=0 SSIPE=4
20220317 014106.141 INFO             PET2 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220317 014106.141 INFO             PET2 model1:  PE=5 SSI=0 SSIPE=5
20220317 014106.141 INFO             PET2 model1: --- VMK::log() end ---------------------------------------
20220317 014106.141 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220317 014106.448 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220317 014106.755 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220317 014107.063 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220317 014107.370 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220317 014107.677 INFO             PET2 Exiting 'user1_run'
20220317 014109.291 INFO             PET2  NUMBER_OF_PROCESSORS           6
20220317 014109.291 INFO             PET2  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220317 014109.291 INFO             PET2 Finalizing ESMF
20220317 014102.897 INFO             PET3 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220317 014102.897 INFO             PET3 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220317 014102.897 INFO             PET3 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220317 014102.897 INFO             PET3 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220317 014102.897 INFO             PET3 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220317 014102.897 INFO             PET3 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220317 014102.897 INFO             PET3 Running with ESMF Version   : v8.3.0b09-100-g5ff85a963f
20220317 014102.897 INFO             PET3 ESMF library build date/time: "Mar 17 2022" "01:27:56"
20220317 014102.897 INFO             PET3 ESMF library build location : /lfs/h1/emc/ptmp/Mark.Potts/gfortran_10.3.0_mpi_O_jedwards_pio_update2
20220317 014102.897 INFO             PET3 ESMF_COMM                   : mpi
20220317 014102.897 INFO             PET3 ESMF_MOAB                   : enabled
20220317 014102.897 INFO             PET3 ESMF_LAPACK                 : enabled
20220317 014102.897 INFO             PET3 ESMF_NETCDF                 : enabled
20220317 014102.897 INFO             PET3 ESMF_PNETCDF                : disabled
20220317 014102.897 INFO             PET3 ESMF_PIO                    : enabled
20220317 014102.897 INFO             PET3 ESMF_YAMLCPP                : enabled
20220317 014102.898 INFO             PET3 --- VMK::logSystem() start -------------------------------
20220317 014102.898 INFO             PET3 esmfComm=mpi
20220317 014102.898 INFO             PET3 isPthreadsEnabled=1
20220317 014102.898 INFO             PET3 isOpenMPEnabled=1
20220317 014102.898 INFO             PET3 isOpenACCEnabled=0
20220317 014102.898 INFO             PET3 isSsiSharedMemoryEnabled=1
20220317 014102.898 INFO             PET3 ssiCount=1 peCount=6
20220317 014102.898 INFO             PET3 PE=0 SSI=0 SSIPE=0
20220317 014102.898 INFO             PET3 PE=1 SSI=0 SSIPE=1
20220317 014102.898 INFO             PET3 PE=2 SSI=0 SSIPE=2
20220317 014102.898 INFO             PET3 PE=3 SSI=0 SSIPE=3
20220317 014102.898 INFO             PET3 PE=4 SSI=0 SSIPE=4
20220317 014102.898 INFO             PET3 PE=5 SSI=0 SSIPE=5
20220317 014102.898 INFO             PET3 --- VMK::logSystem() MPI Control Variables ---------------
20220317 014102.898 INFO             PET3 index=   0          MPIR_CVAR_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE : specifies the cutoff size of the send buffer (in bytes) above which the reduce_scatter functions attempt to use the pairwise exchange algorithm.  In addition, the op must be commutative and the communicator size < MPIR_CVAR_REDUCE_SCATTER_MAX_COMMSIZE for the pairwise exchange algorithm to be used.
20220317 014102.898 INFO             PET3 index=   1                       MPIR_CVAR_REDUCE_SCATTER_MAX_COMMSIZE : specifies the max communicator size that will trigger use of the pairwise exchange algorithm, provided the op is commutative.  The pairwise exchange algorithm is not well suited for scaling to high process counts, so for larger communicators, a recursive halving algorithm is used instead.
20220317 014102.898 INFO             PET3 index=   2                           MPIR_CVAR_NETWORK_BUFFER_COLL_OPT : If set to 1, the MPICH library will use the optimized shared- memory based "network buffer" design for collective operations.  This feature is closely tied to the shared-memory collective optimization available in Cray MPICH. If enabled, the shared-memory buffer is also registered with the NIC and can be used directly to  perform off-node transfers, bypassing the Nemesis channel layer.  This feature is disabled if MPICH_SHARED_MEM_COLL_OPT  is set to 0. Currently, this optimization is only available  for the MPI_Bcast collective operation. To disable this feature,  set MPICH_NETWORK_BUFFER_COLL_OPT to 0.  Default: 0
20220317 014102.898 INFO             PET3 index=   3                                MPIR_CVAR_ALLTOALL_SYNC_FREQ : Adjusts the number of outstanding messages each Alltoall process  will allow.  Default is variable.
20220317 014102.898 INFO             PET3 index=   4                                 MPIR_CVAR_ALLTOALL_BLK_SIZE : The transfer size in bytes for the Alltoall chunking algorithm.  Default is 16384.
20220317 014102.898 INFO             PET3 index=   5                       MPIR_CVAR_ALLTOALL_CHUNKING_MAX_NODES : The maximum number of nodes to use the alltoall chunking algorithm. Above this value, the throttled algorithm will be used.  This is only applicable to SS-11.
20220317 014102.898 INFO             PET3 index=   6                              MPIR_CVAR_ALLGATHER_VSHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized gather/bcast algorithm is used instead of the optimized ring algorithm for MPI_Allgather. The gather/bcast algorithm is better suited for small messages. Defaults: For communicator sizes of <=512 ranks, 1024 bytes. For communicator sizes of >512 ranks, 4096 bytes.
20220317 014102.898 INFO             PET3 index=   7                             MPIR_CVAR_ALLGATHERV_VSHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized gatherv/bcast algorithm is used instead of the optimized ring algorithm for MPI_Allgatherv. The gatherv/bcast algorithm is better suited for small messages. Defaults: For communicator sizes of <=512 ranks, 1024 bytes. For communicator sizes of >512 ranks, 4096 bytes.
20220317 014102.898 INFO             PET3 index=   8                                  MPIR_CVAR_ALLREDUCE_NO_SMP : If set, MPI_Allreduce uses an algorithm that is not smp- aware. This provides a consistent ordering of the specified allreduce operation regardless of system configuration. Note:  This algorithm may not perform as well as the default smp-aware algorithms as it does not take advantage of rank topology. Default: not set
20220317 014102.898 INFO             PET3 index=   9                                MPIR_CVAR_ALLTOALL_SHORT_MSG : Adjusts the cut-off points at and below which the store and forward Alltoall algorithm is used for short messages. The default value is dependent upon the total number of ranks in the MPI communicator used for the MPI_Alltoall call and the Alltoall algorithm being selected. Defaults: If using one of the non-default send/recv algorithms on Aries, the defaults are: if communicator size <= 512, 2048 bytes if communicator size > 512 and <= 1024, 1024 bytes if communicator size > 1024 and <= 65536, 128 bytes if communicator size > 65536 and <= 131072, 64 bytes if communicator size > 131072 , 32 bytes
20220317 014102.898 INFO             PET3 index=  10                                MPIR_CVAR_ALLTOALLV_THROTTLE : Sets the per-process maximum number of outstanding Isends and Irecvs that can be posted concurrently for the optimized send/recv MPI_Alltoallv algorithm. For large messages, consider decreasing the throttle to 1 or 2 to improve performance. Defaults: 8
20220317 014102.898 INFO             PET3 index=  11                                   MPIR_CVAR_BCAST_ONLY_TREE : If set to 1, MPI_Bcast uses an smp-aware tree algorithm regardless of data size. The tree algorithm generally scales well to high processor counts on Cray XE systems. If set to 0, MPI_Bcast uses a variety of algorithms (tree, scatter, or ring) depending on message size and other factors. These other algorithms generally do not scale well when using more than 512 processors on Cray XE systems. Default: 1
20220317 014102.898 INFO             PET3 index=  12                             MPIR_CVAR_BCAST_INTERNODE_RADIX : Used to set the radix of the inter-node tree. This can be set to any integer value greater than or equal to 2. Default: 4
20220317 014102.898 INFO             PET3 index=  13                             MPIR_CVAR_BCAST_INTRANODE_RADIX : Used to set the radix of the intra-node tree. This can be set to any integer value greater than or equal to 2. Default: 4
20220317 014102.898 INFO             PET3 index=  14                                      MPIR_CVAR_COLL_OPT_OFF : If set, disables collective optimizations which use nondefault, architecture-specific algorithms for some MPI collective operations. By default, all collective optimized algorithms are enabled. To disable all collective optimized algorithms, set MPICH_COLL_OPT_OFF to 1. To disable optimized algorithms for selected MPI collectives, set the value to a comma-separated list of the desired collective names. Names are not case-sensitive. Any unrecognizable name is flagged with a warning message and ignored. For example, to disable the MPI_Allgather optimized collective algorithm, set MPICH_COLL_OPT_OFF=mpi_allgather. The following collective names are recognized: MPI_Allgather, MPI_Allgatherv, MPI_Allreduce, MPI_Alltoall, MPI_Alltoallv, MPI_Bcast, MPI_Gatherv, MPI_Scatterv, MPI_Igatherv, and MPI_Iallreduce. Default: Not enabled.
20220317 014102.898 INFO             PET3 index=  15                                         MPIR_CVAR_COLL_SYNC : If set, a Barrier is performed at the beginning of each specified MPI collective function. This forces all processes participating in that collective to sync up before the collective can begin. To disable this feature for all MPI collectives, set the value to 0. This is the default. To enable this feature for all MPI collectives, set the value to 1. To enable this feature for selected MPI collectives, set the value to a comma-separated list of the desired collective names. Names are not case-sensitive. Any unrecognizable name is flagged with a warning message and ignored. The following collective names are recognized: MPI_Allgather, MPI_Allgatherv, MPI_Allreduce, MPI_Alltoall, MPI_Alltoallv, MPI_Alltoallw, MPI_Bcast, MPI_Exscan, MPI_Gather, MPI_Gatherv, MPI_Reduce, MPI_Reduce_scatter, MPI_Scan, MPI_Scatter, and MPI_Scatterv. Default: Not enabled.
20220317 014102.898 INFO             PET3 index=  16                                 MPIR_CVAR_GATHERV_SHORT_MSG : Adjusts the cutoff point at and below which the optimized tree MPI_Gatherv algorithm is used instead of the optimized permission-to-send MPI_Gatherv algorithm. The cutoff is based on the average size of the variable MPI_Gatherv message sizes.  Default: 131072 bytes
20220317 014102.898 INFO             PET3 index=  17                                     MPIR_CVAR_REDUCE_NO_SMP : If set, MPI_Reduce uses an algorithm that is not smp-aware. This provides a consistent ordering of the specified reduce operation regardless of system configuration. Note:  This algorithm may not perform as well as the default smp-aware algorithms as it does not take advantage of rank topology. Default: not set
20220317 014102.898 INFO             PET3 index=  18                              MPIR_CVAR_SCATTERV_SYNCHRONOUS : The default, non-optimized ANL MPI_Scatterv algorithm uses asynchronous sends by default for communicator sizes less than 200,000 ranks. If set, this environment variable causes MPI_Scatterv to switch to using blocking sends, which may be beneficial in certain cases involving large data sizes or high process counts. For communicator sizes equal to or greater than 200,000 ranks, the blocking send algorithm is used by default. Default: not enabled
20220317 014102.898 INFO             PET3 index=  19                               MPIR_CVAR_SHARED_MEM_COLL_OPT : If set, the MPICH library will use the optimized shared- memory based design for collective operations. On Gemini and Aries systems, the supported collective operations are: MPI_Allreduce, MPI_Iallreduce, MPI_Barrier, and MPI_Bcast. To enable all available shared-memory optimizations, set MPICH_SHARED_MEM_COLL_OPT to 1. To enable this feature for a specific set of collective operations, set MPICH_SHARED_MEM_COLL_OPT to a comma- separated list of collective names. For example, to enable this optimization for MPI_Bcast only, set MPICH_SHARED_MEM_COLL_OPT=MPI_Bcast. To enable this optimization for MPI_Allreduce only, set MPICH_SHARED_MEM_COLL_OPT=MPI_Allreduce. Unsupported names are flagged with a warning message and ignored. Default: set
20220317 014102.898 INFO             PET3 index=  20                   MPIR_CVAR_ALLREDUCE_GPU_STAGING_THRESHOLD : If the sendbuf/recvbuf of an Allreduce operation is on GPU-resident memory regions, for payload sizes larger than this variable,  the Cray optimized staging implementation will be used.  By default, the staging implementation is used for all payload sizes.  This variable has no effect if MPICH_GPU_COLL_STAGING_AREA_OPT is set to 0. Default: 0
20220317 014102.898 INFO             PET3 index=  21                         MPIR_CVAR_GPU_COLL_STAGING_BUF_SIZE : This variable determines the size of the staging buffers used for some collectives (such as MPI_Allreduce) when sendbuf/recvbuf on GPU-resident buffers. This variable has no effect if  MPICH_GPU_COLL_STAGING_AREA_OPT is set to 0.   Default: 262144
20220317 014102.898 INFO             PET3 index=  22                                 MPIR_CVAR_GATHERV_SYNC_FREQ : Only applicable to the Gatherv permission-to-send algorithm.  Adjusts  the number of outstanding receives the root for Gatherv will allow.   Default is 16.
20220317 014102.898 INFO             PET3 index=  23                              MPIR_CVAR_GATHERV_MAX_TMP_SIZE : Only applicable to the Gatherv tree algorithm.  Sets the maximum amount of temporary memory GAtherv will allow a rank to allocate when using the tree-based algorithm.  Each rank allocates a different amount, with many allocating no extra memory.  If any rank requires more than this amount of temporary buffer space, a different algorithm is used. Default is 512MB.
20220317 014102.898 INFO             PET3 index=  24                             MPIR_CVAR_GATHERV_MIN_COMM_SIZE : Cray MPI offers two optimized Gatherv algorithms.  A tree algorithm for small messages and a permission-to-send send algorithm for larger messages.  Set  this value to the minimum communicator size to attempt use of either of the  Cray optimized Gatherv algorithms. Default is 64
20220317 014102.898 INFO             PET3 index=  25                                MPIR_CVAR_SCATTERV_SHORT_MSG : Adjusts the cutoff point at and below which the optimized tree MPI_Scatterv algorithm is used instead of the optimized staggered send algorithm.  The cutoff is in bytes, based on the average size of the variable MPI_Scatterv message sizes. Default behavior if unset is: For communicator sizes of <= 512 ranks, 2048 bytes For communicator sizes of > 512 ranks, 8192 bytes
20220317 014102.898 INFO             PET3 index=  26                                MPIR_CVAR_SCATTERV_SYNC_FREQ : Only applicable to the Scatterv staggered send algorithm.  Adjusts the number of outstanding sends the root for Scatterv will use. Default is 16.
20220317 014102.898 INFO             PET3 index=  27                             MPIR_CVAR_SCATTERV_MAX_TMP_SIZE : Only applicable to the Scatterv tree algorithm.  Sets the maximum amount of temporary memory Scatterv will allow a rank to allocate when using the tree-based algorithm.  Each rank allocates a different amount, with many allocating no extra memory.  If any rank requires more than this amount of temporary buffer space, a different algorithm is used. Default is 512MB.
20220317 014102.898 INFO             PET3 index=  28                            MPIR_CVAR_SCATTERV_MIN_COMM_SIZE : Cray MPI offers two optimized Scatterv algorithms.  A tree algorithm for small messages and a staggered send algorithm for larger messages.  Set this value to the minimum communicator size to attempt use of either of the Cray optimized Scatterv algorithms. Default is 64
20220317 014102.898 INFO             PET3 index=  29                           MPIR_CVAR_MPIIO_ABORT_ON_RW_ERROR : If set to enable, causes MPI-IO to abort immediately after issuing an error message if an I/O error occurs during a system read() or write() call. This applies only to I/O errors for system read() and write() calls made as a result of MPI I/O calls. It does not apply to I/O errors for other MPI I/O calls such as MPI_File_open(), nor does it apply to read() and write() calls made by means other than MPI I/O calls. Abort on error is not standard behavior. The MPI Standard specifies that the default error handling for MPI I/O calls is to return an error code to the application rather than aborting the application, but since errors on write or read are almost always unexpected and usually not recoverable, it may be preferable to abort as soon as the error is detected. Doing so does not allow any recovery, but does provide the most information about the error and terminates the job quickly. If the Cray Abnormal Termination Processing (ATP) feature is enabled, the abort will result in a full stack backtrace writte
20220317 014102.898 INFO             PET3 index=  30                MPIR_CVAR_MPIIO_AGGREGATOR_PLACEMENT_DISPLAY : This variable controls whether the placement of the aggregators will be displayed when a file is opened. The placement can be  controlled on a per file basis with the aggregator_placement_stride  hint. If set, displays the assignment of MPIIO collective buffering aggregators for reads/writes of a shared file, showing rank and node ID (nid). For example: Aggregator Placement for /lus/scratch/myfile RankReorderMethod=3  AggPlacementStride=-1 AGG    Rank       nid ----  ------  -------- 0       0  nid00578 1       4  nid00579 2       1  nid00606 3       5  nid00607 4       2  nid00578 5       6  nid00579 6       3  nid00606 7       7  nid00607 Default: not set
20220317 014102.898 INFO             PET3 index=  31                 MPIR_CVAR_MPIIO_AGGREGATOR_PLACEMENT_STRIDE : Partially controls to which nodes MPIIO collective buffering aggregators are assigned. See the notes below on the order of nodes. Network traffic and resulting I/O performance may be affected by the assignments. If set to 1, consecutive nodes are used. The number of aggregators assigned per node is controlled by the cb_config_list hint. By default, no more than one aggregator per node will be assigned if there are at least as many nodes as aggregators. If set to a value greater than 1, node selection is strided across the available nodes by this value. If the stride times the number of aggregators exceeds the number of nodes, the assignments will wrap around, which is usually not optimal for performance. If set to -1, node selection is strided across available nodes by the value of the number of nodes divided by the number of aggregators (integer division, minimum value of 1). The purpose is to spread out the nodes to reduce network congestion. Note:  The order of nodes can be shown by setting the MPICH_RANK
20220317 014102.898 INFO             PET3 index=  32                                    MPIR_CVAR_MPIIO_CB_ALIGN : Sets the default value for the cb_align hint. Files opened with MPI_File_open wil have this value for the cb_align hint unless the hint is set on a per file basis with either the MPICH_MPIIO_HINTS environment variable or from within a program with the MPI_Info_set() call. Note:  Only MPICH_MPIIO_CB_ALIGN == 2 is fully supported. Other values are for internal testing only. Default: 2
20220317 014102.898 INFO             PET3 index=  33                                MPIR_CVAR_MPIIO_DVS_MAXNODES : Note:  This environment variable in relevant only for file systems accessed from Cray system compute nodes via DVS server nodes; e.g. GPFS or PANFS. As described in the dvs(5) man page, the environment variable DVS_MAXNODES can be used to set the stripe width— that is, the number of DVS server nodes—used to access a file in "stripe parallel mode." For most files, and especially for small files, setting DVS_MAXNODES to 1 ("cluster parallel mode") is preferred. The MPICH_MPIIO_DVS_MAXNODES environment variable enables you to leave DVS_MAXNODES set to 1 and then use MPICH_MPIIO_DVS_MAXNODES to temporarily override DVS_MAXNODES when it is advantageous to specify wider striping for files being opened by the MPI_File_open() call. The range of values accepted by MPICH_MPIIO_DVS_MAXNODES goes from 1 to the number of server nodes specified on the mount with the nnodes mount option. DVS_MAXNODES is not set by default. Therefore, for MPICH_MPIIO_DVS_MAXNODES to have any effect, DVS_MAXNODES must be defined before p
20220317 014102.898 INFO             PET3 index=  34                                       MPIR_CVAR_MPIIO_HINTS : If set, override the default value of one or more MPI I/O hints. This also overrides any values that were set by using calls to MPI_Info_set in the application code. The new values apply to the file the next time it is opened using an MPI_File_open() call. After the MPI_File_open() call, subsequent MPI_Info_set calls can be used to pass new MPI I/O hints that take precedence over some of the environment variable values. Other MPI I/O hints such as striping_factor, striping_unit, cb_nodes, and cb_config_list cannot be changed after the MPI_File_open() call, as these are evaluated and applied only during the file open process. An MPI_File_close call followed by an MPI_File_open call can be used to restart the MPI I/O hint evaluation process. The syntax for this environment variable is a comma- separated list of specifications. Each individual specification is a pathname_pattern followed by a colon- separated list of one or more key=value pairs. In each key=value pair, the key is the MPI-IO hint name, and the v
20220317 014102.898 INFO             PET3 index=  35                               MPIR_CVAR_MPIIO_HINTS_DISPLAY : If set, causes rank 0 in the participating communicator to display the names and values of all MPI-IO hints that are set for the file being opened with the MPI_File_open call. It also displays relevant environment variables whether or not MPICH_ENV_DISPLAY is set. Default: not enabled.
20220317 014102.898 INFO             PET3 index=  36                               MPIR_CVAR_MPIIO_MAX_NUM_IRECV : When MPIIO collective buffering is used, this environment variable controls the number of outstanding MPI_Irecv calls allowed before an MPI_Waitall is done. Default: 50
20220317 014102.898 INFO             PET3 index=  37                               MPIR_CVAR_MPIIO_MAX_NUM_ISEND : When MPIIO collective buffering is used, this environment variable controls the number of outstanding MPI_Isend calls allowed before an MPI_Waitall is done. Default: 50
20220317 014102.898 INFO             PET3 index=  38                              MPIR_CVAR_MPIIO_MAX_SIZE_ISEND : When MPIIO collective buffering is used, this environment variable limits MPI_Isend by the amount of data being sent rather than by the number of calls. Default: 10485760 bytes
20220317 014102.898 INFO             PET3 index=  39                                       MPIR_CVAR_MPIIO_STATS : If set to 1, a summary of file write and read access patterns is written by rank 0 to stderr. This information provides some insight into how I/O performance may be improved. The information is provided on a per-file basis and is written when the file is closed. It does not provide any timing information. If set to 2, a set of data files are written to the working directory, one file for each rank, with the filename prefix specified by the MPICH_MPIIO_STATS_FILE environment variable. The data is in comma-separated values (CSV) format, which can be summarized with the cray_mpiio_summary script in the /opt/cray/mpt/version/gni/bin directory. Additional example scripts are provided in that directory to further process and display the data. Only enabled if MPIR_CVAR_MPIIO_CB_ALIGN=2. Default: not set
20220317 014102.898 INFO             PET3 index=  40                                  MPIR_CVAR_MPIIO_STATS_FILE : Specifies the filename prefix for the set of data files written when MPICH_MPIIO_STATS is set to 2. The filename prefix may be a full absolute pathname or a relative pathname. Summary plots of these files can be generated using the cray_mpiio_summary script from the /opt/cray/mpt/version/gni/bin directory. Other example scripts for post-processing this data can also be found in /opt/cray/mpt/version/gni/bin. Default: _cray_mpiio_stats_
20220317 014102.898 INFO             PET3 index=  41                         MPIR_CVAR_MPIIO_STATS_INTERVAL_MSEC : Specifies the time interval in milliseconds for each MPICH_MPIIO_STATS data point. Default: 250
20220317 014102.898 INFO             PET3 index=  42                                      MPIR_CVAR_MPIIO_TIMERS : If set to 0, or not set at all, no timing data is collected. If set to 1, timing data for different phases in MPI-IO is collected locally by each MPI process and then during MPI_File_close the data is consolidated and printed. Some timing data is displayed in seconds, other data is displayed in clock ticks, possibly scaled down. Also see MPICH_MPIIO_TIMERS_SCALE The relative values of the reported times are more important to the analysis than the absolute time. More detailed information about MPI-IO performance can be obtained by using the MPICH_MPIIO_STATS feature and by using the CrayPat and Apprentice2 Timeline Report of I/O bandwidth. Only enabled if MPIR_CVAR_MPIIO_CB_ALIGN=2. Default: 0
20220317 014102.898 INFO             PET3 index=  43                                MPIR_CVAR_MPIIO_TIMERS_SCALE : Specifies the power of 2 to use to scale the times reported by MPICH_MPIIO_TIMERS.  The raw times are collected in clock ticks. This generally is a very large number and reducing all the times by the same scaling factor makes for a more compact display. If set to 0, or not set at all, MPI-IO automatically determines a scaling factor to limit the report times to 9 or fewer digits. This auto-determined value is displayed.  To make run to run comparisons, you can set the scaling factor to your preferred value. Default: 0
20220317 014102.898 INFO             PET3 index=  44                                  MPIR_CVAR_MPIIO_TIME_WAITS : If set to non-zero, time how long this rank has to wait for other ranks to catch up.  This separates true metadata time from imbalance time. This is disabled when MPICH_MPIIO_TIMERS is not set.  Otherwise it defaults to 1. Default: 1
20220317 014102.898 INFO             PET3 index=  45                          MPIR_CVAR_MPIIO_WRITE_EXIT_BARRIER : If set to non-zero, collective write's will barrier on exit Default: 1
20220317 014102.898 INFO             PET3 index=  46                               MPIR_CVAR_MPIIO_DS_WRITE_CRAY : If set to non-zero, collective write's with data sieving will be optimized  Default: 1
20220317 014102.898 INFO             PET3 index=  47                         MPIR_CVAR_MPIIO_OFI_STARTUP_CONNECT : If set to enable, causes MPI-IO to establish connections between ranks and aggregators during the openning of the file to be used for MPIIO collective operations
20220317 014102.899 INFO             PET3 index=  48                MPIR_CVAR_MPIIO_OFI_STARTUP_NODES_AGGREGATOR : If MPIIO_OFI_STARTUP_CONNECT is enabled, this specifies the number of nodes that will concurrently attempt to connext to each aggrgator. Each rank on each node will establish its own connection to the aggregator. Default: 2
20220317 014102.899 INFO             PET3 index=  49                                           MPIR_CVAR_DPM_DIR : Sets the directory to use for MPI port name publishing in the  file-based nameserv implementation, as well as publishing the  credential obtained from libdrc.
20220317 014102.899 INFO             PET3 index=  50                               MPIR_CVAR_SINGLE_HOST_ENABLED : If true and if running as a single node, then inter-node communications is never initialized. The helps prevent the use of scarce hardware resources.
20220317 014102.899 INFO             PET3 index=  51                                       MPIR_CVAR_OFI_VERBOSE : If set to 1, enable verbose output about the CH4 OFI device. If set to 2 or higher, enable more verbose output.
20220317 014102.899 INFO             PET3 index=  52                                 MPIR_CVAR_OFI_XRCD_BASE_DIR : Specifies the base directory for the XRCD file.  This is only  used if the job is not launched via PALS launcher.  If launched  via PALS, we use the private, temporary PALS_SPOOL_DIR directory. Default: /tmp
20220317 014102.899 INFO             PET3 index=  53                                   MPIR_CVAR_OFI_NIC_VERBOSE : If set to 1 or higher, enables verbose output about the CH4 OFI NIC selection.   Non-fatal warnings are also displayed.  If set to 2 or higher, each rank in  the job will display it's NIC selection.  If set to 3 or higher, more verbose  output is enabled (for debug purposes).
20220317 014102.899 INFO             PET3 index=  54                                    MPIR_CVAR_OFI_NIC_POLICY : Specifies the rank-to-nic policy.  The supported options are:  NUMA | GPU | BLOCK | ROUND-ROBIN | USER
20220317 014102.899 INFO             PET3 index=  55                                   MPIR_CVAR_OFI_NIC_MAPPING : Specifies the precise rank-to-NIC mapping to use on each node.  This is  required if the NIC_POLICY is set to USER.  Each local rank must have a NIC mapping assigned by this env variable. If there are fewer MPI ranks on any  node, that part of the MAPPING string will be ignored.  For example: To assign local ranks 0,16,32,48 to NIC 0, and remaining ranks to NIC 1 MPICH_OFI_NIC_MAPPING="0:0,16,32,48; 1:1-15,17-31,33-47,49-63"
20220317 014102.899 INFO             PET3 index=  56                                      MPIR_CVAR_OFI_NUM_NICS : If set, specifies the number of NICs the job can use on each node. By default, when multiple NICs per node are available, MPI attempts to use them all.  If using fewer NICs is desired, this env variable  can be set to indicate the maximum number of NICs per node MPI will  use. By default, consecutive NICs are used, starting with index 0.   To limit use to 1 NIC/node, use : export MPICH_OFI_NUM_NICS=1 To limit use to 2 NICs/node, use: export MPICH_OFI_NUM_NICS=2 To specify alternative NIC index values, you may indicate the desired index values by adding a colon, followed by the NIC indexes.  Use  quotes so the shell doesn't interfere.  For example: To limit use to 1 NIC/node, index 1    : export MPICH_OFI_NUM_NICS="1:1" To limit use to 2 NICs/node, index 1&3 : export MPICH_OFI_NUM_NICS="2:1,3"
20220317 014102.899 INFO             PET3 index=  57                        MPIR_CVAR_OFI_SKIP_NIC_SYMMETRY_TEST : If set to 1, CrayMPICH will bypass it's check for NIC symmetry on all nodes in the job. By default, this check is run during MPI_Init to  make sure all the nodes in the job have the same number of NICs available.
20220317 014102.899 INFO             PET3 index=  58                               MPIR_CVAR_OFI_STARTUP_CONNECT : If set to 1, CrayMPICH will create COMM_WORLD static OFI connections to every other rank during MPI_Init.
20220317 014102.899 INFO             PET3 index=  59                           MPIR_CVAR_OFI_RMA_STARTUP_CONNECT : If set to 1, CrayMPICH RMA will create static OFI connections to every other rank on the same node during MPI_Init. This will also enable MPIR_CVAR_OFI_STARTUP_CONNECT.
20220317 014102.899 INFO             PET3 index=  60                                MPIR_CVAR_OFI_DEFAULT_TCLASS : If set, CrayMPICH will assign the requested default TC (traffic class) to the job domain.  By default, all endpoints inherit this TC.
20220317 014102.899 INFO             PET3 index=  61                                 MPIR_CVAR_OFI_TCLASS_ERRORS : Determines how CrayMPICH responds to traffic class errors.  Valid values are "warn", "silent" or "error".
20220317 014102.899 INFO             PET3 index=  62                                  MPIR_CVAR_OFI_CXI_PID_BASE : Set to the base value that MPI will use to assign portal IDs for the CXI provider.  Max PID value is 510.  MPI uses PID_BASE+lrank as the unique value per rank per node.  This is only applicable to SS11.
20220317 014102.899 INFO             PET3 index=  63                          MPIR_CVAR_OFI_USE_SCALABLE_STARTUP : Set to false (0) to bypass the scalable startup feature for CXI. This is only applicable to SS11.
20220317 014102.899 INFO             PET3 index=  64                                       MPIR_CVAR_UCX_VERBOSE : If set to 1, enable verbose output about the CH4 UCX device. If set to 2 or higher, enable more verbose output.
20220317 014102.899 INFO             PET3 index=  65                                  MPIR_CVAR_UCX_RC_MAX_RANKS : By default, CrayMPI selects either the UCX RC or UD protocols for inter-node messaging.  If a job is launched with MPICH_UCX_RC_MAX_RANKS  ranks or fewer, the RC protocol is selected.  If more ranks are launched,  the UCX UD protocol is chosen.  The RC protocol does not scale well due to the resource requirements.  Note this default can be overridden by setting the UCX_TLS environment variable.
20220317 014102.899 INFO             PET3 index=  66                              MPIR_CVAR_SMP_SINGLE_COPY_MODE : If non-null, choose an on-node copy mode for large messages. This variable can be set to XPMEM, CMA, or NONE. By default, Cray MPICH will look to use XPMEM. If XPMEM is requested, but not available, Cray MPICH will attempt to use CMA. (NOTE: Cray MPICH does not support CMA currently. The env. variables and CVARs are set up to allow CMA usage with future versions of Cray MPICH) If both XPMEM and CMA cannot be used, Cray MPICH will fall back to using the ANL shared memory implementation (2-copy) Default: NULL
20220317 014102.899 INFO             PET3 index=  67                              MPIR_CVAR_SMP_SINGLE_COPY_SIZE : Specifies the minimum message size in bytes to consider for single-copy transfers for on-node messages. This applies only to the SMP (on-node shared memory) device. The value is interpreted as bytes, unless the string ends in a K, which indicates kilobytes, or M, which indicates megabytes. Default: 8192
20220317 014102.899 INFO             PET3 index=  68                       MPIR_CVAR_SHM_PROGRESS_MAX_BATCH_SIZE : Adjusts the maximum number of on-node requests that can be processed in a single batch. Higher values for the maximum batch size can lower the overhead due to entering the progress engine, but can also  delay the processing of off-node message requests. Default is 8.
20220317 014102.899 INFO             PET3 index=  69                                MPIR_CVAR_CH4_RMA_THREAD_HOT : If true, the big lock is disabled for certain RMA operations
20220317 014102.899 INFO             PET3 index=  70                                    MPIR_CVAR_ABORT_ON_ERROR : If set, causes MPICH to abort and produce a core dump when MPICH detects an internal error. Note that the core dump size limit (usually 0 bytes by default) must be reset to an appropriate value in order to enable coredumps. Default: Not enabled.
20220317 014102.899 INFO             PET3 index=  71                                   MPIR_CVAR_CPUMASK_DISPLAY : If set, causes each MPI rank in the job to display its CPU affinity bitmask. Note that this reports only the CPU affinity masks for the MPI ranks; if you have a hybrid program, it does not provide any thread information. The bitmask is read from right to left, meaning the value in the rightmost position corresponds to CPU 0 on the node.
20220317 014102.899 INFO             PET3 index=  72                                       MPIR_CVAR_ENV_DISPLAY : If set, causes rank 0 to display all MPICH environment variables and their current settings at MPI initialization time. If two or more nodes are used, MPICH/GNI environment settings are also included in the listing. Default: Not enabled.
20220317 014102.899 INFO             PET3 index=  73                                  MPIR_CVAR_OPTIMIZED_MEMCPY : Specifies which version of memcpy to use. Valid values are: 0         Use the system (glibc) version of memcpy. 1         Use an optimized version of memcpy if one is available for the processor being used. In this release, an optimized version of memcpy() is available only for Intel processors. 2         Use a highly optimized version of memcpy that provides better performance in some areas but may have performance regressions in other areas, if one is available for the processor being used. In this release, a highly optimized version of memcpy() is available only for Intel Haswell processors. MPICH_OPTIMIZED_MEMCPY is overridden by MPICH_USE_SYSTEM_MEMCPY. If MPICH_USE_SYSTEM_MEMCPY is set, MPICH_OPTIMIZED_MEMCPY is ignored and the system (glibc) version of memcpy() is used. Default: 1
20220317 014102.899 INFO             PET3 index=  74                                     MPIR_CVAR_STATS_DISPLAY : If set to 1, a summary of MPI statistics, also available through the MPI Tools Interface, will be written by rank 0 to stderr. If set to 2, all ranks will produce an individualized statistics summary and write to file on a per-rank basis. The MPICH_STATS_FILE determines the prefix of the file to be used. This information may provide insight into how MPI performance may be improved. Default: 0
20220317 014102.899 INFO             PET3 index=  75                                   MPIR_CVAR_STATS_VERBOSITY : Specifies the verbosity of the MPI statistics summary. This information may provide insight into how MPI performance may be improved. Increase the value for more detailed summary. Default: 1 1        USER_BASIC  (default) 2        USER_DETAIL 3        USER_ALL 4        TUNER_BASIC 5        TUNER_DETAIL 6        TUNER_ALL 7        MPIDEV_BASIC 8        MPIDEV_DETAIL 9        MPIDEV_ALL
20220317 014102.899 INFO             PET3 index=  76                                        MPIR_CVAR_STATS_FILE : Specifies the filename prefix for the set of data files written when MPICH_STATS_DISPLAY is set to 2. The filename prefix may be a full absolute pathname or a relative pathname. Default: _cray_stats_
20220317 014102.899 INFO             PET3 index=  77                              MPIR_CVAR_RANK_REORDER_DISPLAY : If set, causes rank 0 to display which node each MPI rank resides in. The rank order can be manipulated via the MPICH_RANK_REORDER_METHOD environment variable or MPIR_CVAR_RANK_REORDER_METHOD control variable. Default: Not set
20220317 014102.899 INFO             PET3 index=  78                               MPIR_CVAR_RANK_REORDER_METHOD : Overrides the default MPI rank placement scheme. If this variable is not set, the default aprun launcher placement policy is used. The default policy for aprun is SMP-style placement. To display the MPI rank placement information, set MPICH_RANK_REORDER_DISPLAY. See manpage for more details. Default: 1, for SMP-style placement.
20220317 014102.899 INFO             PET3 index=  79                                 MPIR_CVAR_USE_SYSTEM_MEMCPY : Note:  This environment variable is deprecated and scheduled to be removed in a future release. Use MPICH_OPTIMIZED_MEMCPY instead. If set, use the system (glibc) version of memcpy(); otherwise, an optimized version of memcpy() may be used. Currently, an optimized version of memcpy() is available only for Intel processors. Default: Not set
20220317 014102.899 INFO             PET3 index=  80                                   MPIR_CVAR_VERSION_DISPLAY : If set, causes MPICH to display the CRAY MPICH version number as well as build date information. Default: Not enabled
20220317 014102.899 INFO             PET3 index=  81                          MPIR_CVAR_USE_GPU_STREAM_TRIGGERED : If set, causes MPICH to allow using stream triggered GPU communication operations. Default: Not enabled
20220317 014102.899 INFO             PET3 index=  82                               MPIR_CVAR_NUM_MAX_GPU_STREAMS : If set, causes MPICH to allow using the set maximum number of streams concurrently in the stream triggered GPU communication operations. Default: 26
20220317 014102.899 INFO             PET3 index=  83                                  MPIR_CVAR_MEMCPY_MEM_CHECK : If set, enables a check of the memcpy() source and destination areas. If they overlap, the application asserts with an error message listing the file, line, and memory range overlap. If this error is found, correct it either by changing the memory ranges or possibly by using MPI_IN_PLACE. Default: not set (off)
20220317 014102.899 INFO             PET3 index=  84                                     MPIR_CVAR_MSG_QUEUE_DBG : If set, turns on TotalView Message Queue Debugging support so that message queues are tracked in the TotalView debugger and a message queue graph can be generated. Enabling this feature degrades performance. Default: not enabled.
20220317 014102.899 INFO             PET3 index=  85                             MPIR_CVAR_NO_BUFFER_ALIAS_CHECK : If set, the buffer alias error check for collectives is disabled. The MPI standard does not allow aliasing of type OUT or INOUT parameters on the same collective function call. The use of MPI_IN_PLACE is required in these scenarios. A new check was added in MPT 5.2 to detect this condition and report the error. To bypass this check, set MPICH_NO_BUFFER_ALIAS_CHECK to any value. Default: not set
20220317 014102.899 INFO             PET3 index=  86                                MPIR_CVAR_ALLOC_MEM_AFFINITY : Controls the affinity of the memory region allocated by the MPI_Alloc_mem() or MPI_Win_allocate() operations. On systems that do not offer High Bandwidth Memory capabilities, (such as Intel Haswell or Broadwell processors), this env. variable is ignored and memory affinity is set to DDR. On Phi processors, such as KNL (and KNH, KNP in the future), this env. variable allows users to specifically request the memory returned by MPI_Alloc_mem() and MPI_Win_allocate() to be bound to either DDR, or the MCDRAM. Users can request a specific page size or memory binding policy via the MPICH_ALLOC_MEM_POLICY and MPICH_ALLOC_MEM_PG_SZ env. variables. Default: SYS_DEFAULT
20220317 014102.899 INFO             PET3 index=  87                             MPIR_CVAR_INTERNAL_MEM_AFFINITY : Controls the affinity of internal memory regions allocated by the MPI library. This variable currently affects the memory affinity of the mail-boxes used for off-node communication, and the shared-memory regions that are used for on-node pt2pt and collective ops. On systems that do not offer High Bandwidth Memory capabilities, (such as Intel Haswell or Broadwell processors), this env. variable is ignored and memory affinity is set to DDR. On Phi processors, such as KNL (and KNH, KNP in the future), this env. variable allows users to specifically request the internal memory regions used by the MPI library to be bound to either DDR, or the MCDRAM. The default affinity settings will be governed by the system defaults. For example, on a KNL system configured in the Quad/Flat mode, if the job is run with numactl --membind=1, all of MPI's internal memory will be bound to MCDRAM if this variable is not set. Default: SYS_DEFAULT
20220317 014102.899 INFO             PET3 index=  88                                  MPIR_CVAR_ALLOC_MEM_POLICY : Controls the memory affinity policy on systems with specialized memory hardware. By default, the memory policy is set to "{P}referred". Other accepted values are "{M}andatory" and "{I}"nterleave. Default: Preferred
20220317 014102.899 INFO             PET3 index=  89                                   MPIR_CVAR_ALLOC_MEM_PG_SZ : Controls the page size for the MPI_Alloc_mem() and MPI_Win_allocate() operations. This parameters defaults to 4KB base pages. The supported values are 2M, 4M, 8M, 16M, 32M, 64M, 128M, 256M, 512M, 1G and 2G. Default: 4096
20220317 014102.899 INFO             PET3 index=  90                                   MPIR_CVAR_OPT_THREAD_SYNC : Controls the mechanism used to implement thread-synchronization inside the Cray MPICH library. If set to 1, an optimized synchronization implementation is used. If set to 0, Cray MPICH falls back to using a pthread_mutex-based thread-synchronization implementation. Default: 1
20220317 014102.899 INFO             PET3 index=  91                                 MPIR_CVAR_THREAD_YIELD_FREQ : Determines how often a thread yields while waiting to acquire a lock in the new Cray optimized locking impl. This variable has no effect if MPICH_CRAY_OPT_THREAD_SYNC is 0. Default: 10000
20220317 014102.899 INFO             PET3 index=  92                                   MPIR_CVAR_MEM_DEBUG_FNAME : If set, the MPI library creates new files with the user specified name and writes various important memory statistics into these files. This information can be useful for post-processing. Users can set MPICH_MEM_DEBUG_FNAME to any suitable string. The resulting files are named as "string".<pid>.<MPI-rank>. For example, if MPICH_MEM_DEBUG_FNAME is set to "MEM_DBG_MSGS" the debug file for rank0 will be named "MEM_DBG_MSGS.<pid>.0 and written to the user's current working directory. If this flag is not set, the MPI library will redirect all the debug messages to stderr. Default: unset (disabled)
20220317 014102.899 INFO             PET3 index=  93                                   MPIR_CVAR_MALLOC_FALLBACK : Set the policy for fallback behavior when attempting to allocate large pages for internal buffers and insufficient large pages are available to satisfy the request. If set to enabled, MPICH falls back to using malloc in such cases. Default: not enabled (process fails and job terminates if insufficient large pages are available to satisfy the request)
20220317 014102.899 INFO             PET3 index=  94                              MPIR_CVAR_NO_DIRECT_GPU_ACCESS : If true, IPC and NIC-GPU Direct Access capabilities are not used for GPU-to-GPU transfers. This is mainly used for debugging.  Default: 0
20220317 014102.899 INFO             PET3 index=  95                                      MPIR_CVAR_G2G_PIPELINE : If nonzero, the device-host and network transfers will be overlapped to pipeline GPU-to-GPU transfers. Setting MPICH_G2G_PIPELINE to N will allow N GPU-to-GPU messages to be efficiently in-flight at any one time. If MPICH_G2G_PIPELINE is nonzero but MPICH_GPU_SUPPORT_ENABLED is disabled, MPICH_G2G_PIPELINE will be turned off. If MPICH_GPU_SUPPORT_ENABLED is enabled but MPICH_G2G_PIPELINE is 0, the default value is set to 8.  The pipeline-based implementation is being carried over from  CH3/Aries implementation. We think the pipelined-based implementation will not be necessary on SS-11 systems.  This logic may get dropped in the future.  Default: not set
20220317 014102.899 INFO             PET3 index=  96                               MPIR_CVAR_GPU_SUPPORT_ENABLED : If set, allows the MPI application to pass GPU pointers directly to MPI communication functions.  Default: not set
20220317 014102.899 INFO             PET3 index=  97                MPIR_CVAR_GPU_MANAGED_MEMORY_SUPPORT_ENABLED : If set, allows the MPI application to pass pointers allocated via  GPU managed memory allocators to MPI communication functions. Default: not set
20220317 014102.899 INFO             PET3 index=  98                         MPIR_CVAR_GPU_COLL_STAGING_AREA_OPT : If set, allows Cray MPI to use pre-registered CPU-attached memory to optimize collective ops.  Default: set
20220317 014102.899 INFO             PET3 index=  99               MPIR_CVAR_GPU_COLL_REGISTER_SHARED_MEM_REGION : If set, allows Cray MPI to registere CPU-attached shared memory regions  used to optimize collective ops.  Default: set
20220317 014102.899 INFO             PET3 index= 100                                   MPIR_CVAR_GPU_IPC_ENABLED : If set to 1, Cray MPICH will use GPU IPC to move data between GPU devices that are within the same node. If set to 0, Cray MPICH will use staging buffers on the host to implement data transfer between GPU devices that are within the same node. If MPIR_CVAR_GPU_SUPPORT_ENABLED is set, Cray MPICH  automatically attempts to enable the IPC optimizations.   If MPIR_CVAR_NO_DIRECT_GPU_ACCESS is set, IPC optimizations are disabled.  Default: -1
20220317 014102.899 INFO             PET3 index= 101                                 MPIR_CVAR_GPU_IPC_THRESHOLD : Specifies the minimum message size in bytes to consider for single-copy transfers between GPU devices that are within the same compute node. The value is interpreted as bytes, unless the string ends in a K, which indicates kilobytes, or M, which indicates megabytes.
20220317 014102.899 INFO             PET3 index= 102                                  MPIR_CVAR_GPU_IPC_PROTOCOL : Specifies the protocol used for large msg on-node, inter-GPU  data transfers via GPU IPC.  Valid options are RPUT or RGET.  Default: RGET
20220317 014102.899 INFO             PET3 index= 103                                 MPIR_CVAR_GPU_NO_ASYNC_COPY : If set, disables the use of asynchronous memcpy's for on-node GPU-aware transfers. Default: 0
20220317 014102.899 INFO             PET3 index= 104                                      MPIR_CVAR_ENABLE_YAKSA : If set, enables the use of the YAKSA datatype engine for packing and unpacking non-contiguous buffers in both CPU- and GPU-attached memory. Default: 0
20220317 014102.899 INFO             PET3 index= 105                              MPIR_CVAR_GPU_EAGER_DEVICE_MEM : If set to 1, GPU-attached memory will be used for the eager staging buffers for short on-node messages where at least the sending buffer is GPU-attached. If set to 0, this optimization is disabled and we fall-back to using staging buffers on the CPU for small message  intra-node device-host and device-device MPI ops. Default: 1
20220317 014102.899 INFO             PET3 index= 106                       MPIR_CVAR_GPU_EAGER_REGISTER_HOST_MEM : If set to 1, enables MPI to request POSIX  eager memory to be registered with the GPU runtime.  This is an optimization to improve the performance of  small message intra-node, inter-GPU MPI ops that use  POSIX shared memory as bounce buffers.  If set to 0, this optimization is disabled and we rely on GTL to lock/unlock host memory regions for a given on-node, small msg inter-GPU transfer.  Default: 1
20220317 014102.899 INFO             PET3 index= 107                          MPIR_CVAR_GPU_ALLREDUCE_USE_KERNEL : If set, adds a hint that the use of device kernels for reduction operations is desired. MPI is not guaranteed to use a device kernel for all reduction operations. Default: 0
20220317 014102.899 INFO             PET3 index= 108                                   MPIR_CVAR_RMA_MAX_PENDING : Determines how many RMA operation may be outstanding at any time over libfabrics. RMA operations beyond this max will be queued and only issued as pending operations complete. Default: 64
20220317 014102.899 INFO             PET3 index= 109                                MPIR_CVAR_RMA_SHM_ACCUMULATE : If set to 1, enables SHM accumulate operations. If set to 0, disables SHM accumulate operations. It also sets the default for the window hint "disable_shm_accumulate".  Default: 1
20220317 014102.899 INFO             PET3 --- VMK::logSystem() end ---------------------------------
20220317 014102.899 INFO             PET3 main: --- VMK::log() start -------------------------------------
20220317 014102.899 INFO             PET3 main: vm located at: 0x8acae0
20220317 014102.899 INFO             PET3 main: petCount=6 localPet=3 mypthid=23359915741312 currentSsiPe=3
20220317 014102.899 INFO             PET3 main: Current system level affinity pinning for local PET:
20220317 014102.899 INFO             PET3 main:  SSIPE=3
20220317 014102.899 INFO             PET3 main: Current system level OMP_NUM_THREADS setting for local PET: 128
20220317 014102.899 INFO             PET3 main: ssiCount=1 localSsi=0
20220317 014102.899 INFO             PET3 main: mpionly=1 threadsflag=0
20220317 014102.899 INFO             PET3 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220317 014102.899 INFO             PET3 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220317 014102.899 INFO             PET3 main:  PE=0 SSI=0 SSIPE=0
20220317 014102.899 INFO             PET3 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220317 014102.899 INFO             PET3 main:  PE=1 SSI=0 SSIPE=1
20220317 014102.899 INFO             PET3 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220317 014102.899 INFO             PET3 main:  PE=2 SSI=0 SSIPE=2
20220317 014102.899 INFO             PET3 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220317 014102.899 INFO             PET3 main:  PE=3 SSI=0 SSIPE=3
20220317 014102.899 INFO             PET3 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220317 014102.899 INFO             PET3 main:  PE=4 SSI=0 SSIPE=4
20220317 014102.899 INFO             PET3 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220317 014102.899 INFO             PET3 main:  PE=5 SSI=0 SSIPE=5
20220317 014102.899 INFO             PET3 main: --- VMK::log() end ---------------------------------------
20220317 014102.899 INFO             PET3 Executing 'userm1_setvm'
20220317 014102.900 INFO             PET3 Executing 'userm1_register'
20220317 014102.900 INFO             PET3 Executing 'userm2_setvm'
20220317 014102.900 INFO             PET3 Executing 'userm2_register'
20220317 014102.901 INFO             PET3 Entering 'user1_run'
20220317 014102.901 INFO             PET3 model1: --- VMK::log() start -------------------------------------
20220317 014102.901 INFO             PET3 model1: vm located at: 0x9082b0
20220317 014102.901 INFO             PET3 model1: petCount=6 localPet=3 mypthid=23359915741312 currentSsiPe=3
20220317 014102.901 INFO             PET3 model1: Current system level affinity pinning for local PET:
20220317 014102.901 INFO             PET3 model1:  SSIPE=3
20220317 014102.901 INFO             PET3 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220317 014102.901 INFO             PET3 model1: ssiCount=1 localSsi=0
20220317 014102.901 INFO             PET3 model1: mpionly=1 threadsflag=0
20220317 014102.901 INFO             PET3 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220317 014102.901 INFO             PET3 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220317 014102.901 INFO             PET3 model1:  PE=0 SSI=0 SSIPE=0
20220317 014102.901 INFO             PET3 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220317 014102.901 INFO             PET3 model1:  PE=1 SSI=0 SSIPE=1
20220317 014102.901 INFO             PET3 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220317 014102.901 INFO             PET3 model1:  PE=2 SSI=0 SSIPE=2
20220317 014102.901 INFO             PET3 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220317 014102.901 INFO             PET3 model1:  PE=3 SSI=0 SSIPE=3
20220317 014102.901 INFO             PET3 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220317 014102.901 INFO             PET3 model1:  PE=4 SSI=0 SSIPE=4
20220317 014102.901 INFO             PET3 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220317 014102.901 INFO             PET3 model1:  PE=5 SSI=0 SSIPE=5
20220317 014102.901 INFO             PET3 model1: --- VMK::log() end ---------------------------------------
20220317 014102.901 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220317 014103.268 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220317 014103.575 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220317 014103.882 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220317 014104.189 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220317 014104.496 INFO             PET3 Exiting 'user1_run'
20220317 014104.499 INFO             PET3 Entering 'user2_run'
20220317 014104.499 INFO             PET3 model2: --- VMK::log() start -------------------------------------
20220317 014104.499 INFO             PET3 model2: vm located at: 0x908430
20220317 014104.499 INFO             PET3 model2: petCount=2 localPet=1 mypthid=23359915741312 currentSsiPe=3
20220317 014104.499 INFO             PET3 model2: Current system level affinity pinning for local PET:
20220317 014104.499 INFO             PET3 model2:  SSIPE=3
20220317 014104.499 INFO             PET3 model2: Current system level OMP_NUM_THREADS setting for local PET: 3
20220317 014104.499 INFO             PET3 model2: ssiCount=1 localSsi=0
20220317 014104.499 INFO             PET3 model2: mpionly=1 threadsflag=0
20220317 014104.499 INFO             PET3 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220317 014104.499 INFO             PET3 model2: PET=0 lpid=0 tid=0 pid=0 peCount=3 accCount=0
20220317 014104.499 INFO             PET3 model2:  PE=0 SSI=0 SSIPE=0
20220317 014104.499 INFO             PET3 model2:  PE=1 SSI=0 SSIPE=1
20220317 014104.499 INFO             PET3 model2:  PE=2 SSI=0 SSIPE=2
20220317 014104.499 INFO             PET3 model2: PET=1 lpid=1 tid=0 pid=3 peCount=3 accCount=0
20220317 014104.499 INFO             PET3 model2:  PE=3 SSI=0 SSIPE=3
20220317 014104.499 INFO             PET3 model2:  PE=4 SSI=0 SSIPE=4
20220317 014104.499 INFO             PET3 model2:  PE=5 SSI=0 SSIPE=5
20220317 014104.499 INFO             PET3 model2: --- VMK::log() end ---------------------------------------
20220317 014104.499 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220317 014104.499 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220317 014104.499 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220317 014104.854 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220317 014104.854 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220317 014104.854 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220317 014105.176 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220317 014105.176 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220317 014105.176 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220317 014105.499 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220317 014105.499 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220317 014105.499 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220317 014105.821 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220317 014105.821 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220317 014105.821 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220317 014106.143 INFO             PET3  user2_run: All data correct.
20220317 014106.143 INFO             PET3 Exiting 'user2_run'
20220317 014106.143 INFO             PET3 Entering 'user1_run'
20220317 014106.143 INFO             PET3 model1: --- VMK::log() start -------------------------------------
20220317 014106.143 INFO             PET3 model1: vm located at: 0x9082b0
20220317 014106.143 INFO             PET3 model1: petCount=6 localPet=3 mypthid=23359915741312 currentSsiPe=3
20220317 014106.143 INFO             PET3 model1: Current system level affinity pinning for local PET:
20220317 014106.143 INFO             PET3 model1:  SSIPE=3
20220317 014106.143 INFO             PET3 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220317 014106.143 INFO             PET3 model1: ssiCount=1 localSsi=0
20220317 014106.143 INFO             PET3 model1: mpionly=1 threadsflag=0
20220317 014106.143 INFO             PET3 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220317 014106.143 INFO             PET3 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220317 014106.143 INFO             PET3 model1:  PE=0 SSI=0 SSIPE=0
20220317 014106.143 INFO             PET3 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220317 014106.143 INFO             PET3 model1:  PE=1 SSI=0 SSIPE=1
20220317 014106.143 INFO             PET3 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220317 014106.143 INFO             PET3 model1:  PE=2 SSI=0 SSIPE=2
20220317 014106.143 INFO             PET3 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220317 014106.143 INFO             PET3 model1:  PE=3 SSI=0 SSIPE=3
20220317 014106.143 INFO             PET3 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220317 014106.143 INFO             PET3 model1:  PE=4 SSI=0 SSIPE=4
20220317 014106.143 INFO             PET3 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220317 014106.143 INFO             PET3 model1:  PE=5 SSI=0 SSIPE=5
20220317 014106.143 INFO             PET3 model1: --- VMK::log() end ---------------------------------------
20220317 014106.143 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220317 014106.450 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220317 014106.757 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220317 014107.065 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220317 014107.372 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220317 014107.679 INFO             PET3 Exiting 'user1_run'
20220317 014107.679 INFO             PET3 Entering 'user2_run'
20220317 014107.679 INFO             PET3 model2: --- VMK::log() start -------------------------------------
20220317 014107.679 INFO             PET3 model2: vm located at: 0x908430
20220317 014107.679 INFO             PET3 model2: petCount=2 localPet=1 mypthid=23359915741312 currentSsiPe=3
20220317 014107.679 INFO             PET3 model2: Current system level affinity pinning for local PET:
20220317 014107.679 INFO             PET3 model2:  SSIPE=3
20220317 014107.679 INFO             PET3 model2: Current system level OMP_NUM_THREADS setting for local PET: 3
20220317 014107.679 INFO             PET3 model2: ssiCount=1 localSsi=0
20220317 014107.679 INFO             PET3 model2: mpionly=1 threadsflag=0
20220317 014107.679 INFO             PET3 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220317 014107.679 INFO             PET3 model2: PET=0 lpid=0 tid=0 pid=0 peCount=3 accCount=0
20220317 014107.679 INFO             PET3 model2:  PE=0 SSI=0 SSIPE=0
20220317 014107.679 INFO             PET3 model2:  PE=1 SSI=0 SSIPE=1
20220317 014107.679 INFO             PET3 model2:  PE=2 SSI=0 SSIPE=2
20220317 014107.679 INFO             PET3 model2: PET=1 lpid=1 tid=0 pid=3 peCount=3 accCount=0
20220317 014107.679 INFO             PET3 model2:  PE=3 SSI=0 SSIPE=3
20220317 014107.679 INFO             PET3 model2:  PE=4 SSI=0 SSIPE=4
20220317 014107.679 INFO             PET3 model2:  PE=5 SSI=0 SSIPE=5
20220317 014107.679 INFO             PET3 model2: --- VMK::log() end ---------------------------------------
20220317 014107.679 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220317 014107.679 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220317 014107.679 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220317 014108.002 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220317 014108.002 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220317 014108.002 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220317 014108.324 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220317 014108.324 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220317 014108.324 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220317 014108.646 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220317 014108.646 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220317 014108.646 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220317 014108.969 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:            5  Testing data for localDe =           2  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220317 014108.969 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:            4  Testing data for localDe =           1  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220317 014108.969 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:            3  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220317 014109.291 INFO             PET3  user2_run: All data correct.
20220317 014109.291 INFO             PET3 Exiting 'user2_run'
20220317 014109.291 INFO             PET3  NUMBER_OF_PROCESSORS           6
20220317 014109.291 INFO             PET3  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220317 014109.291 INFO             PET3 Finalizing ESMF
20220317 014102.897 INFO             PET4 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220317 014102.897 INFO             PET4 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220317 014102.897 INFO             PET4 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220317 014102.897 INFO             PET4 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220317 014102.897 INFO             PET4 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220317 014102.897 INFO             PET4 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220317 014102.897 INFO             PET4 Running with ESMF Version   : v8.3.0b09-100-g5ff85a963f
20220317 014102.897 INFO             PET4 ESMF library build date/time: "Mar 17 2022" "01:27:56"
20220317 014102.897 INFO             PET4 ESMF library build location : /lfs/h1/emc/ptmp/Mark.Potts/gfortran_10.3.0_mpi_O_jedwards_pio_update2
20220317 014102.897 INFO             PET4 ESMF_COMM                   : mpi
20220317 014102.897 INFO             PET4 ESMF_MOAB                   : enabled
20220317 014102.897 INFO             PET4 ESMF_LAPACK                 : enabled
20220317 014102.897 INFO             PET4 ESMF_NETCDF                 : enabled
20220317 014102.897 INFO             PET4 ESMF_PNETCDF                : disabled
20220317 014102.897 INFO             PET4 ESMF_PIO                    : enabled
20220317 014102.897 INFO             PET4 ESMF_YAMLCPP                : enabled
20220317 014102.898 INFO             PET4 --- VMK::logSystem() start -------------------------------
20220317 014102.898 INFO             PET4 esmfComm=mpi
20220317 014102.898 INFO             PET4 isPthreadsEnabled=1
20220317 014102.898 INFO             PET4 isOpenMPEnabled=1
20220317 014102.898 INFO             PET4 isOpenACCEnabled=0
20220317 014102.898 INFO             PET4 isSsiSharedMemoryEnabled=1
20220317 014102.898 INFO             PET4 ssiCount=1 peCount=6
20220317 014102.898 INFO             PET4 PE=0 SSI=0 SSIPE=0
20220317 014102.898 INFO             PET4 PE=1 SSI=0 SSIPE=1
20220317 014102.898 INFO             PET4 PE=2 SSI=0 SSIPE=2
20220317 014102.898 INFO             PET4 PE=3 SSI=0 SSIPE=3
20220317 014102.898 INFO             PET4 PE=4 SSI=0 SSIPE=4
20220317 014102.898 INFO             PET4 PE=5 SSI=0 SSIPE=5
20220317 014102.898 INFO             PET4 --- VMK::logSystem() MPI Control Variables ---------------
20220317 014102.898 INFO             PET4 index=   0          MPIR_CVAR_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE : specifies the cutoff size of the send buffer (in bytes) above which the reduce_scatter functions attempt to use the pairwise exchange algorithm.  In addition, the op must be commutative and the communicator size < MPIR_CVAR_REDUCE_SCATTER_MAX_COMMSIZE for the pairwise exchange algorithm to be used.
20220317 014102.898 INFO             PET4 index=   1                       MPIR_CVAR_REDUCE_SCATTER_MAX_COMMSIZE : specifies the max communicator size that will trigger use of the pairwise exchange algorithm, provided the op is commutative.  The pairwise exchange algorithm is not well suited for scaling to high process counts, so for larger communicators, a recursive halving algorithm is used instead.
20220317 014102.898 INFO             PET4 index=   2                           MPIR_CVAR_NETWORK_BUFFER_COLL_OPT : If set to 1, the MPICH library will use the optimized shared- memory based "network buffer" design for collective operations.  This feature is closely tied to the shared-memory collective optimization available in Cray MPICH. If enabled, the shared-memory buffer is also registered with the NIC and can be used directly to  perform off-node transfers, bypassing the Nemesis channel layer.  This feature is disabled if MPICH_SHARED_MEM_COLL_OPT  is set to 0. Currently, this optimization is only available  for the MPI_Bcast collective operation. To disable this feature,  set MPICH_NETWORK_BUFFER_COLL_OPT to 0.  Default: 0
20220317 014102.898 INFO             PET4 index=   3                                MPIR_CVAR_ALLTOALL_SYNC_FREQ : Adjusts the number of outstanding messages each Alltoall process  will allow.  Default is variable.
20220317 014102.898 INFO             PET4 index=   4                                 MPIR_CVAR_ALLTOALL_BLK_SIZE : The transfer size in bytes for the Alltoall chunking algorithm.  Default is 16384.
20220317 014102.898 INFO             PET4 index=   5                       MPIR_CVAR_ALLTOALL_CHUNKING_MAX_NODES : The maximum number of nodes to use the alltoall chunking algorithm. Above this value, the throttled algorithm will be used.  This is only applicable to SS-11.
20220317 014102.898 INFO             PET4 index=   6                              MPIR_CVAR_ALLGATHER_VSHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized gather/bcast algorithm is used instead of the optimized ring algorithm for MPI_Allgather. The gather/bcast algorithm is better suited for small messages. Defaults: For communicator sizes of <=512 ranks, 1024 bytes. For communicator sizes of >512 ranks, 4096 bytes.
20220317 014102.898 INFO             PET4 index=   7                             MPIR_CVAR_ALLGATHERV_VSHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized gatherv/bcast algorithm is used instead of the optimized ring algorithm for MPI_Allgatherv. The gatherv/bcast algorithm is better suited for small messages. Defaults: For communicator sizes of <=512 ranks, 1024 bytes. For communicator sizes of >512 ranks, 4096 bytes.
20220317 014102.898 INFO             PET4 index=   8                                  MPIR_CVAR_ALLREDUCE_NO_SMP : If set, MPI_Allreduce uses an algorithm that is not smp- aware. This provides a consistent ordering of the specified allreduce operation regardless of system configuration. Note:  This algorithm may not perform as well as the default smp-aware algorithms as it does not take advantage of rank topology. Default: not set
20220317 014102.898 INFO             PET4 index=   9                                MPIR_CVAR_ALLTOALL_SHORT_MSG : Adjusts the cut-off points at and below which the store and forward Alltoall algorithm is used for short messages. The default value is dependent upon the total number of ranks in the MPI communicator used for the MPI_Alltoall call and the Alltoall algorithm being selected. Defaults: If using one of the non-default send/recv algorithms on Aries, the defaults are: if communicator size <= 512, 2048 bytes if communicator size > 512 and <= 1024, 1024 bytes if communicator size > 1024 and <= 65536, 128 bytes if communicator size > 65536 and <= 131072, 64 bytes if communicator size > 131072 , 32 bytes
20220317 014102.898 INFO             PET4 index=  10                                MPIR_CVAR_ALLTOALLV_THROTTLE : Sets the per-process maximum number of outstanding Isends and Irecvs that can be posted concurrently for the optimized send/recv MPI_Alltoallv algorithm. For large messages, consider decreasing the throttle to 1 or 2 to improve performance. Defaults: 8
20220317 014102.898 INFO             PET4 index=  11                                   MPIR_CVAR_BCAST_ONLY_TREE : If set to 1, MPI_Bcast uses an smp-aware tree algorithm regardless of data size. The tree algorithm generally scales well to high processor counts on Cray XE systems. If set to 0, MPI_Bcast uses a variety of algorithms (tree, scatter, or ring) depending on message size and other factors. These other algorithms generally do not scale well when using more than 512 processors on Cray XE systems. Default: 1
20220317 014102.898 INFO             PET4 index=  12                             MPIR_CVAR_BCAST_INTERNODE_RADIX : Used to set the radix of the inter-node tree. This can be set to any integer value greater than or equal to 2. Default: 4
20220317 014102.898 INFO             PET4 index=  13                             MPIR_CVAR_BCAST_INTRANODE_RADIX : Used to set the radix of the intra-node tree. This can be set to any integer value greater than or equal to 2. Default: 4
20220317 014102.898 INFO             PET4 index=  14                                      MPIR_CVAR_COLL_OPT_OFF : If set, disables collective optimizations which use nondefault, architecture-specific algorithms for some MPI collective operations. By default, all collective optimized algorithms are enabled. To disable all collective optimized algorithms, set MPICH_COLL_OPT_OFF to 1. To disable optimized algorithms for selected MPI collectives, set the value to a comma-separated list of the desired collective names. Names are not case-sensitive. Any unrecognizable name is flagged with a warning message and ignored. For example, to disable the MPI_Allgather optimized collective algorithm, set MPICH_COLL_OPT_OFF=mpi_allgather. The following collective names are recognized: MPI_Allgather, MPI_Allgatherv, MPI_Allreduce, MPI_Alltoall, MPI_Alltoallv, MPI_Bcast, MPI_Gatherv, MPI_Scatterv, MPI_Igatherv, and MPI_Iallreduce. Default: Not enabled.
20220317 014102.898 INFO             PET4 index=  15                                         MPIR_CVAR_COLL_SYNC : If set, a Barrier is performed at the beginning of each specified MPI collective function. This forces all processes participating in that collective to sync up before the collective can begin. To disable this feature for all MPI collectives, set the value to 0. This is the default. To enable this feature for all MPI collectives, set the value to 1. To enable this feature for selected MPI collectives, set the value to a comma-separated list of the desired collective names. Names are not case-sensitive. Any unrecognizable name is flagged with a warning message and ignored. The following collective names are recognized: MPI_Allgather, MPI_Allgatherv, MPI_Allreduce, MPI_Alltoall, MPI_Alltoallv, MPI_Alltoallw, MPI_Bcast, MPI_Exscan, MPI_Gather, MPI_Gatherv, MPI_Reduce, MPI_Reduce_scatter, MPI_Scan, MPI_Scatter, and MPI_Scatterv. Default: Not enabled.
20220317 014102.898 INFO             PET4 index=  16                                 MPIR_CVAR_GATHERV_SHORT_MSG : Adjusts the cutoff point at and below which the optimized tree MPI_Gatherv algorithm is used instead of the optimized permission-to-send MPI_Gatherv algorithm. The cutoff is based on the average size of the variable MPI_Gatherv message sizes.  Default: 131072 bytes
20220317 014102.898 INFO             PET4 index=  17                                     MPIR_CVAR_REDUCE_NO_SMP : If set, MPI_Reduce uses an algorithm that is not smp-aware. This provides a consistent ordering of the specified reduce operation regardless of system configuration. Note:  This algorithm may not perform as well as the default smp-aware algorithms as it does not take advantage of rank topology. Default: not set
20220317 014102.898 INFO             PET4 index=  18                              MPIR_CVAR_SCATTERV_SYNCHRONOUS : The default, non-optimized ANL MPI_Scatterv algorithm uses asynchronous sends by default for communicator sizes less than 200,000 ranks. If set, this environment variable causes MPI_Scatterv to switch to using blocking sends, which may be beneficial in certain cases involving large data sizes or high process counts. For communicator sizes equal to or greater than 200,000 ranks, the blocking send algorithm is used by default. Default: not enabled
20220317 014102.898 INFO             PET4 index=  19                               MPIR_CVAR_SHARED_MEM_COLL_OPT : If set, the MPICH library will use the optimized shared- memory based design for collective operations. On Gemini and Aries systems, the supported collective operations are: MPI_Allreduce, MPI_Iallreduce, MPI_Barrier, and MPI_Bcast. To enable all available shared-memory optimizations, set MPICH_SHARED_MEM_COLL_OPT to 1. To enable this feature for a specific set of collective operations, set MPICH_SHARED_MEM_COLL_OPT to a comma- separated list of collective names. For example, to enable this optimization for MPI_Bcast only, set MPICH_SHARED_MEM_COLL_OPT=MPI_Bcast. To enable this optimization for MPI_Allreduce only, set MPICH_SHARED_MEM_COLL_OPT=MPI_Allreduce. Unsupported names are flagged with a warning message and ignored. Default: set
20220317 014102.898 INFO             PET4 index=  20                   MPIR_CVAR_ALLREDUCE_GPU_STAGING_THRESHOLD : If the sendbuf/recvbuf of an Allreduce operation is on GPU-resident memory regions, for payload sizes larger than this variable,  the Cray optimized staging implementation will be used.  By default, the staging implementation is used for all payload sizes.  This variable has no effect if MPICH_GPU_COLL_STAGING_AREA_OPT is set to 0. Default: 0
20220317 014102.898 INFO             PET4 index=  21                         MPIR_CVAR_GPU_COLL_STAGING_BUF_SIZE : This variable determines the size of the staging buffers used for some collectives (such as MPI_Allreduce) when sendbuf/recvbuf on GPU-resident buffers. This variable has no effect if  MPICH_GPU_COLL_STAGING_AREA_OPT is set to 0.   Default: 262144
20220317 014102.898 INFO             PET4 index=  22                                 MPIR_CVAR_GATHERV_SYNC_FREQ : Only applicable to the Gatherv permission-to-send algorithm.  Adjusts  the number of outstanding receives the root for Gatherv will allow.   Default is 16.
20220317 014102.898 INFO             PET4 index=  23                              MPIR_CVAR_GATHERV_MAX_TMP_SIZE : Only applicable to the Gatherv tree algorithm.  Sets the maximum amount of temporary memory GAtherv will allow a rank to allocate when using the tree-based algorithm.  Each rank allocates a different amount, with many allocating no extra memory.  If any rank requires more than this amount of temporary buffer space, a different algorithm is used. Default is 512MB.
20220317 014102.898 INFO             PET4 index=  24                             MPIR_CVAR_GATHERV_MIN_COMM_SIZE : Cray MPI offers two optimized Gatherv algorithms.  A tree algorithm for small messages and a permission-to-send send algorithm for larger messages.  Set  this value to the minimum communicator size to attempt use of either of the  Cray optimized Gatherv algorithms. Default is 64
20220317 014102.898 INFO             PET4 index=  25                                MPIR_CVAR_SCATTERV_SHORT_MSG : Adjusts the cutoff point at and below which the optimized tree MPI_Scatterv algorithm is used instead of the optimized staggered send algorithm.  The cutoff is in bytes, based on the average size of the variable MPI_Scatterv message sizes. Default behavior if unset is: For communicator sizes of <= 512 ranks, 2048 bytes For communicator sizes of > 512 ranks, 8192 bytes
20220317 014102.898 INFO             PET4 index=  26                                MPIR_CVAR_SCATTERV_SYNC_FREQ : Only applicable to the Scatterv staggered send algorithm.  Adjusts the number of outstanding sends the root for Scatterv will use. Default is 16.
20220317 014102.898 INFO             PET4 index=  27                             MPIR_CVAR_SCATTERV_MAX_TMP_SIZE : Only applicable to the Scatterv tree algorithm.  Sets the maximum amount of temporary memory Scatterv will allow a rank to allocate when using the tree-based algorithm.  Each rank allocates a different amount, with many allocating no extra memory.  If any rank requires more than this amount of temporary buffer space, a different algorithm is used. Default is 512MB.
20220317 014102.898 INFO             PET4 index=  28                            MPIR_CVAR_SCATTERV_MIN_COMM_SIZE : Cray MPI offers two optimized Scatterv algorithms.  A tree algorithm for small messages and a staggered send algorithm for larger messages.  Set this value to the minimum communicator size to attempt use of either of the Cray optimized Scatterv algorithms. Default is 64
20220317 014102.898 INFO             PET4 index=  29                           MPIR_CVAR_MPIIO_ABORT_ON_RW_ERROR : If set to enable, causes MPI-IO to abort immediately after issuing an error message if an I/O error occurs during a system read() or write() call. This applies only to I/O errors for system read() and write() calls made as a result of MPI I/O calls. It does not apply to I/O errors for other MPI I/O calls such as MPI_File_open(), nor does it apply to read() and write() calls made by means other than MPI I/O calls. Abort on error is not standard behavior. The MPI Standard specifies that the default error handling for MPI I/O calls is to return an error code to the application rather than aborting the application, but since errors on write or read are almost always unexpected and usually not recoverable, it may be preferable to abort as soon as the error is detected. Doing so does not allow any recovery, but does provide the most information about the error and terminates the job quickly. If the Cray Abnormal Termination Processing (ATP) feature is enabled, the abort will result in a full stack backtrace writte
20220317 014102.898 INFO             PET4 index=  30                MPIR_CVAR_MPIIO_AGGREGATOR_PLACEMENT_DISPLAY : This variable controls whether the placement of the aggregators will be displayed when a file is opened. The placement can be  controlled on a per file basis with the aggregator_placement_stride  hint. If set, displays the assignment of MPIIO collective buffering aggregators for reads/writes of a shared file, showing rank and node ID (nid). For example: Aggregator Placement for /lus/scratch/myfile RankReorderMethod=3  AggPlacementStride=-1 AGG    Rank       nid ----  ------  -------- 0       0  nid00578 1       4  nid00579 2       1  nid00606 3       5  nid00607 4       2  nid00578 5       6  nid00579 6       3  nid00606 7       7  nid00607 Default: not set
20220317 014102.898 INFO             PET4 index=  31                 MPIR_CVAR_MPIIO_AGGREGATOR_PLACEMENT_STRIDE : Partially controls to which nodes MPIIO collective buffering aggregators are assigned. See the notes below on the order of nodes. Network traffic and resulting I/O performance may be affected by the assignments. If set to 1, consecutive nodes are used. The number of aggregators assigned per node is controlled by the cb_config_list hint. By default, no more than one aggregator per node will be assigned if there are at least as many nodes as aggregators. If set to a value greater than 1, node selection is strided across the available nodes by this value. If the stride times the number of aggregators exceeds the number of nodes, the assignments will wrap around, which is usually not optimal for performance. If set to -1, node selection is strided across available nodes by the value of the number of nodes divided by the number of aggregators (integer division, minimum value of 1). The purpose is to spread out the nodes to reduce network congestion. Note:  The order of nodes can be shown by setting the MPICH_RANK
20220317 014102.898 INFO             PET4 index=  32                                    MPIR_CVAR_MPIIO_CB_ALIGN : Sets the default value for the cb_align hint. Files opened with MPI_File_open wil have this value for the cb_align hint unless the hint is set on a per file basis with either the MPICH_MPIIO_HINTS environment variable or from within a program with the MPI_Info_set() call. Note:  Only MPICH_MPIIO_CB_ALIGN == 2 is fully supported. Other values are for internal testing only. Default: 2
20220317 014102.898 INFO             PET4 index=  33                                MPIR_CVAR_MPIIO_DVS_MAXNODES : Note:  This environment variable in relevant only for file systems accessed from Cray system compute nodes via DVS server nodes; e.g. GPFS or PANFS. As described in the dvs(5) man page, the environment variable DVS_MAXNODES can be used to set the stripe width— that is, the number of DVS server nodes—used to access a file in "stripe parallel mode." For most files, and especially for small files, setting DVS_MAXNODES to 1 ("cluster parallel mode") is preferred. The MPICH_MPIIO_DVS_MAXNODES environment variable enables you to leave DVS_MAXNODES set to 1 and then use MPICH_MPIIO_DVS_MAXNODES to temporarily override DVS_MAXNODES when it is advantageous to specify wider striping for files being opened by the MPI_File_open() call. The range of values accepted by MPICH_MPIIO_DVS_MAXNODES goes from 1 to the number of server nodes specified on the mount with the nnodes mount option. DVS_MAXNODES is not set by default. Therefore, for MPICH_MPIIO_DVS_MAXNODES to have any effect, DVS_MAXNODES must be defined before p
20220317 014102.898 INFO             PET4 index=  34                                       MPIR_CVAR_MPIIO_HINTS : If set, override the default value of one or more MPI I/O hints. This also overrides any values that were set by using calls to MPI_Info_set in the application code. The new values apply to the file the next time it is opened using an MPI_File_open() call. After the MPI_File_open() call, subsequent MPI_Info_set calls can be used to pass new MPI I/O hints that take precedence over some of the environment variable values. Other MPI I/O hints such as striping_factor, striping_unit, cb_nodes, and cb_config_list cannot be changed after the MPI_File_open() call, as these are evaluated and applied only during the file open process. An MPI_File_close call followed by an MPI_File_open call can be used to restart the MPI I/O hint evaluation process. The syntax for this environment variable is a comma- separated list of specifications. Each individual specification is a pathname_pattern followed by a colon- separated list of one or more key=value pairs. In each key=value pair, the key is the MPI-IO hint name, and the v
20220317 014102.898 INFO             PET4 index=  35                               MPIR_CVAR_MPIIO_HINTS_DISPLAY : If set, causes rank 0 in the participating communicator to display the names and values of all MPI-IO hints that are set for the file being opened with the MPI_File_open call. It also displays relevant environment variables whether or not MPICH_ENV_DISPLAY is set. Default: not enabled.
20220317 014102.898 INFO             PET4 index=  36                               MPIR_CVAR_MPIIO_MAX_NUM_IRECV : When MPIIO collective buffering is used, this environment variable controls the number of outstanding MPI_Irecv calls allowed before an MPI_Waitall is done. Default: 50
20220317 014102.898 INFO             PET4 index=  37                               MPIR_CVAR_MPIIO_MAX_NUM_ISEND : When MPIIO collective buffering is used, this environment variable controls the number of outstanding MPI_Isend calls allowed before an MPI_Waitall is done. Default: 50
20220317 014102.898 INFO             PET4 index=  38                              MPIR_CVAR_MPIIO_MAX_SIZE_ISEND : When MPIIO collective buffering is used, this environment variable limits MPI_Isend by the amount of data being sent rather than by the number of calls. Default: 10485760 bytes
20220317 014102.898 INFO             PET4 index=  39                                       MPIR_CVAR_MPIIO_STATS : If set to 1, a summary of file write and read access patterns is written by rank 0 to stderr. This information provides some insight into how I/O performance may be improved. The information is provided on a per-file basis and is written when the file is closed. It does not provide any timing information. If set to 2, a set of data files are written to the working directory, one file for each rank, with the filename prefix specified by the MPICH_MPIIO_STATS_FILE environment variable. The data is in comma-separated values (CSV) format, which can be summarized with the cray_mpiio_summary script in the /opt/cray/mpt/version/gni/bin directory. Additional example scripts are provided in that directory to further process and display the data. Only enabled if MPIR_CVAR_MPIIO_CB_ALIGN=2. Default: not set
20220317 014102.898 INFO             PET4 index=  40                                  MPIR_CVAR_MPIIO_STATS_FILE : Specifies the filename prefix for the set of data files written when MPICH_MPIIO_STATS is set to 2. The filename prefix may be a full absolute pathname or a relative pathname. Summary plots of these files can be generated using the cray_mpiio_summary script from the /opt/cray/mpt/version/gni/bin directory. Other example scripts for post-processing this data can also be found in /opt/cray/mpt/version/gni/bin. Default: _cray_mpiio_stats_
20220317 014102.898 INFO             PET4 index=  41                         MPIR_CVAR_MPIIO_STATS_INTERVAL_MSEC : Specifies the time interval in milliseconds for each MPICH_MPIIO_STATS data point. Default: 250
20220317 014102.898 INFO             PET4 index=  42                                      MPIR_CVAR_MPIIO_TIMERS : If set to 0, or not set at all, no timing data is collected. If set to 1, timing data for different phases in MPI-IO is collected locally by each MPI process and then during MPI_File_close the data is consolidated and printed. Some timing data is displayed in seconds, other data is displayed in clock ticks, possibly scaled down. Also see MPICH_MPIIO_TIMERS_SCALE The relative values of the reported times are more important to the analysis than the absolute time. More detailed information about MPI-IO performance can be obtained by using the MPICH_MPIIO_STATS feature and by using the CrayPat and Apprentice2 Timeline Report of I/O bandwidth. Only enabled if MPIR_CVAR_MPIIO_CB_ALIGN=2. Default: 0
20220317 014102.898 INFO             PET4 index=  43                                MPIR_CVAR_MPIIO_TIMERS_SCALE : Specifies the power of 2 to use to scale the times reported by MPICH_MPIIO_TIMERS.  The raw times are collected in clock ticks. This generally is a very large number and reducing all the times by the same scaling factor makes for a more compact display. If set to 0, or not set at all, MPI-IO automatically determines a scaling factor to limit the report times to 9 or fewer digits. This auto-determined value is displayed.  To make run to run comparisons, you can set the scaling factor to your preferred value. Default: 0
20220317 014102.898 INFO             PET4 index=  44                                  MPIR_CVAR_MPIIO_TIME_WAITS : If set to non-zero, time how long this rank has to wait for other ranks to catch up.  This separates true metadata time from imbalance time. This is disabled when MPICH_MPIIO_TIMERS is not set.  Otherwise it defaults to 1. Default: 1
20220317 014102.898 INFO             PET4 index=  45                          MPIR_CVAR_MPIIO_WRITE_EXIT_BARRIER : If set to non-zero, collective write's will barrier on exit Default: 1
20220317 014102.898 INFO             PET4 index=  46                               MPIR_CVAR_MPIIO_DS_WRITE_CRAY : If set to non-zero, collective write's with data sieving will be optimized  Default: 1
20220317 014102.898 INFO             PET4 index=  47                         MPIR_CVAR_MPIIO_OFI_STARTUP_CONNECT : If set to enable, causes MPI-IO to establish connections between ranks and aggregators during the openning of the file to be used for MPIIO collective operations
20220317 014102.898 INFO             PET4 index=  48                MPIR_CVAR_MPIIO_OFI_STARTUP_NODES_AGGREGATOR : If MPIIO_OFI_STARTUP_CONNECT is enabled, this specifies the number of nodes that will concurrently attempt to connext to each aggrgator. Each rank on each node will establish its own connection to the aggregator. Default: 2
20220317 014102.898 INFO             PET4 index=  49                                           MPIR_CVAR_DPM_DIR : Sets the directory to use for MPI port name publishing in the  file-based nameserv implementation, as well as publishing the  credential obtained from libdrc.
20220317 014102.899 INFO             PET4 index=  50                               MPIR_CVAR_SINGLE_HOST_ENABLED : If true and if running as a single node, then inter-node communications is never initialized. The helps prevent the use of scarce hardware resources.
20220317 014102.899 INFO             PET4 index=  51                                       MPIR_CVAR_OFI_VERBOSE : If set to 1, enable verbose output about the CH4 OFI device. If set to 2 or higher, enable more verbose output.
20220317 014102.899 INFO             PET4 index=  52                                 MPIR_CVAR_OFI_XRCD_BASE_DIR : Specifies the base directory for the XRCD file.  This is only  used if the job is not launched via PALS launcher.  If launched  via PALS, we use the private, temporary PALS_SPOOL_DIR directory. Default: /tmp
20220317 014102.899 INFO             PET4 index=  53                                   MPIR_CVAR_OFI_NIC_VERBOSE : If set to 1 or higher, enables verbose output about the CH4 OFI NIC selection.   Non-fatal warnings are also displayed.  If set to 2 or higher, each rank in  the job will display it's NIC selection.  If set to 3 or higher, more verbose  output is enabled (for debug purposes).
20220317 014102.899 INFO             PET4 index=  54                                    MPIR_CVAR_OFI_NIC_POLICY : Specifies the rank-to-nic policy.  The supported options are:  NUMA | GPU | BLOCK | ROUND-ROBIN | USER
20220317 014102.899 INFO             PET4 index=  55                                   MPIR_CVAR_OFI_NIC_MAPPING : Specifies the precise rank-to-NIC mapping to use on each node.  This is  required if the NIC_POLICY is set to USER.  Each local rank must have a NIC mapping assigned by this env variable. If there are fewer MPI ranks on any  node, that part of the MAPPING string will be ignored.  For example: To assign local ranks 0,16,32,48 to NIC 0, and remaining ranks to NIC 1 MPICH_OFI_NIC_MAPPING="0:0,16,32,48; 1:1-15,17-31,33-47,49-63"
20220317 014102.899 INFO             PET4 index=  56                                      MPIR_CVAR_OFI_NUM_NICS : If set, specifies the number of NICs the job can use on each node. By default, when multiple NICs per node are available, MPI attempts to use them all.  If using fewer NICs is desired, this env variable  can be set to indicate the maximum number of NICs per node MPI will  use. By default, consecutive NICs are used, starting with index 0.   To limit use to 1 NIC/node, use : export MPICH_OFI_NUM_NICS=1 To limit use to 2 NICs/node, use: export MPICH_OFI_NUM_NICS=2 To specify alternative NIC index values, you may indicate the desired index values by adding a colon, followed by the NIC indexes.  Use  quotes so the shell doesn't interfere.  For example: To limit use to 1 NIC/node, index 1    : export MPICH_OFI_NUM_NICS="1:1" To limit use to 2 NICs/node, index 1&3 : export MPICH_OFI_NUM_NICS="2:1,3"
20220317 014102.899 INFO             PET4 index=  57                        MPIR_CVAR_OFI_SKIP_NIC_SYMMETRY_TEST : If set to 1, CrayMPICH will bypass it's check for NIC symmetry on all nodes in the job. By default, this check is run during MPI_Init to  make sure all the nodes in the job have the same number of NICs available.
20220317 014102.899 INFO             PET4 index=  58                               MPIR_CVAR_OFI_STARTUP_CONNECT : If set to 1, CrayMPICH will create COMM_WORLD static OFI connections to every other rank during MPI_Init.
20220317 014102.899 INFO             PET4 index=  59                           MPIR_CVAR_OFI_RMA_STARTUP_CONNECT : If set to 1, CrayMPICH RMA will create static OFI connections to every other rank on the same node during MPI_Init. This will also enable MPIR_CVAR_OFI_STARTUP_CONNECT.
20220317 014102.899 INFO             PET4 index=  60                                MPIR_CVAR_OFI_DEFAULT_TCLASS : If set, CrayMPICH will assign the requested default TC (traffic class) to the job domain.  By default, all endpoints inherit this TC.
20220317 014102.899 INFO             PET4 index=  61                                 MPIR_CVAR_OFI_TCLASS_ERRORS : Determines how CrayMPICH responds to traffic class errors.  Valid values are "warn", "silent" or "error".
20220317 014102.899 INFO             PET4 index=  62                                  MPIR_CVAR_OFI_CXI_PID_BASE : Set to the base value that MPI will use to assign portal IDs for the CXI provider.  Max PID value is 510.  MPI uses PID_BASE+lrank as the unique value per rank per node.  This is only applicable to SS11.
20220317 014102.899 INFO             PET4 index=  63                          MPIR_CVAR_OFI_USE_SCALABLE_STARTUP : Set to false (0) to bypass the scalable startup feature for CXI. This is only applicable to SS11.
20220317 014102.899 INFO             PET4 index=  64                                       MPIR_CVAR_UCX_VERBOSE : If set to 1, enable verbose output about the CH4 UCX device. If set to 2 or higher, enable more verbose output.
20220317 014102.899 INFO             PET4 index=  65                                  MPIR_CVAR_UCX_RC_MAX_RANKS : By default, CrayMPI selects either the UCX RC or UD protocols for inter-node messaging.  If a job is launched with MPICH_UCX_RC_MAX_RANKS  ranks or fewer, the RC protocol is selected.  If more ranks are launched,  the UCX UD protocol is chosen.  The RC protocol does not scale well due to the resource requirements.  Note this default can be overridden by setting the UCX_TLS environment variable.
20220317 014102.899 INFO             PET4 index=  66                              MPIR_CVAR_SMP_SINGLE_COPY_MODE : If non-null, choose an on-node copy mode for large messages. This variable can be set to XPMEM, CMA, or NONE. By default, Cray MPICH will look to use XPMEM. If XPMEM is requested, but not available, Cray MPICH will attempt to use CMA. (NOTE: Cray MPICH does not support CMA currently. The env. variables and CVARs are set up to allow CMA usage with future versions of Cray MPICH) If both XPMEM and CMA cannot be used, Cray MPICH will fall back to using the ANL shared memory implementation (2-copy) Default: NULL
20220317 014102.899 INFO             PET4 index=  67                              MPIR_CVAR_SMP_SINGLE_COPY_SIZE : Specifies the minimum message size in bytes to consider for single-copy transfers for on-node messages. This applies only to the SMP (on-node shared memory) device. The value is interpreted as bytes, unless the string ends in a K, which indicates kilobytes, or M, which indicates megabytes. Default: 8192
20220317 014102.899 INFO             PET4 index=  68                       MPIR_CVAR_SHM_PROGRESS_MAX_BATCH_SIZE : Adjusts the maximum number of on-node requests that can be processed in a single batch. Higher values for the maximum batch size can lower the overhead due to entering the progress engine, but can also  delay the processing of off-node message requests. Default is 8.
20220317 014102.899 INFO             PET4 index=  69                                MPIR_CVAR_CH4_RMA_THREAD_HOT : If true, the big lock is disabled for certain RMA operations
20220317 014102.899 INFO             PET4 index=  70                                    MPIR_CVAR_ABORT_ON_ERROR : If set, causes MPICH to abort and produce a core dump when MPICH detects an internal error. Note that the core dump size limit (usually 0 bytes by default) must be reset to an appropriate value in order to enable coredumps. Default: Not enabled.
20220317 014102.899 INFO             PET4 index=  71                                   MPIR_CVAR_CPUMASK_DISPLAY : If set, causes each MPI rank in the job to display its CPU affinity bitmask. Note that this reports only the CPU affinity masks for the MPI ranks; if you have a hybrid program, it does not provide any thread information. The bitmask is read from right to left, meaning the value in the rightmost position corresponds to CPU 0 on the node.
20220317 014102.899 INFO             PET4 index=  72                                       MPIR_CVAR_ENV_DISPLAY : If set, causes rank 0 to display all MPICH environment variables and their current settings at MPI initialization time. If two or more nodes are used, MPICH/GNI environment settings are also included in the listing. Default: Not enabled.
20220317 014102.899 INFO             PET4 index=  73                                  MPIR_CVAR_OPTIMIZED_MEMCPY : Specifies which version of memcpy to use. Valid values are: 0         Use the system (glibc) version of memcpy. 1         Use an optimized version of memcpy if one is available for the processor being used. In this release, an optimized version of memcpy() is available only for Intel processors. 2         Use a highly optimized version of memcpy that provides better performance in some areas but may have performance regressions in other areas, if one is available for the processor being used. In this release, a highly optimized version of memcpy() is available only for Intel Haswell processors. MPICH_OPTIMIZED_MEMCPY is overridden by MPICH_USE_SYSTEM_MEMCPY. If MPICH_USE_SYSTEM_MEMCPY is set, MPICH_OPTIMIZED_MEMCPY is ignored and the system (glibc) version of memcpy() is used. Default: 1
20220317 014102.899 INFO             PET4 index=  74                                     MPIR_CVAR_STATS_DISPLAY : If set to 1, a summary of MPI statistics, also available through the MPI Tools Interface, will be written by rank 0 to stderr. If set to 2, all ranks will produce an individualized statistics summary and write to file on a per-rank basis. The MPICH_STATS_FILE determines the prefix of the file to be used. This information may provide insight into how MPI performance may be improved. Default: 0
20220317 014102.899 INFO             PET4 index=  75                                   MPIR_CVAR_STATS_VERBOSITY : Specifies the verbosity of the MPI statistics summary. This information may provide insight into how MPI performance may be improved. Increase the value for more detailed summary. Default: 1 1        USER_BASIC  (default) 2        USER_DETAIL 3        USER_ALL 4        TUNER_BASIC 5        TUNER_DETAIL 6        TUNER_ALL 7        MPIDEV_BASIC 8        MPIDEV_DETAIL 9        MPIDEV_ALL
20220317 014102.899 INFO             PET4 index=  76                                        MPIR_CVAR_STATS_FILE : Specifies the filename prefix for the set of data files written when MPICH_STATS_DISPLAY is set to 2. The filename prefix may be a full absolute pathname or a relative pathname. Default: _cray_stats_
20220317 014102.899 INFO             PET4 index=  77                              MPIR_CVAR_RANK_REORDER_DISPLAY : If set, causes rank 0 to display which node each MPI rank resides in. The rank order can be manipulated via the MPICH_RANK_REORDER_METHOD environment variable or MPIR_CVAR_RANK_REORDER_METHOD control variable. Default: Not set
20220317 014102.899 INFO             PET4 index=  78                               MPIR_CVAR_RANK_REORDER_METHOD : Overrides the default MPI rank placement scheme. If this variable is not set, the default aprun launcher placement policy is used. The default policy for aprun is SMP-style placement. To display the MPI rank placement information, set MPICH_RANK_REORDER_DISPLAY. See manpage for more details. Default: 1, for SMP-style placement.
20220317 014102.899 INFO             PET4 index=  79                                 MPIR_CVAR_USE_SYSTEM_MEMCPY : Note:  This environment variable is deprecated and scheduled to be removed in a future release. Use MPICH_OPTIMIZED_MEMCPY instead. If set, use the system (glibc) version of memcpy(); otherwise, an optimized version of memcpy() may be used. Currently, an optimized version of memcpy() is available only for Intel processors. Default: Not set
20220317 014102.899 INFO             PET4 index=  80                                   MPIR_CVAR_VERSION_DISPLAY : If set, causes MPICH to display the CRAY MPICH version number as well as build date information. Default: Not enabled
20220317 014102.899 INFO             PET4 index=  81                          MPIR_CVAR_USE_GPU_STREAM_TRIGGERED : If set, causes MPICH to allow using stream triggered GPU communication operations. Default: Not enabled
20220317 014102.899 INFO             PET4 index=  82                               MPIR_CVAR_NUM_MAX_GPU_STREAMS : If set, causes MPICH to allow using the set maximum number of streams concurrently in the stream triggered GPU communication operations. Default: 26
20220317 014102.899 INFO             PET4 index=  83                                  MPIR_CVAR_MEMCPY_MEM_CHECK : If set, enables a check of the memcpy() source and destination areas. If they overlap, the application asserts with an error message listing the file, line, and memory range overlap. If this error is found, correct it either by changing the memory ranges or possibly by using MPI_IN_PLACE. Default: not set (off)
20220317 014102.899 INFO             PET4 index=  84                                     MPIR_CVAR_MSG_QUEUE_DBG : If set, turns on TotalView Message Queue Debugging support so that message queues are tracked in the TotalView debugger and a message queue graph can be generated. Enabling this feature degrades performance. Default: not enabled.
20220317 014102.899 INFO             PET4 index=  85                             MPIR_CVAR_NO_BUFFER_ALIAS_CHECK : If set, the buffer alias error check for collectives is disabled. The MPI standard does not allow aliasing of type OUT or INOUT parameters on the same collective function call. The use of MPI_IN_PLACE is required in these scenarios. A new check was added in MPT 5.2 to detect this condition and report the error. To bypass this check, set MPICH_NO_BUFFER_ALIAS_CHECK to any value. Default: not set
20220317 014102.899 INFO             PET4 index=  86                                MPIR_CVAR_ALLOC_MEM_AFFINITY : Controls the affinity of the memory region allocated by the MPI_Alloc_mem() or MPI_Win_allocate() operations. On systems that do not offer High Bandwidth Memory capabilities, (such as Intel Haswell or Broadwell processors), this env. variable is ignored and memory affinity is set to DDR. On Phi processors, such as KNL (and KNH, KNP in the future), this env. variable allows users to specifically request the memory returned by MPI_Alloc_mem() and MPI_Win_allocate() to be bound to either DDR, or the MCDRAM. Users can request a specific page size or memory binding policy via the MPICH_ALLOC_MEM_POLICY and MPICH_ALLOC_MEM_PG_SZ env. variables. Default: SYS_DEFAULT
20220317 014102.899 INFO             PET4 index=  87                             MPIR_CVAR_INTERNAL_MEM_AFFINITY : Controls the affinity of internal memory regions allocated by the MPI library. This variable currently affects the memory affinity of the mail-boxes used for off-node communication, and the shared-memory regions that are used for on-node pt2pt and collective ops. On systems that do not offer High Bandwidth Memory capabilities, (such as Intel Haswell or Broadwell processors), this env. variable is ignored and memory affinity is set to DDR. On Phi processors, such as KNL (and KNH, KNP in the future), this env. variable allows users to specifically request the internal memory regions used by the MPI library to be bound to either DDR, or the MCDRAM. The default affinity settings will be governed by the system defaults. For example, on a KNL system configured in the Quad/Flat mode, if the job is run with numactl --membind=1, all of MPI's internal memory will be bound to MCDRAM if this variable is not set. Default: SYS_DEFAULT
20220317 014102.899 INFO             PET4 index=  88                                  MPIR_CVAR_ALLOC_MEM_POLICY : Controls the memory affinity policy on systems with specialized memory hardware. By default, the memory policy is set to "{P}referred". Other accepted values are "{M}andatory" and "{I}"nterleave. Default: Preferred
20220317 014102.899 INFO             PET4 index=  89                                   MPIR_CVAR_ALLOC_MEM_PG_SZ : Controls the page size for the MPI_Alloc_mem() and MPI_Win_allocate() operations. This parameters defaults to 4KB base pages. The supported values are 2M, 4M, 8M, 16M, 32M, 64M, 128M, 256M, 512M, 1G and 2G. Default: 4096
20220317 014102.899 INFO             PET4 index=  90                                   MPIR_CVAR_OPT_THREAD_SYNC : Controls the mechanism used to implement thread-synchronization inside the Cray MPICH library. If set to 1, an optimized synchronization implementation is used. If set to 0, Cray MPICH falls back to using a pthread_mutex-based thread-synchronization implementation. Default: 1
20220317 014102.899 INFO             PET4 index=  91                                 MPIR_CVAR_THREAD_YIELD_FREQ : Determines how often a thread yields while waiting to acquire a lock in the new Cray optimized locking impl. This variable has no effect if MPICH_CRAY_OPT_THREAD_SYNC is 0. Default: 10000
20220317 014102.899 INFO             PET4 index=  92                                   MPIR_CVAR_MEM_DEBUG_FNAME : If set, the MPI library creates new files with the user specified name and writes various important memory statistics into these files. This information can be useful for post-processing. Users can set MPICH_MEM_DEBUG_FNAME to any suitable string. The resulting files are named as "string".<pid>.<MPI-rank>. For example, if MPICH_MEM_DEBUG_FNAME is set to "MEM_DBG_MSGS" the debug file for rank0 will be named "MEM_DBG_MSGS.<pid>.0 and written to the user's current working directory. If this flag is not set, the MPI library will redirect all the debug messages to stderr. Default: unset (disabled)
20220317 014102.899 INFO             PET4 index=  93                                   MPIR_CVAR_MALLOC_FALLBACK : Set the policy for fallback behavior when attempting to allocate large pages for internal buffers and insufficient large pages are available to satisfy the request. If set to enabled, MPICH falls back to using malloc in such cases. Default: not enabled (process fails and job terminates if insufficient large pages are available to satisfy the request)
20220317 014102.899 INFO             PET4 index=  94                              MPIR_CVAR_NO_DIRECT_GPU_ACCESS : If true, IPC and NIC-GPU Direct Access capabilities are not used for GPU-to-GPU transfers. This is mainly used for debugging.  Default: 0
20220317 014102.899 INFO             PET4 index=  95                                      MPIR_CVAR_G2G_PIPELINE : If nonzero, the device-host and network transfers will be overlapped to pipeline GPU-to-GPU transfers. Setting MPICH_G2G_PIPELINE to N will allow N GPU-to-GPU messages to be efficiently in-flight at any one time. If MPICH_G2G_PIPELINE is nonzero but MPICH_GPU_SUPPORT_ENABLED is disabled, MPICH_G2G_PIPELINE will be turned off. If MPICH_GPU_SUPPORT_ENABLED is enabled but MPICH_G2G_PIPELINE is 0, the default value is set to 8.  The pipeline-based implementation is being carried over from  CH3/Aries implementation. We think the pipelined-based implementation will not be necessary on SS-11 systems.  This logic may get dropped in the future.  Default: not set
20220317 014102.899 INFO             PET4 index=  96                               MPIR_CVAR_GPU_SUPPORT_ENABLED : If set, allows the MPI application to pass GPU pointers directly to MPI communication functions.  Default: not set
20220317 014102.899 INFO             PET4 index=  97                MPIR_CVAR_GPU_MANAGED_MEMORY_SUPPORT_ENABLED : If set, allows the MPI application to pass pointers allocated via  GPU managed memory allocators to MPI communication functions. Default: not set
20220317 014102.899 INFO             PET4 index=  98                         MPIR_CVAR_GPU_COLL_STAGING_AREA_OPT : If set, allows Cray MPI to use pre-registered CPU-attached memory to optimize collective ops.  Default: set
20220317 014102.899 INFO             PET4 index=  99               MPIR_CVAR_GPU_COLL_REGISTER_SHARED_MEM_REGION : If set, allows Cray MPI to registere CPU-attached shared memory regions  used to optimize collective ops.  Default: set
20220317 014102.899 INFO             PET4 index= 100                                   MPIR_CVAR_GPU_IPC_ENABLED : If set to 1, Cray MPICH will use GPU IPC to move data between GPU devices that are within the same node. If set to 0, Cray MPICH will use staging buffers on the host to implement data transfer between GPU devices that are within the same node. If MPIR_CVAR_GPU_SUPPORT_ENABLED is set, Cray MPICH  automatically attempts to enable the IPC optimizations.   If MPIR_CVAR_NO_DIRECT_GPU_ACCESS is set, IPC optimizations are disabled.  Default: -1
20220317 014102.899 INFO             PET4 index= 101                                 MPIR_CVAR_GPU_IPC_THRESHOLD : Specifies the minimum message size in bytes to consider for single-copy transfers between GPU devices that are within the same compute node. The value is interpreted as bytes, unless the string ends in a K, which indicates kilobytes, or M, which indicates megabytes.
20220317 014102.899 INFO             PET4 index= 102                                  MPIR_CVAR_GPU_IPC_PROTOCOL : Specifies the protocol used for large msg on-node, inter-GPU  data transfers via GPU IPC.  Valid options are RPUT or RGET.  Default: RGET
20220317 014102.899 INFO             PET4 index= 103                                 MPIR_CVAR_GPU_NO_ASYNC_COPY : If set, disables the use of asynchronous memcpy's for on-node GPU-aware transfers. Default: 0
20220317 014102.899 INFO             PET4 index= 104                                      MPIR_CVAR_ENABLE_YAKSA : If set, enables the use of the YAKSA datatype engine for packing and unpacking non-contiguous buffers in both CPU- and GPU-attached memory. Default: 0
20220317 014102.899 INFO             PET4 index= 105                              MPIR_CVAR_GPU_EAGER_DEVICE_MEM : If set to 1, GPU-attached memory will be used for the eager staging buffers for short on-node messages where at least the sending buffer is GPU-attached. If set to 0, this optimization is disabled and we fall-back to using staging buffers on the CPU for small message  intra-node device-host and device-device MPI ops. Default: 1
20220317 014102.899 INFO             PET4 index= 106                       MPIR_CVAR_GPU_EAGER_REGISTER_HOST_MEM : If set to 1, enables MPI to request POSIX  eager memory to be registered with the GPU runtime.  This is an optimization to improve the performance of  small message intra-node, inter-GPU MPI ops that use  POSIX shared memory as bounce buffers.  If set to 0, this optimization is disabled and we rely on GTL to lock/unlock host memory regions for a given on-node, small msg inter-GPU transfer.  Default: 1
20220317 014102.899 INFO             PET4 index= 107                          MPIR_CVAR_GPU_ALLREDUCE_USE_KERNEL : If set, adds a hint that the use of device kernels for reduction operations is desired. MPI is not guaranteed to use a device kernel for all reduction operations. Default: 0
20220317 014102.899 INFO             PET4 index= 108                                   MPIR_CVAR_RMA_MAX_PENDING : Determines how many RMA operation may be outstanding at any time over libfabrics. RMA operations beyond this max will be queued and only issued as pending operations complete. Default: 64
20220317 014102.899 INFO             PET4 index= 109                                MPIR_CVAR_RMA_SHM_ACCUMULATE : If set to 1, enables SHM accumulate operations. If set to 0, disables SHM accumulate operations. It also sets the default for the window hint "disable_shm_accumulate".  Default: 1
20220317 014102.899 INFO             PET4 --- VMK::logSystem() end ---------------------------------
20220317 014102.899 INFO             PET4 main: --- VMK::log() start -------------------------------------
20220317 014102.899 INFO             PET4 main: vm located at: 0x1606ae0
20220317 014102.899 INFO             PET4 main: petCount=6 localPet=4 mypthid=22853871870080 currentSsiPe=4
20220317 014102.899 INFO             PET4 main: Current system level affinity pinning for local PET:
20220317 014102.899 INFO             PET4 main:  SSIPE=4
20220317 014102.899 INFO             PET4 main: Current system level OMP_NUM_THREADS setting for local PET: 128
20220317 014102.899 INFO             PET4 main: ssiCount=1 localSsi=0
20220317 014102.899 INFO             PET4 main: mpionly=1 threadsflag=0
20220317 014102.899 INFO             PET4 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220317 014102.899 INFO             PET4 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220317 014102.899 INFO             PET4 main:  PE=0 SSI=0 SSIPE=0
20220317 014102.899 INFO             PET4 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220317 014102.899 INFO             PET4 main:  PE=1 SSI=0 SSIPE=1
20220317 014102.899 INFO             PET4 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220317 014102.899 INFO             PET4 main:  PE=2 SSI=0 SSIPE=2
20220317 014102.899 INFO             PET4 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220317 014102.899 INFO             PET4 main:  PE=3 SSI=0 SSIPE=3
20220317 014102.899 INFO             PET4 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220317 014102.899 INFO             PET4 main:  PE=4 SSI=0 SSIPE=4
20220317 014102.899 INFO             PET4 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220317 014102.899 INFO             PET4 main:  PE=5 SSI=0 SSIPE=5
20220317 014102.899 INFO             PET4 main: --- VMK::log() end ---------------------------------------
20220317 014102.899 INFO             PET4 Executing 'userm1_setvm'
20220317 014102.900 INFO             PET4 Executing 'userm1_register'
20220317 014102.900 INFO             PET4 Executing 'userm2_setvm'
20220317 014102.900 DEBUG            PET4 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220317 014102.900 DEBUG            PET4 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220317 014102.901 INFO             PET4 Entering 'user1_run'
20220317 014102.901 INFO             PET4 model1: --- VMK::log() start -------------------------------------
20220317 014102.901 INFO             PET4 model1: vm located at: 0x16622b0
20220317 014102.901 INFO             PET4 model1: petCount=6 localPet=4 mypthid=22853871870080 currentSsiPe=4
20220317 014102.901 INFO             PET4 model1: Current system level affinity pinning for local PET:
20220317 014102.901 INFO             PET4 model1:  SSIPE=4
20220317 014102.901 INFO             PET4 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220317 014102.901 INFO             PET4 model1: ssiCount=1 localSsi=0
20220317 014102.901 INFO             PET4 model1: mpionly=1 threadsflag=0
20220317 014102.901 INFO             PET4 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220317 014102.901 INFO             PET4 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220317 014102.901 INFO             PET4 model1:  PE=0 SSI=0 SSIPE=0
20220317 014102.901 INFO             PET4 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220317 014102.901 INFO             PET4 model1:  PE=1 SSI=0 SSIPE=1
20220317 014102.901 INFO             PET4 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220317 014102.901 INFO             PET4 model1:  PE=2 SSI=0 SSIPE=2
20220317 014102.901 INFO             PET4 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220317 014102.901 INFO             PET4 model1:  PE=3 SSI=0 SSIPE=3
20220317 014102.901 INFO             PET4 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220317 014102.901 INFO             PET4 model1:  PE=4 SSI=0 SSIPE=4
20220317 014102.901 INFO             PET4 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220317 014102.901 INFO             PET4 model1:  PE=5 SSI=0 SSIPE=5
20220317 014102.901 INFO             PET4 model1: --- VMK::log() end ---------------------------------------
20220317 014102.901 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220317 014103.270 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220317 014103.577 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220317 014103.884 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220317 014104.191 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220317 014104.497 INFO             PET4 Exiting 'user1_run'
20220317 014106.143 INFO             PET4 Entering 'user1_run'
20220317 014106.143 INFO             PET4 model1: --- VMK::log() start -------------------------------------
20220317 014106.143 INFO             PET4 model1: vm located at: 0x16622b0
20220317 014106.143 INFO             PET4 model1: petCount=6 localPet=4 mypthid=22853871870080 currentSsiPe=4
20220317 014106.143 INFO             PET4 model1: Current system level affinity pinning for local PET:
20220317 014106.143 INFO             PET4 model1:  SSIPE=4
20220317 014106.144 INFO             PET4 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220317 014106.144 INFO             PET4 model1: ssiCount=1 localSsi=0
20220317 014106.144 INFO             PET4 model1: mpionly=1 threadsflag=0
20220317 014106.144 INFO             PET4 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220317 014106.144 INFO             PET4 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220317 014106.144 INFO             PET4 model1:  PE=0 SSI=0 SSIPE=0
20220317 014106.144 INFO             PET4 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220317 014106.144 INFO             PET4 model1:  PE=1 SSI=0 SSIPE=1
20220317 014106.144 INFO             PET4 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220317 014106.144 INFO             PET4 model1:  PE=2 SSI=0 SSIPE=2
20220317 014106.144 INFO             PET4 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220317 014106.144 INFO             PET4 model1:  PE=3 SSI=0 SSIPE=3
20220317 014106.144 INFO             PET4 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220317 014106.144 INFO             PET4 model1:  PE=4 SSI=0 SSIPE=4
20220317 014106.144 INFO             PET4 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220317 014106.144 INFO             PET4 model1:  PE=5 SSI=0 SSIPE=5
20220317 014106.144 INFO             PET4 model1: --- VMK::log() end ---------------------------------------
20220317 014106.144 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220317 014106.450 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220317 014106.757 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220317 014107.064 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220317 014107.371 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220317 014107.678 INFO             PET4 Exiting 'user1_run'
20220317 014109.291 INFO             PET4  NUMBER_OF_PROCESSORS           6
20220317 014109.291 INFO             PET4  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220317 014109.291 INFO             PET4 Finalizing ESMF
20220317 014102.898 INFO             PET5 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220317 014102.898 INFO             PET5 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220317 014102.898 INFO             PET5 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220317 014102.898 INFO             PET5 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220317 014102.898 INFO             PET5 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220317 014102.898 INFO             PET5 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220317 014102.898 INFO             PET5 Running with ESMF Version   : v8.3.0b09-100-g5ff85a963f
20220317 014102.898 INFO             PET5 ESMF library build date/time: "Mar 17 2022" "01:27:56"
20220317 014102.898 INFO             PET5 ESMF library build location : /lfs/h1/emc/ptmp/Mark.Potts/gfortran_10.3.0_mpi_O_jedwards_pio_update2
20220317 014102.898 INFO             PET5 ESMF_COMM                   : mpi
20220317 014102.898 INFO             PET5 ESMF_MOAB                   : enabled
20220317 014102.898 INFO             PET5 ESMF_LAPACK                 : enabled
20220317 014102.898 INFO             PET5 ESMF_NETCDF                 : enabled
20220317 014102.898 INFO             PET5 ESMF_PNETCDF                : disabled
20220317 014102.898 INFO             PET5 ESMF_PIO                    : enabled
20220317 014102.898 INFO             PET5 ESMF_YAMLCPP                : enabled
20220317 014102.898 INFO             PET5 --- VMK::logSystem() start -------------------------------
20220317 014102.898 INFO             PET5 esmfComm=mpi
20220317 014102.898 INFO             PET5 isPthreadsEnabled=1
20220317 014102.898 INFO             PET5 isOpenMPEnabled=1
20220317 014102.898 INFO             PET5 isOpenACCEnabled=0
20220317 014102.898 INFO             PET5 isSsiSharedMemoryEnabled=1
20220317 014102.898 INFO             PET5 ssiCount=1 peCount=6
20220317 014102.898 INFO             PET5 PE=0 SSI=0 SSIPE=0
20220317 014102.898 INFO             PET5 PE=1 SSI=0 SSIPE=1
20220317 014102.898 INFO             PET5 PE=2 SSI=0 SSIPE=2
20220317 014102.898 INFO             PET5 PE=3 SSI=0 SSIPE=3
20220317 014102.898 INFO             PET5 PE=4 SSI=0 SSIPE=4
20220317 014102.898 INFO             PET5 PE=5 SSI=0 SSIPE=5
20220317 014102.898 INFO             PET5 --- VMK::logSystem() MPI Control Variables ---------------
20220317 014102.898 INFO             PET5 index=   0          MPIR_CVAR_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE : specifies the cutoff size of the send buffer (in bytes) above which the reduce_scatter functions attempt to use the pairwise exchange algorithm.  In addition, the op must be commutative and the communicator size < MPIR_CVAR_REDUCE_SCATTER_MAX_COMMSIZE for the pairwise exchange algorithm to be used.
20220317 014102.898 INFO             PET5 index=   1                       MPIR_CVAR_REDUCE_SCATTER_MAX_COMMSIZE : specifies the max communicator size that will trigger use of the pairwise exchange algorithm, provided the op is commutative.  The pairwise exchange algorithm is not well suited for scaling to high process counts, so for larger communicators, a recursive halving algorithm is used instead.
20220317 014102.898 INFO             PET5 index=   2                           MPIR_CVAR_NETWORK_BUFFER_COLL_OPT : If set to 1, the MPICH library will use the optimized shared- memory based "network buffer" design for collective operations.  This feature is closely tied to the shared-memory collective optimization available in Cray MPICH. If enabled, the shared-memory buffer is also registered with the NIC and can be used directly to  perform off-node transfers, bypassing the Nemesis channel layer.  This feature is disabled if MPICH_SHARED_MEM_COLL_OPT  is set to 0. Currently, this optimization is only available  for the MPI_Bcast collective operation. To disable this feature,  set MPICH_NETWORK_BUFFER_COLL_OPT to 0.  Default: 0
20220317 014102.898 INFO             PET5 index=   3                                MPIR_CVAR_ALLTOALL_SYNC_FREQ : Adjusts the number of outstanding messages each Alltoall process  will allow.  Default is variable.
20220317 014102.898 INFO             PET5 index=   4                                 MPIR_CVAR_ALLTOALL_BLK_SIZE : The transfer size in bytes for the Alltoall chunking algorithm.  Default is 16384.
20220317 014102.898 INFO             PET5 index=   5                       MPIR_CVAR_ALLTOALL_CHUNKING_MAX_NODES : The maximum number of nodes to use the alltoall chunking algorithm. Above this value, the throttled algorithm will be used.  This is only applicable to SS-11.
20220317 014102.898 INFO             PET5 index=   6                              MPIR_CVAR_ALLGATHER_VSHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized gather/bcast algorithm is used instead of the optimized ring algorithm for MPI_Allgather. The gather/bcast algorithm is better suited for small messages. Defaults: For communicator sizes of <=512 ranks, 1024 bytes. For communicator sizes of >512 ranks, 4096 bytes.
20220317 014102.898 INFO             PET5 index=   7                             MPIR_CVAR_ALLGATHERV_VSHORT_MSG : Adjusts the cutoff point at and below which the architecture-specific optimized gatherv/bcast algorithm is used instead of the optimized ring algorithm for MPI_Allgatherv. The gatherv/bcast algorithm is better suited for small messages. Defaults: For communicator sizes of <=512 ranks, 1024 bytes. For communicator sizes of >512 ranks, 4096 bytes.
20220317 014102.898 INFO             PET5 index=   8                                  MPIR_CVAR_ALLREDUCE_NO_SMP : If set, MPI_Allreduce uses an algorithm that is not smp- aware. This provides a consistent ordering of the specified allreduce operation regardless of system configuration. Note:  This algorithm may not perform as well as the default smp-aware algorithms as it does not take advantage of rank topology. Default: not set
20220317 014102.898 INFO             PET5 index=   9                                MPIR_CVAR_ALLTOALL_SHORT_MSG : Adjusts the cut-off points at and below which the store and forward Alltoall algorithm is used for short messages. The default value is dependent upon the total number of ranks in the MPI communicator used for the MPI_Alltoall call and the Alltoall algorithm being selected. Defaults: If using one of the non-default send/recv algorithms on Aries, the defaults are: if communicator size <= 512, 2048 bytes if communicator size > 512 and <= 1024, 1024 bytes if communicator size > 1024 and <= 65536, 128 bytes if communicator size > 65536 and <= 131072, 64 bytes if communicator size > 131072 , 32 bytes
20220317 014102.898 INFO             PET5 index=  10                                MPIR_CVAR_ALLTOALLV_THROTTLE : Sets the per-process maximum number of outstanding Isends and Irecvs that can be posted concurrently for the optimized send/recv MPI_Alltoallv algorithm. For large messages, consider decreasing the throttle to 1 or 2 to improve performance. Defaults: 8
20220317 014102.898 INFO             PET5 index=  11                                   MPIR_CVAR_BCAST_ONLY_TREE : If set to 1, MPI_Bcast uses an smp-aware tree algorithm regardless of data size. The tree algorithm generally scales well to high processor counts on Cray XE systems. If set to 0, MPI_Bcast uses a variety of algorithms (tree, scatter, or ring) depending on message size and other factors. These other algorithms generally do not scale well when using more than 512 processors on Cray XE systems. Default: 1
20220317 014102.898 INFO             PET5 index=  12                             MPIR_CVAR_BCAST_INTERNODE_RADIX : Used to set the radix of the inter-node tree. This can be set to any integer value greater than or equal to 2. Default: 4
20220317 014102.898 INFO             PET5 index=  13                             MPIR_CVAR_BCAST_INTRANODE_RADIX : Used to set the radix of the intra-node tree. This can be set to any integer value greater than or equal to 2. Default: 4
20220317 014102.898 INFO             PET5 index=  14                                      MPIR_CVAR_COLL_OPT_OFF : If set, disables collective optimizations which use nondefault, architecture-specific algorithms for some MPI collective operations. By default, all collective optimized algorithms are enabled. To disable all collective optimized algorithms, set MPICH_COLL_OPT_OFF to 1. To disable optimized algorithms for selected MPI collectives, set the value to a comma-separated list of the desired collective names. Names are not case-sensitive. Any unrecognizable name is flagged with a warning message and ignored. For example, to disable the MPI_Allgather optimized collective algorithm, set MPICH_COLL_OPT_OFF=mpi_allgather. The following collective names are recognized: MPI_Allgather, MPI_Allgatherv, MPI_Allreduce, MPI_Alltoall, MPI_Alltoallv, MPI_Bcast, MPI_Gatherv, MPI_Scatterv, MPI_Igatherv, and MPI_Iallreduce. Default: Not enabled.
20220317 014102.898 INFO             PET5 index=  15                                         MPIR_CVAR_COLL_SYNC : If set, a Barrier is performed at the beginning of each specified MPI collective function. This forces all processes participating in that collective to sync up before the collective can begin. To disable this feature for all MPI collectives, set the value to 0. This is the default. To enable this feature for all MPI collectives, set the value to 1. To enable this feature for selected MPI collectives, set the value to a comma-separated list of the desired collective names. Names are not case-sensitive. Any unrecognizable name is flagged with a warning message and ignored. The following collective names are recognized: MPI_Allgather, MPI_Allgatherv, MPI_Allreduce, MPI_Alltoall, MPI_Alltoallv, MPI_Alltoallw, MPI_Bcast, MPI_Exscan, MPI_Gather, MPI_Gatherv, MPI_Reduce, MPI_Reduce_scatter, MPI_Scan, MPI_Scatter, and MPI_Scatterv. Default: Not enabled.
20220317 014102.898 INFO             PET5 index=  16                                 MPIR_CVAR_GATHERV_SHORT_MSG : Adjusts the cutoff point at and below which the optimized tree MPI_Gatherv algorithm is used instead of the optimized permission-to-send MPI_Gatherv algorithm. The cutoff is based on the average size of the variable MPI_Gatherv message sizes.  Default: 131072 bytes
20220317 014102.898 INFO             PET5 index=  17                                     MPIR_CVAR_REDUCE_NO_SMP : If set, MPI_Reduce uses an algorithm that is not smp-aware. This provides a consistent ordering of the specified reduce operation regardless of system configuration. Note:  This algorithm may not perform as well as the default smp-aware algorithms as it does not take advantage of rank topology. Default: not set
20220317 014102.898 INFO             PET5 index=  18                              MPIR_CVAR_SCATTERV_SYNCHRONOUS : The default, non-optimized ANL MPI_Scatterv algorithm uses asynchronous sends by default for communicator sizes less than 200,000 ranks. If set, this environment variable causes MPI_Scatterv to switch to using blocking sends, which may be beneficial in certain cases involving large data sizes or high process counts. For communicator sizes equal to or greater than 200,000 ranks, the blocking send algorithm is used by default. Default: not enabled
20220317 014102.898 INFO             PET5 index=  19                               MPIR_CVAR_SHARED_MEM_COLL_OPT : If set, the MPICH library will use the optimized shared- memory based design for collective operations. On Gemini and Aries systems, the supported collective operations are: MPI_Allreduce, MPI_Iallreduce, MPI_Barrier, and MPI_Bcast. To enable all available shared-memory optimizations, set MPICH_SHARED_MEM_COLL_OPT to 1. To enable this feature for a specific set of collective operations, set MPICH_SHARED_MEM_COLL_OPT to a comma- separated list of collective names. For example, to enable this optimization for MPI_Bcast only, set MPICH_SHARED_MEM_COLL_OPT=MPI_Bcast. To enable this optimization for MPI_Allreduce only, set MPICH_SHARED_MEM_COLL_OPT=MPI_Allreduce. Unsupported names are flagged with a warning message and ignored. Default: set
20220317 014102.898 INFO             PET5 index=  20                   MPIR_CVAR_ALLREDUCE_GPU_STAGING_THRESHOLD : If the sendbuf/recvbuf of an Allreduce operation is on GPU-resident memory regions, for payload sizes larger than this variable,  the Cray optimized staging implementation will be used.  By default, the staging implementation is used for all payload sizes.  This variable has no effect if MPICH_GPU_COLL_STAGING_AREA_OPT is set to 0. Default: 0
20220317 014102.898 INFO             PET5 index=  21                         MPIR_CVAR_GPU_COLL_STAGING_BUF_SIZE : This variable determines the size of the staging buffers used for some collectives (such as MPI_Allreduce) when sendbuf/recvbuf on GPU-resident buffers. This variable has no effect if  MPICH_GPU_COLL_STAGING_AREA_OPT is set to 0.   Default: 262144
20220317 014102.898 INFO             PET5 index=  22                                 MPIR_CVAR_GATHERV_SYNC_FREQ : Only applicable to the Gatherv permission-to-send algorithm.  Adjusts  the number of outstanding receives the root for Gatherv will allow.   Default is 16.
20220317 014102.898 INFO             PET5 index=  23                              MPIR_CVAR_GATHERV_MAX_TMP_SIZE : Only applicable to the Gatherv tree algorithm.  Sets the maximum amount of temporary memory GAtherv will allow a rank to allocate when using the tree-based algorithm.  Each rank allocates a different amount, with many allocating no extra memory.  If any rank requires more than this amount of temporary buffer space, a different algorithm is used. Default is 512MB.
20220317 014102.898 INFO             PET5 index=  24                             MPIR_CVAR_GATHERV_MIN_COMM_SIZE : Cray MPI offers two optimized Gatherv algorithms.  A tree algorithm for small messages and a permission-to-send send algorithm for larger messages.  Set  this value to the minimum communicator size to attempt use of either of the  Cray optimized Gatherv algorithms. Default is 64
20220317 014102.898 INFO             PET5 index=  25                                MPIR_CVAR_SCATTERV_SHORT_MSG : Adjusts the cutoff point at and below which the optimized tree MPI_Scatterv algorithm is used instead of the optimized staggered send algorithm.  The cutoff is in bytes, based on the average size of the variable MPI_Scatterv message sizes. Default behavior if unset is: For communicator sizes of <= 512 ranks, 2048 bytes For communicator sizes of > 512 ranks, 8192 bytes
20220317 014102.898 INFO             PET5 index=  26                                MPIR_CVAR_SCATTERV_SYNC_FREQ : Only applicable to the Scatterv staggered send algorithm.  Adjusts the number of outstanding sends the root for Scatterv will use. Default is 16.
20220317 014102.898 INFO             PET5 index=  27                             MPIR_CVAR_SCATTERV_MAX_TMP_SIZE : Only applicable to the Scatterv tree algorithm.  Sets the maximum amount of temporary memory Scatterv will allow a rank to allocate when using the tree-based algorithm.  Each rank allocates a different amount, with many allocating no extra memory.  If any rank requires more than this amount of temporary buffer space, a different algorithm is used. Default is 512MB.
20220317 014102.898 INFO             PET5 index=  28                            MPIR_CVAR_SCATTERV_MIN_COMM_SIZE : Cray MPI offers two optimized Scatterv algorithms.  A tree algorithm for small messages and a staggered send algorithm for larger messages.  Set this value to the minimum communicator size to attempt use of either of the Cray optimized Scatterv algorithms. Default is 64
20220317 014102.898 INFO             PET5 index=  29                           MPIR_CVAR_MPIIO_ABORT_ON_RW_ERROR : If set to enable, causes MPI-IO to abort immediately after issuing an error message if an I/O error occurs during a system read() or write() call. This applies only to I/O errors for system read() and write() calls made as a result of MPI I/O calls. It does not apply to I/O errors for other MPI I/O calls such as MPI_File_open(), nor does it apply to read() and write() calls made by means other than MPI I/O calls. Abort on error is not standard behavior. The MPI Standard specifies that the default error handling for MPI I/O calls is to return an error code to the application rather than aborting the application, but since errors on write or read are almost always unexpected and usually not recoverable, it may be preferable to abort as soon as the error is detected. Doing so does not allow any recovery, but does provide the most information about the error and terminates the job quickly. If the Cray Abnormal Termination Processing (ATP) feature is enabled, the abort will result in a full stack backtrace writte
20220317 014102.898 INFO             PET5 index=  30                MPIR_CVAR_MPIIO_AGGREGATOR_PLACEMENT_DISPLAY : This variable controls whether the placement of the aggregators will be displayed when a file is opened. The placement can be  controlled on a per file basis with the aggregator_placement_stride  hint. If set, displays the assignment of MPIIO collective buffering aggregators for reads/writes of a shared file, showing rank and node ID (nid). For example: Aggregator Placement for /lus/scratch/myfile RankReorderMethod=3  AggPlacementStride=-1 AGG    Rank       nid ----  ------  -------- 0       0  nid00578 1       4  nid00579 2       1  nid00606 3       5  nid00607 4       2  nid00578 5       6  nid00579 6       3  nid00606 7       7  nid00607 Default: not set
20220317 014102.898 INFO             PET5 index=  31                 MPIR_CVAR_MPIIO_AGGREGATOR_PLACEMENT_STRIDE : Partially controls to which nodes MPIIO collective buffering aggregators are assigned. See the notes below on the order of nodes. Network traffic and resulting I/O performance may be affected by the assignments. If set to 1, consecutive nodes are used. The number of aggregators assigned per node is controlled by the cb_config_list hint. By default, no more than one aggregator per node will be assigned if there are at least as many nodes as aggregators. If set to a value greater than 1, node selection is strided across the available nodes by this value. If the stride times the number of aggregators exceeds the number of nodes, the assignments will wrap around, which is usually not optimal for performance. If set to -1, node selection is strided across available nodes by the value of the number of nodes divided by the number of aggregators (integer division, minimum value of 1). The purpose is to spread out the nodes to reduce network congestion. Note:  The order of nodes can be shown by setting the MPICH_RANK
20220317 014102.898 INFO             PET5 index=  32                                    MPIR_CVAR_MPIIO_CB_ALIGN : Sets the default value for the cb_align hint. Files opened with MPI_File_open wil have this value for the cb_align hint unless the hint is set on a per file basis with either the MPICH_MPIIO_HINTS environment variable or from within a program with the MPI_Info_set() call. Note:  Only MPICH_MPIIO_CB_ALIGN == 2 is fully supported. Other values are for internal testing only. Default: 2
20220317 014102.898 INFO             PET5 index=  33                                MPIR_CVAR_MPIIO_DVS_MAXNODES : Note:  This environment variable in relevant only for file systems accessed from Cray system compute nodes via DVS server nodes; e.g. GPFS or PANFS. As described in the dvs(5) man page, the environment variable DVS_MAXNODES can be used to set the stripe width— that is, the number of DVS server nodes—used to access a file in "stripe parallel mode." For most files, and especially for small files, setting DVS_MAXNODES to 1 ("cluster parallel mode") is preferred. The MPICH_MPIIO_DVS_MAXNODES environment variable enables you to leave DVS_MAXNODES set to 1 and then use MPICH_MPIIO_DVS_MAXNODES to temporarily override DVS_MAXNODES when it is advantageous to specify wider striping for files being opened by the MPI_File_open() call. The range of values accepted by MPICH_MPIIO_DVS_MAXNODES goes from 1 to the number of server nodes specified on the mount with the nnodes mount option. DVS_MAXNODES is not set by default. Therefore, for MPICH_MPIIO_DVS_MAXNODES to have any effect, DVS_MAXNODES must be defined before p
20220317 014102.898 INFO             PET5 index=  34                                       MPIR_CVAR_MPIIO_HINTS : If set, override the default value of one or more MPI I/O hints. This also overrides any values that were set by using calls to MPI_Info_set in the application code. The new values apply to the file the next time it is opened using an MPI_File_open() call. After the MPI_File_open() call, subsequent MPI_Info_set calls can be used to pass new MPI I/O hints that take precedence over some of the environment variable values. Other MPI I/O hints such as striping_factor, striping_unit, cb_nodes, and cb_config_list cannot be changed after the MPI_File_open() call, as these are evaluated and applied only during the file open process. An MPI_File_close call followed by an MPI_File_open call can be used to restart the MPI I/O hint evaluation process. The syntax for this environment variable is a comma- separated list of specifications. Each individual specification is a pathname_pattern followed by a colon- separated list of one or more key=value pairs. In each key=value pair, the key is the MPI-IO hint name, and the v
20220317 014102.898 INFO             PET5 index=  35                               MPIR_CVAR_MPIIO_HINTS_DISPLAY : If set, causes rank 0 in the participating communicator to display the names and values of all MPI-IO hints that are set for the file being opened with the MPI_File_open call. It also displays relevant environment variables whether or not MPICH_ENV_DISPLAY is set. Default: not enabled.
20220317 014102.898 INFO             PET5 index=  36                               MPIR_CVAR_MPIIO_MAX_NUM_IRECV : When MPIIO collective buffering is used, this environment variable controls the number of outstanding MPI_Irecv calls allowed before an MPI_Waitall is done. Default: 50
20220317 014102.898 INFO             PET5 index=  37                               MPIR_CVAR_MPIIO_MAX_NUM_ISEND : When MPIIO collective buffering is used, this environment variable controls the number of outstanding MPI_Isend calls allowed before an MPI_Waitall is done. Default: 50
20220317 014102.898 INFO             PET5 index=  38                              MPIR_CVAR_MPIIO_MAX_SIZE_ISEND : When MPIIO collective buffering is used, this environment variable limits MPI_Isend by the amount of data being sent rather than by the number of calls. Default: 10485760 bytes
20220317 014102.898 INFO             PET5 index=  39                                       MPIR_CVAR_MPIIO_STATS : If set to 1, a summary of file write and read access patterns is written by rank 0 to stderr. This information provides some insight into how I/O performance may be improved. The information is provided on a per-file basis and is written when the file is closed. It does not provide any timing information. If set to 2, a set of data files are written to the working directory, one file for each rank, with the filename prefix specified by the MPICH_MPIIO_STATS_FILE environment variable. The data is in comma-separated values (CSV) format, which can be summarized with the cray_mpiio_summary script in the /opt/cray/mpt/version/gni/bin directory. Additional example scripts are provided in that directory to further process and display the data. Only enabled if MPIR_CVAR_MPIIO_CB_ALIGN=2. Default: not set
20220317 014102.898 INFO             PET5 index=  40                                  MPIR_CVAR_MPIIO_STATS_FILE : Specifies the filename prefix for the set of data files written when MPICH_MPIIO_STATS is set to 2. The filename prefix may be a full absolute pathname or a relative pathname. Summary plots of these files can be generated using the cray_mpiio_summary script from the /opt/cray/mpt/version/gni/bin directory. Other example scripts for post-processing this data can also be found in /opt/cray/mpt/version/gni/bin. Default: _cray_mpiio_stats_
20220317 014102.898 INFO             PET5 index=  41                         MPIR_CVAR_MPIIO_STATS_INTERVAL_MSEC : Specifies the time interval in milliseconds for each MPICH_MPIIO_STATS data point. Default: 250
20220317 014102.898 INFO             PET5 index=  42                                      MPIR_CVAR_MPIIO_TIMERS : If set to 0, or not set at all, no timing data is collected. If set to 1, timing data for different phases in MPI-IO is collected locally by each MPI process and then during MPI_File_close the data is consolidated and printed. Some timing data is displayed in seconds, other data is displayed in clock ticks, possibly scaled down. Also see MPICH_MPIIO_TIMERS_SCALE The relative values of the reported times are more important to the analysis than the absolute time. More detailed information about MPI-IO performance can be obtained by using the MPICH_MPIIO_STATS feature and by using the CrayPat and Apprentice2 Timeline Report of I/O bandwidth. Only enabled if MPIR_CVAR_MPIIO_CB_ALIGN=2. Default: 0
20220317 014102.898 INFO             PET5 index=  43                                MPIR_CVAR_MPIIO_TIMERS_SCALE : Specifies the power of 2 to use to scale the times reported by MPICH_MPIIO_TIMERS.  The raw times are collected in clock ticks. This generally is a very large number and reducing all the times by the same scaling factor makes for a more compact display. If set to 0, or not set at all, MPI-IO automatically determines a scaling factor to limit the report times to 9 or fewer digits. This auto-determined value is displayed.  To make run to run comparisons, you can set the scaling factor to your preferred value. Default: 0
20220317 014102.898 INFO             PET5 index=  44                                  MPIR_CVAR_MPIIO_TIME_WAITS : If set to non-zero, time how long this rank has to wait for other ranks to catch up.  This separates true metadata time from imbalance time. This is disabled when MPICH_MPIIO_TIMERS is not set.  Otherwise it defaults to 1. Default: 1
20220317 014102.898 INFO             PET5 index=  45                          MPIR_CVAR_MPIIO_WRITE_EXIT_BARRIER : If set to non-zero, collective write's will barrier on exit Default: 1
20220317 014102.898 INFO             PET5 index=  46                               MPIR_CVAR_MPIIO_DS_WRITE_CRAY : If set to non-zero, collective write's with data sieving will be optimized  Default: 1
20220317 014102.898 INFO             PET5 index=  47                         MPIR_CVAR_MPIIO_OFI_STARTUP_CONNECT : If set to enable, causes MPI-IO to establish connections between ranks and aggregators during the openning of the file to be used for MPIIO collective operations
20220317 014102.898 INFO             PET5 index=  48                MPIR_CVAR_MPIIO_OFI_STARTUP_NODES_AGGREGATOR : If MPIIO_OFI_STARTUP_CONNECT is enabled, this specifies the number of nodes that will concurrently attempt to connext to each aggrgator. Each rank on each node will establish its own connection to the aggregator. Default: 2
20220317 014102.898 INFO             PET5 index=  49                                           MPIR_CVAR_DPM_DIR : Sets the directory to use for MPI port name publishing in the  file-based nameserv implementation, as well as publishing the  credential obtained from libdrc.
20220317 014102.899 INFO             PET5 index=  50                               MPIR_CVAR_SINGLE_HOST_ENABLED : If true and if running as a single node, then inter-node communications is never initialized. The helps prevent the use of scarce hardware resources.
20220317 014102.899 INFO             PET5 index=  51                                       MPIR_CVAR_OFI_VERBOSE : If set to 1, enable verbose output about the CH4 OFI device. If set to 2 or higher, enable more verbose output.
20220317 014102.899 INFO             PET5 index=  52                                 MPIR_CVAR_OFI_XRCD_BASE_DIR : Specifies the base directory for the XRCD file.  This is only  used if the job is not launched via PALS launcher.  If launched  via PALS, we use the private, temporary PALS_SPOOL_DIR directory. Default: /tmp
20220317 014102.899 INFO             PET5 index=  53                                   MPIR_CVAR_OFI_NIC_VERBOSE : If set to 1 or higher, enables verbose output about the CH4 OFI NIC selection.   Non-fatal warnings are also displayed.  If set to 2 or higher, each rank in  the job will display it's NIC selection.  If set to 3 or higher, more verbose  output is enabled (for debug purposes).
20220317 014102.899 INFO             PET5 index=  54                                    MPIR_CVAR_OFI_NIC_POLICY : Specifies the rank-to-nic policy.  The supported options are:  NUMA | GPU | BLOCK | ROUND-ROBIN | USER
20220317 014102.899 INFO             PET5 index=  55                                   MPIR_CVAR_OFI_NIC_MAPPING : Specifies the precise rank-to-NIC mapping to use on each node.  This is  required if the NIC_POLICY is set to USER.  Each local rank must have a NIC mapping assigned by this env variable. If there are fewer MPI ranks on any  node, that part of the MAPPING string will be ignored.  For example: To assign local ranks 0,16,32,48 to NIC 0, and remaining ranks to NIC 1 MPICH_OFI_NIC_MAPPING="0:0,16,32,48; 1:1-15,17-31,33-47,49-63"
20220317 014102.899 INFO             PET5 index=  56                                      MPIR_CVAR_OFI_NUM_NICS : If set, specifies the number of NICs the job can use on each node. By default, when multiple NICs per node are available, MPI attempts to use them all.  If using fewer NICs is desired, this env variable  can be set to indicate the maximum number of NICs per node MPI will  use. By default, consecutive NICs are used, starting with index 0.   To limit use to 1 NIC/node, use : export MPICH_OFI_NUM_NICS=1 To limit use to 2 NICs/node, use: export MPICH_OFI_NUM_NICS=2 To specify alternative NIC index values, you may indicate the desired index values by adding a colon, followed by the NIC indexes.  Use  quotes so the shell doesn't interfere.  For example: To limit use to 1 NIC/node, index 1    : export MPICH_OFI_NUM_NICS="1:1" To limit use to 2 NICs/node, index 1&3 : export MPICH_OFI_NUM_NICS="2:1,3"
20220317 014102.899 INFO             PET5 index=  57                        MPIR_CVAR_OFI_SKIP_NIC_SYMMETRY_TEST : If set to 1, CrayMPICH will bypass it's check for NIC symmetry on all nodes in the job. By default, this check is run during MPI_Init to  make sure all the nodes in the job have the same number of NICs available.
20220317 014102.899 INFO             PET5 index=  58                               MPIR_CVAR_OFI_STARTUP_CONNECT : If set to 1, CrayMPICH will create COMM_WORLD static OFI connections to every other rank during MPI_Init.
20220317 014102.899 INFO             PET5 index=  59                           MPIR_CVAR_OFI_RMA_STARTUP_CONNECT : If set to 1, CrayMPICH RMA will create static OFI connections to every other rank on the same node during MPI_Init. This will also enable MPIR_CVAR_OFI_STARTUP_CONNECT.
20220317 014102.899 INFO             PET5 index=  60                                MPIR_CVAR_OFI_DEFAULT_TCLASS : If set, CrayMPICH will assign the requested default TC (traffic class) to the job domain.  By default, all endpoints inherit this TC.
20220317 014102.899 INFO             PET5 index=  61                                 MPIR_CVAR_OFI_TCLASS_ERRORS : Determines how CrayMPICH responds to traffic class errors.  Valid values are "warn", "silent" or "error".
20220317 014102.899 INFO             PET5 index=  62                                  MPIR_CVAR_OFI_CXI_PID_BASE : Set to the base value that MPI will use to assign portal IDs for the CXI provider.  Max PID value is 510.  MPI uses PID_BASE+lrank as the unique value per rank per node.  This is only applicable to SS11.
20220317 014102.899 INFO             PET5 index=  63                          MPIR_CVAR_OFI_USE_SCALABLE_STARTUP : Set to false (0) to bypass the scalable startup feature for CXI. This is only applicable to SS11.
20220317 014102.899 INFO             PET5 index=  64                                       MPIR_CVAR_UCX_VERBOSE : If set to 1, enable verbose output about the CH4 UCX device. If set to 2 or higher, enable more verbose output.
20220317 014102.899 INFO             PET5 index=  65                                  MPIR_CVAR_UCX_RC_MAX_RANKS : By default, CrayMPI selects either the UCX RC or UD protocols for inter-node messaging.  If a job is launched with MPICH_UCX_RC_MAX_RANKS  ranks or fewer, the RC protocol is selected.  If more ranks are launched,  the UCX UD protocol is chosen.  The RC protocol does not scale well due to the resource requirements.  Note this default can be overridden by setting the UCX_TLS environment variable.
20220317 014102.899 INFO             PET5 index=  66                              MPIR_CVAR_SMP_SINGLE_COPY_MODE : If non-null, choose an on-node copy mode for large messages. This variable can be set to XPMEM, CMA, or NONE. By default, Cray MPICH will look to use XPMEM. If XPMEM is requested, but not available, Cray MPICH will attempt to use CMA. (NOTE: Cray MPICH does not support CMA currently. The env. variables and CVARs are set up to allow CMA usage with future versions of Cray MPICH) If both XPMEM and CMA cannot be used, Cray MPICH will fall back to using the ANL shared memory implementation (2-copy) Default: NULL
20220317 014102.899 INFO             PET5 index=  67                              MPIR_CVAR_SMP_SINGLE_COPY_SIZE : Specifies the minimum message size in bytes to consider for single-copy transfers for on-node messages. This applies only to the SMP (on-node shared memory) device. The value is interpreted as bytes, unless the string ends in a K, which indicates kilobytes, or M, which indicates megabytes. Default: 8192
20220317 014102.899 INFO             PET5 index=  68                       MPIR_CVAR_SHM_PROGRESS_MAX_BATCH_SIZE : Adjusts the maximum number of on-node requests that can be processed in a single batch. Higher values for the maximum batch size can lower the overhead due to entering the progress engine, but can also  delay the processing of off-node message requests. Default is 8.
20220317 014102.899 INFO             PET5 index=  69                                MPIR_CVAR_CH4_RMA_THREAD_HOT : If true, the big lock is disabled for certain RMA operations
20220317 014102.899 INFO             PET5 index=  70                                    MPIR_CVAR_ABORT_ON_ERROR : If set, causes MPICH to abort and produce a core dump when MPICH detects an internal error. Note that the core dump size limit (usually 0 bytes by default) must be reset to an appropriate value in order to enable coredumps. Default: Not enabled.
20220317 014102.899 INFO             PET5 index=  71                                   MPIR_CVAR_CPUMASK_DISPLAY : If set, causes each MPI rank in the job to display its CPU affinity bitmask. Note that this reports only the CPU affinity masks for the MPI ranks; if you have a hybrid program, it does not provide any thread information. The bitmask is read from right to left, meaning the value in the rightmost position corresponds to CPU 0 on the node.
20220317 014102.899 INFO             PET5 index=  72                                       MPIR_CVAR_ENV_DISPLAY : If set, causes rank 0 to display all MPICH environment variables and their current settings at MPI initialization time. If two or more nodes are used, MPICH/GNI environment settings are also included in the listing. Default: Not enabled.
20220317 014102.899 INFO             PET5 index=  73                                  MPIR_CVAR_OPTIMIZED_MEMCPY : Specifies which version of memcpy to use. Valid values are: 0         Use the system (glibc) version of memcpy. 1         Use an optimized version of memcpy if one is available for the processor being used. In this release, an optimized version of memcpy() is available only for Intel processors. 2         Use a highly optimized version of memcpy that provides better performance in some areas but may have performance regressions in other areas, if one is available for the processor being used. In this release, a highly optimized version of memcpy() is available only for Intel Haswell processors. MPICH_OPTIMIZED_MEMCPY is overridden by MPICH_USE_SYSTEM_MEMCPY. If MPICH_USE_SYSTEM_MEMCPY is set, MPICH_OPTIMIZED_MEMCPY is ignored and the system (glibc) version of memcpy() is used. Default: 1
20220317 014102.899 INFO             PET5 index=  74                                     MPIR_CVAR_STATS_DISPLAY : If set to 1, a summary of MPI statistics, also available through the MPI Tools Interface, will be written by rank 0 to stderr. If set to 2, all ranks will produce an individualized statistics summary and write to file on a per-rank basis. The MPICH_STATS_FILE determines the prefix of the file to be used. This information may provide insight into how MPI performance may be improved. Default: 0
20220317 014102.899 INFO             PET5 index=  75                                   MPIR_CVAR_STATS_VERBOSITY : Specifies the verbosity of the MPI statistics summary. This information may provide insight into how MPI performance may be improved. Increase the value for more detailed summary. Default: 1 1        USER_BASIC  (default) 2        USER_DETAIL 3        USER_ALL 4        TUNER_BASIC 5        TUNER_DETAIL 6        TUNER_ALL 7        MPIDEV_BASIC 8        MPIDEV_DETAIL 9        MPIDEV_ALL
20220317 014102.899 INFO             PET5 index=  76                                        MPIR_CVAR_STATS_FILE : Specifies the filename prefix for the set of data files written when MPICH_STATS_DISPLAY is set to 2. The filename prefix may be a full absolute pathname or a relative pathname. Default: _cray_stats_
20220317 014102.899 INFO             PET5 index=  77                              MPIR_CVAR_RANK_REORDER_DISPLAY : If set, causes rank 0 to display which node each MPI rank resides in. The rank order can be manipulated via the MPICH_RANK_REORDER_METHOD environment variable or MPIR_CVAR_RANK_REORDER_METHOD control variable. Default: Not set
20220317 014102.899 INFO             PET5 index=  78                               MPIR_CVAR_RANK_REORDER_METHOD : Overrides the default MPI rank placement scheme. If this variable is not set, the default aprun launcher placement policy is used. The default policy for aprun is SMP-style placement. To display the MPI rank placement information, set MPICH_RANK_REORDER_DISPLAY. See manpage for more details. Default: 1, for SMP-style placement.
20220317 014102.899 INFO             PET5 index=  79                                 MPIR_CVAR_USE_SYSTEM_MEMCPY : Note:  This environment variable is deprecated and scheduled to be removed in a future release. Use MPICH_OPTIMIZED_MEMCPY instead. If set, use the system (glibc) version of memcpy(); otherwise, an optimized version of memcpy() may be used. Currently, an optimized version of memcpy() is available only for Intel processors. Default: Not set
20220317 014102.899 INFO             PET5 index=  80                                   MPIR_CVAR_VERSION_DISPLAY : If set, causes MPICH to display the CRAY MPICH version number as well as build date information. Default: Not enabled
20220317 014102.899 INFO             PET5 index=  81                          MPIR_CVAR_USE_GPU_STREAM_TRIGGERED : If set, causes MPICH to allow using stream triggered GPU communication operations. Default: Not enabled
20220317 014102.899 INFO             PET5 index=  82                               MPIR_CVAR_NUM_MAX_GPU_STREAMS : If set, causes MPICH to allow using the set maximum number of streams concurrently in the stream triggered GPU communication operations. Default: 26
20220317 014102.899 INFO             PET5 index=  83                                  MPIR_CVAR_MEMCPY_MEM_CHECK : If set, enables a check of the memcpy() source and destination areas. If they overlap, the application asserts with an error message listing the file, line, and memory range overlap. If this error is found, correct it either by changing the memory ranges or possibly by using MPI_IN_PLACE. Default: not set (off)
20220317 014102.899 INFO             PET5 index=  84                                     MPIR_CVAR_MSG_QUEUE_DBG : If set, turns on TotalView Message Queue Debugging support so that message queues are tracked in the TotalView debugger and a message queue graph can be generated. Enabling this feature degrades performance. Default: not enabled.
20220317 014102.899 INFO             PET5 index=  85                             MPIR_CVAR_NO_BUFFER_ALIAS_CHECK : If set, the buffer alias error check for collectives is disabled. The MPI standard does not allow aliasing of type OUT or INOUT parameters on the same collective function call. The use of MPI_IN_PLACE is required in these scenarios. A new check was added in MPT 5.2 to detect this condition and report the error. To bypass this check, set MPICH_NO_BUFFER_ALIAS_CHECK to any value. Default: not set
20220317 014102.899 INFO             PET5 index=  86                                MPIR_CVAR_ALLOC_MEM_AFFINITY : Controls the affinity of the memory region allocated by the MPI_Alloc_mem() or MPI_Win_allocate() operations. On systems that do not offer High Bandwidth Memory capabilities, (such as Intel Haswell or Broadwell processors), this env. variable is ignored and memory affinity is set to DDR. On Phi processors, such as KNL (and KNH, KNP in the future), this env. variable allows users to specifically request the memory returned by MPI_Alloc_mem() and MPI_Win_allocate() to be bound to either DDR, or the MCDRAM. Users can request a specific page size or memory binding policy via the MPICH_ALLOC_MEM_POLICY and MPICH_ALLOC_MEM_PG_SZ env. variables. Default: SYS_DEFAULT
20220317 014102.899 INFO             PET5 index=  87                             MPIR_CVAR_INTERNAL_MEM_AFFINITY : Controls the affinity of internal memory regions allocated by the MPI library. This variable currently affects the memory affinity of the mail-boxes used for off-node communication, and the shared-memory regions that are used for on-node pt2pt and collective ops. On systems that do not offer High Bandwidth Memory capabilities, (such as Intel Haswell or Broadwell processors), this env. variable is ignored and memory affinity is set to DDR. On Phi processors, such as KNL (and KNH, KNP in the future), this env. variable allows users to specifically request the internal memory regions used by the MPI library to be bound to either DDR, or the MCDRAM. The default affinity settings will be governed by the system defaults. For example, on a KNL system configured in the Quad/Flat mode, if the job is run with numactl --membind=1, all of MPI's internal memory will be bound to MCDRAM if this variable is not set. Default: SYS_DEFAULT
20220317 014102.899 INFO             PET5 index=  88                                  MPIR_CVAR_ALLOC_MEM_POLICY : Controls the memory affinity policy on systems with specialized memory hardware. By default, the memory policy is set to "{P}referred". Other accepted values are "{M}andatory" and "{I}"nterleave. Default: Preferred
20220317 014102.899 INFO             PET5 index=  89                                   MPIR_CVAR_ALLOC_MEM_PG_SZ : Controls the page size for the MPI_Alloc_mem() and MPI_Win_allocate() operations. This parameters defaults to 4KB base pages. The supported values are 2M, 4M, 8M, 16M, 32M, 64M, 128M, 256M, 512M, 1G and 2G. Default: 4096
20220317 014102.899 INFO             PET5 index=  90                                   MPIR_CVAR_OPT_THREAD_SYNC : Controls the mechanism used to implement thread-synchronization inside the Cray MPICH library. If set to 1, an optimized synchronization implementation is used. If set to 0, Cray MPICH falls back to using a pthread_mutex-based thread-synchronization implementation. Default: 1
20220317 014102.899 INFO             PET5 index=  91                                 MPIR_CVAR_THREAD_YIELD_FREQ : Determines how often a thread yields while waiting to acquire a lock in the new Cray optimized locking impl. This variable has no effect if MPICH_CRAY_OPT_THREAD_SYNC is 0. Default: 10000
20220317 014102.899 INFO             PET5 index=  92                                   MPIR_CVAR_MEM_DEBUG_FNAME : If set, the MPI library creates new files with the user specified name and writes various important memory statistics into these files. This information can be useful for post-processing. Users can set MPICH_MEM_DEBUG_FNAME to any suitable string. The resulting files are named as "string".<pid>.<MPI-rank>. For example, if MPICH_MEM_DEBUG_FNAME is set to "MEM_DBG_MSGS" the debug file for rank0 will be named "MEM_DBG_MSGS.<pid>.0 and written to the user's current working directory. If this flag is not set, the MPI library will redirect all the debug messages to stderr. Default: unset (disabled)
20220317 014102.899 INFO             PET5 index=  93                                   MPIR_CVAR_MALLOC_FALLBACK : Set the policy for fallback behavior when attempting to allocate large pages for internal buffers and insufficient large pages are available to satisfy the request. If set to enabled, MPICH falls back to using malloc in such cases. Default: not enabled (process fails and job terminates if insufficient large pages are available to satisfy the request)
20220317 014102.899 INFO             PET5 index=  94                              MPIR_CVAR_NO_DIRECT_GPU_ACCESS : If true, IPC and NIC-GPU Direct Access capabilities are not used for GPU-to-GPU transfers. This is mainly used for debugging.  Default: 0
20220317 014102.899 INFO             PET5 index=  95                                      MPIR_CVAR_G2G_PIPELINE : If nonzero, the device-host and network transfers will be overlapped to pipeline GPU-to-GPU transfers. Setting MPICH_G2G_PIPELINE to N will allow N GPU-to-GPU messages to be efficiently in-flight at any one time. If MPICH_G2G_PIPELINE is nonzero but MPICH_GPU_SUPPORT_ENABLED is disabled, MPICH_G2G_PIPELINE will be turned off. If MPICH_GPU_SUPPORT_ENABLED is enabled but MPICH_G2G_PIPELINE is 0, the default value is set to 8.  The pipeline-based implementation is being carried over from  CH3/Aries implementation. We think the pipelined-based implementation will not be necessary on SS-11 systems.  This logic may get dropped in the future.  Default: not set
20220317 014102.899 INFO             PET5 index=  96                               MPIR_CVAR_GPU_SUPPORT_ENABLED : If set, allows the MPI application to pass GPU pointers directly to MPI communication functions.  Default: not set
20220317 014102.899 INFO             PET5 index=  97                MPIR_CVAR_GPU_MANAGED_MEMORY_SUPPORT_ENABLED : If set, allows the MPI application to pass pointers allocated via  GPU managed memory allocators to MPI communication functions. Default: not set
20220317 014102.899 INFO             PET5 index=  98                         MPIR_CVAR_GPU_COLL_STAGING_AREA_OPT : If set, allows Cray MPI to use pre-registered CPU-attached memory to optimize collective ops.  Default: set
20220317 014102.899 INFO             PET5 index=  99               MPIR_CVAR_GPU_COLL_REGISTER_SHARED_MEM_REGION : If set, allows Cray MPI to registere CPU-attached shared memory regions  used to optimize collective ops.  Default: set
20220317 014102.899 INFO             PET5 index= 100                                   MPIR_CVAR_GPU_IPC_ENABLED : If set to 1, Cray MPICH will use GPU IPC to move data between GPU devices that are within the same node. If set to 0, Cray MPICH will use staging buffers on the host to implement data transfer between GPU devices that are within the same node. If MPIR_CVAR_GPU_SUPPORT_ENABLED is set, Cray MPICH  automatically attempts to enable the IPC optimizations.   If MPIR_CVAR_NO_DIRECT_GPU_ACCESS is set, IPC optimizations are disabled.  Default: -1
20220317 014102.899 INFO             PET5 index= 101                                 MPIR_CVAR_GPU_IPC_THRESHOLD : Specifies the minimum message size in bytes to consider for single-copy transfers between GPU devices that are within the same compute node. The value is interpreted as bytes, unless the string ends in a K, which indicates kilobytes, or M, which indicates megabytes.
20220317 014102.899 INFO             PET5 index= 102                                  MPIR_CVAR_GPU_IPC_PROTOCOL : Specifies the protocol used for large msg on-node, inter-GPU  data transfers via GPU IPC.  Valid options are RPUT or RGET.  Default: RGET
20220317 014102.899 INFO             PET5 index= 103                                 MPIR_CVAR_GPU_NO_ASYNC_COPY : If set, disables the use of asynchronous memcpy's for on-node GPU-aware transfers. Default: 0
20220317 014102.899 INFO             PET5 index= 104                                      MPIR_CVAR_ENABLE_YAKSA : If set, enables the use of the YAKSA datatype engine for packing and unpacking non-contiguous buffers in both CPU- and GPU-attached memory. Default: 0
20220317 014102.899 INFO             PET5 index= 105                              MPIR_CVAR_GPU_EAGER_DEVICE_MEM : If set to 1, GPU-attached memory will be used for the eager staging buffers for short on-node messages where at least the sending buffer is GPU-attached. If set to 0, this optimization is disabled and we fall-back to using staging buffers on the CPU for small message  intra-node device-host and device-device MPI ops. Default: 1
20220317 014102.899 INFO             PET5 index= 106                       MPIR_CVAR_GPU_EAGER_REGISTER_HOST_MEM : If set to 1, enables MPI to request POSIX  eager memory to be registered with the GPU runtime.  This is an optimization to improve the performance of  small message intra-node, inter-GPU MPI ops that use  POSIX shared memory as bounce buffers.  If set to 0, this optimization is disabled and we rely on GTL to lock/unlock host memory regions for a given on-node, small msg inter-GPU transfer.  Default: 1
20220317 014102.899 INFO             PET5 index= 107                          MPIR_CVAR_GPU_ALLREDUCE_USE_KERNEL : If set, adds a hint that the use of device kernels for reduction operations is desired. MPI is not guaranteed to use a device kernel for all reduction operations. Default: 0
20220317 014102.899 INFO             PET5 index= 108                                   MPIR_CVAR_RMA_MAX_PENDING : Determines how many RMA operation may be outstanding at any time over libfabrics. RMA operations beyond this max will be queued and only issued as pending operations complete. Default: 64
20220317 014102.899 INFO             PET5 index= 109                                MPIR_CVAR_RMA_SHM_ACCUMULATE : If set to 1, enables SHM accumulate operations. If set to 0, disables SHM accumulate operations. It also sets the default for the window hint "disable_shm_accumulate".  Default: 1
20220317 014102.899 INFO             PET5 --- VMK::logSystem() end ---------------------------------
20220317 014102.899 INFO             PET5 main: --- VMK::log() start -------------------------------------
20220317 014102.899 INFO             PET5 main: vm located at: 0xee7ae0
20220317 014102.899 INFO             PET5 main: petCount=6 localPet=5 mypthid=23147085267072 currentSsiPe=5
20220317 014102.899 INFO             PET5 main: Current system level affinity pinning for local PET:
20220317 014102.899 INFO             PET5 main:  SSIPE=5
20220317 014102.899 INFO             PET5 main: Current system level OMP_NUM_THREADS setting for local PET: 128
20220317 014102.899 INFO             PET5 main: ssiCount=1 localSsi=0
20220317 014102.899 INFO             PET5 main: mpionly=1 threadsflag=0
20220317 014102.899 INFO             PET5 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220317 014102.899 INFO             PET5 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220317 014102.899 INFO             PET5 main:  PE=0 SSI=0 SSIPE=0
20220317 014102.899 INFO             PET5 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220317 014102.899 INFO             PET5 main:  PE=1 SSI=0 SSIPE=1
20220317 014102.899 INFO             PET5 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220317 014102.899 INFO             PET5 main:  PE=2 SSI=0 SSIPE=2
20220317 014102.899 INFO             PET5 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220317 014102.899 INFO             PET5 main:  PE=3 SSI=0 SSIPE=3
20220317 014102.899 INFO             PET5 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220317 014102.899 INFO             PET5 main:  PE=4 SSI=0 SSIPE=4
20220317 014102.899 INFO             PET5 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220317 014102.899 INFO             PET5 main:  PE=5 SSI=0 SSIPE=5
20220317 014102.899 INFO             PET5 main: --- VMK::log() end ---------------------------------------
20220317 014102.899 INFO             PET5 Executing 'userm1_setvm'
20220317 014102.900 INFO             PET5 Executing 'userm1_register'
20220317 014102.900 INFO             PET5 Executing 'userm2_setvm'
20220317 014102.900 DEBUG            PET5 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220317 014102.900 DEBUG            PET5 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220317 014102.901 INFO             PET5 Entering 'user1_run'
20220317 014102.901 INFO             PET5 model1: --- VMK::log() start -------------------------------------
20220317 014102.901 INFO             PET5 model1: vm located at: 0xf432b0
20220317 014102.901 INFO             PET5 model1: petCount=6 localPet=5 mypthid=23147085267072 currentSsiPe=5
20220317 014102.901 INFO             PET5 model1: Current system level affinity pinning for local PET:
20220317 014102.901 INFO             PET5 model1:  SSIPE=5
20220317 014102.901 INFO             PET5 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220317 014102.901 INFO             PET5 model1: ssiCount=1 localSsi=0
20220317 014102.901 INFO             PET5 model1: mpionly=1 threadsflag=0
20220317 014102.901 INFO             PET5 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220317 014102.901 INFO             PET5 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220317 014102.901 INFO             PET5 model1:  PE=0 SSI=0 SSIPE=0
20220317 014102.901 INFO             PET5 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220317 014102.901 INFO             PET5 model1:  PE=1 SSI=0 SSIPE=1
20220317 014102.901 INFO             PET5 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220317 014102.901 INFO             PET5 model1:  PE=2 SSI=0 SSIPE=2
20220317 014102.901 INFO             PET5 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220317 014102.901 INFO             PET5 model1:  PE=3 SSI=0 SSIPE=3
20220317 014102.901 INFO             PET5 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220317 014102.901 INFO             PET5 model1:  PE=4 SSI=0 SSIPE=4
20220317 014102.901 INFO             PET5 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220317 014102.901 INFO             PET5 model1:  PE=5 SSI=0 SSIPE=5
20220317 014102.901 INFO             PET5 model1: --- VMK::log() end ---------------------------------------
20220317 014102.901 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220317 014103.270 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220317 014103.577 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220317 014103.884 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220317 014104.191 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220317 014104.497 INFO             PET5 Exiting 'user1_run'
20220317 014106.143 INFO             PET5 Entering 'user1_run'
20220317 014106.143 INFO             PET5 model1: --- VMK::log() start -------------------------------------
20220317 014106.143 INFO             PET5 model1: vm located at: 0xf432b0
20220317 014106.143 INFO             PET5 model1: petCount=6 localPet=5 mypthid=23147085267072 currentSsiPe=5
20220317 014106.143 INFO             PET5 model1: Current system level affinity pinning for local PET:
20220317 014106.143 INFO             PET5 model1:  SSIPE=5
20220317 014106.144 INFO             PET5 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220317 014106.144 INFO             PET5 model1: ssiCount=1 localSsi=0
20220317 014106.144 INFO             PET5 model1: mpionly=1 threadsflag=0
20220317 014106.144 INFO             PET5 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220317 014106.144 INFO             PET5 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220317 014106.144 INFO             PET5 model1:  PE=0 SSI=0 SSIPE=0
20220317 014106.144 INFO             PET5 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220317 014106.144 INFO             PET5 model1:  PE=1 SSI=0 SSIPE=1
20220317 014106.144 INFO             PET5 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220317 014106.144 INFO             PET5 model1:  PE=2 SSI=0 SSIPE=2
20220317 014106.144 INFO             PET5 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220317 014106.144 INFO             PET5 model1:  PE=3 SSI=0 SSIPE=3
20220317 014106.144 INFO             PET5 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220317 014106.144 INFO             PET5 model1:  PE=4 SSI=0 SSIPE=4
20220317 014106.144 INFO             PET5 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220317 014106.144 INFO             PET5 model1:  PE=5 SSI=0 SSIPE=5
20220317 014106.144 INFO             PET5 model1: --- VMK::log() end ---------------------------------------
20220317 014106.144 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220317 014106.451 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220317 014106.758 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220317 014107.065 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220317 014107.372 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220317 014107.679 INFO             PET5 Exiting 'user1_run'
20220317 014109.291 INFO             PET5  NUMBER_OF_PROCESSORS           6
20220317 014109.291 INFO             PET5  PASS  System Test ESMF_ArraySharedDeSSISTest, ESMF_ArraySharedDeSSISTest.F90, line 297
20220317 014109.291 INFO             PET5 Finalizing ESMF
