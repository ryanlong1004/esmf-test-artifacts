build time -- 2022-03-20 06:05:27.360416
20220320 062518.028 INFO             PET0 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220320 062518.028 INFO             PET0 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220320 062518.028 INFO             PET0 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220320 062518.028 INFO             PET0 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220320 062518.028 INFO             PET0 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220320 062518.028 INFO             PET0 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220320 062518.028 INFO             PET0 Running with ESMF Version   : v8.3.0b09-102-gabaf3ef548
20220320 062518.028 INFO             PET0 ESMF library build date/time: "Mar 20 2022" "06:03:36"
20220320 062518.028 INFO             PET0 ESMF library build location : /glade/scratch/rlong/esmf-testing/gfortran_10.1.0_mpt_O_jedwards_pio_update2
20220320 062518.028 INFO             PET0 ESMF_COMM                   : mpt
20220320 062518.028 INFO             PET0 ESMF_MOAB                   : enabled
20220320 062518.028 INFO             PET0 ESMF_LAPACK                 : enabled
20220320 062518.028 INFO             PET0 ESMF_NETCDF                 : enabled
20220320 062518.028 INFO             PET0 ESMF_PNETCDF                : disabled
20220320 062518.028 INFO             PET0 ESMF_PIO                    : enabled
20220320 062518.028 INFO             PET0 ESMF_YAMLCPP                : enabled
20220320 062518.030 INFO             PET0 --- VMK::logSystem() start -------------------------------
20220320 062518.030 INFO             PET0 esmfComm=mpt
20220320 062518.030 INFO             PET0 isPthreadsEnabled=1
20220320 062518.030 INFO             PET0 isOpenMPEnabled=1
20220320 062518.030 INFO             PET0 isOpenACCEnabled=0
20220320 062518.030 INFO             PET0 isSsiSharedMemoryEnabled=1
20220320 062518.030 INFO             PET0 ssiCount=1 peCount=6
20220320 062518.030 INFO             PET0 PE=0 SSI=0 SSIPE=0
20220320 062518.030 INFO             PET0 PE=1 SSI=0 SSIPE=1
20220320 062518.030 INFO             PET0 PE=2 SSI=0 SSIPE=2
20220320 062518.030 INFO             PET0 PE=3 SSI=0 SSIPE=3
20220320 062518.030 INFO             PET0 PE=4 SSI=0 SSIPE=4
20220320 062518.030 INFO             PET0 PE=5 SSI=0 SSIPE=5
20220320 062518.030 INFO             PET0 --- VMK::logSystem() MPI Control Variables ---------------
20220320 062518.030 INFO             PET0 index=   0                                        MPI_ADJUST_ALLGATHER : Algorithm for MPI_Allgather
20220320 062518.030 INFO             PET0 index=   1                                       MPI_ADJUST_ALLGATHERV : Algorithm for MPI_Allgatherv
20220320 062518.030 INFO             PET0 index=   2                                        MPI_ADJUST_ALLREDUCE : Algorithm for MPI_Allreduce
20220320 062518.030 INFO             PET0 index=   3                                         MPI_ADJUST_ALLTOALL : Algorithm for MPI_Alltoall
20220320 062518.030 INFO             PET0 index=   4                                        MPI_ADJUST_ALLTOALLV : Algorithm for MPI_Alltoallv
20220320 062518.030 INFO             PET0 index=   5                                        MPI_ADJUST_ALLTOALLW : Algorithm for MPI_Alltoallw
20220320 062518.030 INFO             PET0 index=   6                                          MPI_ADJUST_BARRIER : Algorithm for MPI_Barrier
20220320 062518.030 INFO             PET0 index=   7                                            MPI_ADJUST_BCAST : Algorithm for MPI_Bcast
20220320 062518.030 INFO             PET0 index=   8                                           MPI_ADJUST_EXSCAN : Algorithm for MPI_Exscan
20220320 062518.030 INFO             PET0 index=   9                                           MPI_ADJUST_GATHER : Algorithm for MPI_Gather
20220320 062518.030 INFO             PET0 index=  10                                          MPI_ADJUST_GATHERV : Algorithm for MPI_Gatherv
20220320 062518.032 INFO             PET0 index=  11                                   MPI_ADJUST_REDUCE_SCATTER : Algorithm for MPI_Reduce_scatter
20220320 062518.032 INFO             PET0 index=  12                                           MPI_ADJUST_REDUCE : Algorithm for MPI_Reduce
20220320 062518.032 INFO             PET0 index=  13                                             MPI_ADJUST_SCAN : Algorithm for MPI_Scan
20220320 062518.032 INFO             PET0 index=  14                                          MPI_ADJUST_SCATTER : Algorithm for MPI_Scatter
20220320 062518.032 INFO             PET0 index=  15                                         MPI_ADJUST_SCATTERV : Algorithm for MPI_Scatterv
20220320 062518.032 INFO             PET0 index=  16                                                   MPI_ARRAY : Alternative array name for communicating with Array Services
20220320 062518.032 INFO             PET0 index=  17                                          MPI_ASYNC_PROGRESS : Enables asynchronous progress of non-blocking operations
20220320 062518.032 INFO             PET0 index=  18                                              MPI_BUFFER_MAX : Threshold for blocking message size resulting in the use of zero-copy
20220320 062518.033 INFO             PET0 index=  19                                              MPI_BUFS_LIMIT : Max number of internal buffers a single transfer can use
20220320 062518.033 INFO             PET0 index=  20                                           MPI_BUFS_PER_PROC : Number of private message buffers per process for send and receive of long messages
20220320 062518.033 INFO             PET0 index=  21                                              MPI_CHECK_ARGS : Enables checking of MPI function arguments
20220320 062518.033 INFO             PET0 index=  22                                             MPI_CLOCKSOURCE : Time sources to use to support the MPI_Wtime() and MPI_Wtick() functions
20220320 062518.033 INFO             PET0 index=  23                                           MPI_KCOPY_ENABLED : Uses the Linux kernel to do large transfers between local ranks
20220320 062518.033 INFO             PET0 index=  24                                           MPI_COLL_A2A_FRAG : Average send size up to which MPT will use optimizations in MPI_Alltoall() and its variants
20220320 062518.033 INFO             PET0 index=  25                                        MPI_COLL_CLUSTER_OPT : Controls whether node leaders are used to accelerate collectives
20220320 062518.033 INFO             PET0 index=  26                                            MPI_COLL_GATHERV : Average send size up to which MPT will use optimizations in MPI_Gatherv()
20220320 062518.033 INFO             PET0 index=  27                                            MPI_COLL_LEADERS : Controls whether MPT uses enhanced node leaders to improve performance of collectives
20220320 062518.033 INFO             PET0 index=  28                                     MPI_COLL_NUMA_THRESHOLD : Number of NUMA nodes per host beyond which MPT will enable special NUMA performance optimizations for collective operations
20220320 062518.033 INFO             PET0 index=  29                                                MPI_COLL_OPT : Controls whether optimized collectives are enabled
20220320 062518.033 INFO             PET0 index=  30                                        MPI_COLL_OPT_VERBOSE : Enables display of optimized collective communication algorithms that are active for every communicator
20220320 062518.033 INFO             PET0 index=  31                                             MPI_COLL_PREREG : Enables pre-registration of data buffers for collectives with IB Region cache for potential amortization across subsequent calls
20220320 062518.033 INFO             PET0 index=  32                                         MPI_COLL_RED_RB_MIN : Threshold under which a doubling Allreduce is used
20220320 062518.033 INFO             PET0 index=  33                                       MPI_COLL_REPRODUCIBLE : Controls whether MPT does reductions in a reproducible manner to avoid ordering issues with floating point calculations
20220320 062518.033 INFO             PET0 index=  34                                               MPI_COLL_SYNC : Enables usage of collective algorithms suitable for tighlty synchronized applications
20220320 062518.033 INFO             PET0 index=  35                                                MPI_COMM_MAX : Max number of communicators that can be used in an MPI program
20220320 062518.033 INFO             PET0 index=  36                                        MPI_CONTEXT_MULTIPLE : Controls whether MPT will attempt to allocate separate network resources for each MPI Communicator that is created
20220320 062518.033 INFO             PET0 index=  37                                                MPI_COREDUMP : Controls which ranks of an MPI job can dump core on receipt of a core-dumping signal
20220320 062518.033 INFO             PET0 index=  38                                       MPI_COREDUMP_DEBUGGER : Controls the debugger MPT uses to display stacktraces of ranks upon receipt of a core-dumping signal
20220320 062518.033 INFO             PET0 index=  39                                         MPI_CUDA_BUFFER_MAX : Max message size that controls whether MPT uses default message buffering or CUDA IPC to do zero-copy transfer
20220320 062518.033 INFO             PET0 index=  40                          MPI_DEFAULT_SINGLE_COPY_BUFFER_MAX : Controls zero-copy message criteria for shared memory
20220320 062518.033 INFO             PET0 index=  41                                 MPI_DEFAULT_SINGLE_COPY_OFF : Enables buffering always, even for messages that qualify for single copy
20220320 062518.033 INFO             PET0 index=  42                                                     MPI_DIR : Sets the working directory on a host when using Array Services
20220320 062518.033 INFO             PET0 index=  43                                        MPI_DISPLAY_SETTINGS : Causes MPT to display the default and current settings of the environment variables controlling it
20220320 062518.033 INFO             PET0 index=  44                                             MPI_DSM_CPULIST : Specifies a list of CPUs on which to run an MPI application
20220320 062518.033 INFO             PET0 index=  45                                          MPI_DSM_DISTRIBUTE : Enables NUMA-aware CPU pinning of MPI Ranks
20220320 062518.033 INFO             PET0 index=  46                                                 MPI_DSM_OFF : Turns off nonuniform memory access (NUMA) optimization in the MPI library
20220320 062518.033 INFO             PET0 index=  47                                             MPI_DSM_VERBOSE : Causes MPT to print information about process placement
20220320 062518.033 INFO             PET0 index=  48                                            MPI_GATHER_RANKS : Causes MPT to attempt to collect all the workers on a host under a single shepherd,
20220320 062518.033 INFO             PET0 index=  49                                     MPI_GRAPH_OPTIMIZATIONS : Controls the number of optimization loops for MPI_Graph_create
20220320 062518.033 INFO             PET0 index=  50                                               MPI_GROUP_MAX : Determines the maximum number of groups that can simultaneously exist for any single MPI process
20220320 062518.033 INFO             PET0 index=  51                                               MPI_INIT_LATE : If set and MPT is launching under SLURM or MPI_SHEPHERD mode, then MPT will delay all initialization until MPI_Init() is called
20220320 062518.033 INFO             PET0 index=  52                                          MPI_LAUNCH_TIMEOUT : Number of seconds to wait before timing out when starting up remote jobs
20220320 062518.033 INFO             PET0 index=  53                                        MPI_MAPPED_HEAP_SIZE : Max amount of heap in bytes per MPI process that is memory mapped between processes under the same shepherd
20220320 062518.033 INFO             PET0 index=  54                                       MPI_MAPPED_STACK_SIZE : Amount of stack in bytes that is memory mapped per MPI process
20220320 062518.033 INFO             PET0 index=  55                                               MPI_MEM_ALIGN : Alignment for memory allocations to help eliminate precision errors related to special CPU instructions for reductions and floating point calculations
20220320 062518.033 INFO             PET0 index=  56                                              MPI_MEMMAP_OFF : Turns off the memory mapping feature used in zero-copy transfers and MPI one-sided communication
20220320 062518.033 INFO             PET0 index=  57                                           MPI_MEMORY_REPORT : Specifies whether to print information about MPT's internal memory usage
20220320 062518.033 INFO             PET0 index=  58                                      MPI_MEMORY_REPORT_FILE : If set this is the file to print memory report information to
20220320 062518.033 INFO             PET0 index=  59                                                 MPI_MSG_MEM : Controls the total amount of memory used by MPT on temporary message headers
20220320 062518.033 INFO             PET0 index=  60                                                     MPI_NAP : Controls the way in which ranks wait for events to occur
20220320 062518.033 INFO             PET0 index=  61                                              MPI_NUM_QUICKS : Limits the number of outstanding quick RDMA transfers per rank
20220320 062518.033 INFO             PET0 index=  62                                         MPI_OMP_NUM_THREADS : Represents the value of the OMP_NUM_THREADS environment variable for each host-program specification on the mpirun command line
20220320 062518.033 INFO             PET0 index=  63                                          MPI_OPENMP_INTEROP : Causes the placement of MPI processes to better accommodate the OpenMP threads associated with each process
20220320 062518.033 INFO             PET0 index=  64                                                    MPI_PMIX : Causes MPT to use the PMIx interface instead of PMI2 when working with SLURM
20220320 062518.033 INFO             PET0 index=  65                                           MPI_PREFAULT_HEAP : Amount of xpmem-attached memory that should be prefaulted
20220320 062518.033 INFO             PET0 index=  66                                               MPI_QUERYABLE : Enables use of the mpt_query command to query a MPI job about its activities and statistics
20220320 062518.033 INFO             PET0 index=  67                                           MPI_REQUEST_DEBUG : Enables extra checking of the use of MPI requests
20220320 062518.033 INFO             PET0 index=  68                                             MPI_REQUEST_MAX : Determines the number of MPI_Requests preallocated by MPT to track simultaneous nonblocking sends and receives within each MPI process
20220320 062518.033 INFO             PET0 index=  69                                              MPI_RESET_PATH : Causes MPT to set the PATH environment variable of MPI processess using a system default instead of the mpirun process environment setting
20220320 062518.033 INFO             PET0 index=  70                                                MPI_SHEPHERD : Causes MPT to use a completely separate mpt_shepherd program which does a fork+exec of each rank
20220320 062518.033 INFO             PET0 index=  71                                                 MPI_SIGTRAP : Specifies if MPT's signal handler should override any existing signal handlers for signals SIGSEGV, SIGQUIT, SIGILL, SIGABRT, SIGBUS, and SIGFPE
20220320 062518.033 INFO             PET0 index=  72                                                   MPI_STATS : Enables printing of MPI internal statistics
20220320 062518.033 INFO             PET0 index=  73                                              MPI_STATS_FILE : Name of the file that contains information from MPI_STATS and other statistics counters
20220320 062518.033 INFO             PET0 index=  74                                           MPI_STATUS_SIGNAL : Enables MPT to receive a signal which will prompt each rank to print status information to stderr
20220320 062518.033 INFO             PET0 index=  75                                          MPI_SYNC_THRESHOLD : Transfer size in bytes up to which MPT will attempt to send the data immediately in MPI_Ssend and MPI_Issend
20220320 062518.033 INFO             PET0 index=  76                                             MPI_SYSLOG_COPY : Enables messages about MPT internal errors and system failures to get copied to the system log
20220320 062518.033 INFO             PET0 index=  77                                              MPI_TYPE_DEPTH : Maximum number of nesting levels for derived data types
20220320 062518.033 INFO             PET0 index=  78                                                MPI_TYPE_MAX : Maximum number of data types that can simultaneously exist for any single MPI process
20220320 062518.033 INFO             PET0 index=  79                                        MPI_UNBUFFERED_STDIO : Disables buffering of stdio and stderr output
20220320 062518.033 INFO             PET0 index=  80                                                MPI_UNIVERSE : Additional hosts on which MPI processes may be launched with MPI_Comm_spawn and MPI_Comm_spawn_multiple functions
20220320 062518.033 INFO             PET0 index=  81                                           MPI_UNIVERSE_SIZE : Controls number of ranks when running in spawn capable mode
20220320 062518.033 INFO             PET0 index=  82                                          MPI_UNWEIGHTED_OLD : Enables pre-2.13 ABI of MPT for MPI_UNWEIGHTED
20220320 062518.033 INFO             PET0 index=  83                                                MPI_USE_CUDA : Enables CUDA support and GPU buffers for all operations
20220320 062518.033 INFO             PET0 index=  84                                             MPI_USING_VTUNE : Enables compatibility with Intel Vtune for MPT's classic SGI ABI
20220320 062518.033 INFO             PET0 index=  85                                                 MPI_VERBOSE : Enables display of information like interconnect device usage and environment variables set to non-default values
20220320 062518.033 INFO             PET0 index=  86                                                MPI_VERBOSE2 : Allows additional MPT diagnostic information to be printed in the standard output stream
20220320 062518.033 INFO             PET0 index=  87                                          MPI_WATCHDOG_TIMER : Minutes to wait before an MPI process that is unable to contact another process aborts the application
20220320 062518.033 INFO             PET0 index=  88                                               MPI_WILDCARDS : When set to false allows MPT to make some optimizations in MPI_Recv() and MPI_Probe() if MPI_ANY_SOURCE is not used
20220320 062518.033 INFO             PET0 index=  89                                                MPI_WIN_MODE : Selects RDMA capabilities of hardware v/s normal send/receive based emulation for one-sided communication
20220320 062518.033 INFO             PET0 index=  90                                               MPI_WORLD_MAP : Rank to host mapping
20220320 062518.033 INFO             PET0 index=  91                                           MPI_XPMEM_ENABLED : Determines whether HPE XPMEM is used or not
20220320 062518.033 INFO             PET0 index=  92                                            MPIO_DIRECT_READ : Enables Direct IO in ROMIO
20220320 062518.033 INFO             PET0 index=  93                                           MPIO_DIRECT_WRITE : Enables Direct IO in ROMIO
20220320 062518.033 INFO             PET0 index=  94                                 MPIO_DIRECT_READ_CHUNK_SIZE : Read chunk size when Direct IO is enabled in ROMIO
20220320 062518.033 INFO             PET0 index=  95                                MPIO_DIRECT_WRITE_CHUNK_SIZE : Write chunk size when Direct IO is enabled in ROMIO
20220320 062518.033 INFO             PET0 index=  96                                 MPIO_LUSTRE_WRITE_AGGMETHOD : Write aggregrate method for ROMIO Lustre filesystem
20220320 062518.033 INFO             PET0 index=  97                                   MPIO_LUSTRE_GCYC_MIN_ITER : Minimum number of iterations for group-cyclic aggregator
20220320 062518.033 INFO             PET0 index=  98                                    profiled_recv_request_id : identity of the request-of-interest
20220320 062518.033 INFO             PET0 --- VMK::logSystem() end ---------------------------------
20220320 062518.033 INFO             PET0 main: --- VMK::log() start -------------------------------------
20220320 062518.033 INFO             PET0 main: vm located at: 0x18a7de0
20220320 062518.033 INFO             PET0 main: petCount=6 localPet=0 mypthid=46940161916672 currentSsiPe=0
20220320 062518.033 INFO             PET0 main: Current system level affinity pinning for local PET:
20220320 062518.033 INFO             PET0 main:  SSIPE=0
20220320 062518.033 INFO             PET0 main: Current system level OMP_NUM_THREADS setting for local PET: 36
20220320 062518.033 INFO             PET0 main: ssiCount=1 localSsi=0
20220320 062518.033 INFO             PET0 main: mpionly=1 threadsflag=0
20220320 062518.033 INFO             PET0 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220320 062518.033 INFO             PET0 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220320 062518.033 INFO             PET0 main:  PE=0 SSI=0 SSIPE=0
20220320 062518.033 INFO             PET0 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220320 062518.033 INFO             PET0 main:  PE=1 SSI=0 SSIPE=1
20220320 062518.033 INFO             PET0 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220320 062518.033 INFO             PET0 main:  PE=2 SSI=0 SSIPE=2
20220320 062518.033 INFO             PET0 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220320 062518.033 INFO             PET0 main:  PE=3 SSI=0 SSIPE=3
20220320 062518.033 INFO             PET0 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220320 062518.033 INFO             PET0 main:  PE=4 SSI=0 SSIPE=4
20220320 062518.033 INFO             PET0 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220320 062518.033 INFO             PET0 main:  PE=5 SSI=0 SSIPE=5
20220320 062518.033 INFO             PET0 main: --- VMK::log() end ---------------------------------------
20220320 062518.035 INFO             PET0 Executing 'userm1_setvm'
20220320 062518.035 INFO             PET0 Executing 'userm1_register'
20220320 062518.035 INFO             PET0 Executing 'userm2_setvm'
20220320 062518.035 INFO             PET0 Executing 'userm2_register'
20220320 062518.035 INFO             PET0 model1: --- VMK::log() start -------------------------------------
20220320 062518.035 INFO             PET0 model1: vm located at: 0x292c660
20220320 062518.035 INFO             PET0 model1: petCount=6 localPet=0 mypthid=46940161916672 currentSsiPe=0
20220320 062518.035 INFO             PET0 model1: Current system level affinity pinning for local PET:
20220320 062518.035 INFO             PET0 model1:  SSIPE=0
20220320 062518.035 INFO             PET0 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220320 062518.035 INFO             PET0 model1: ssiCount=1 localSsi=0
20220320 062518.035 INFO             PET0 model1: mpionly=1 threadsflag=0
20220320 062518.035 INFO             PET0 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220320 062518.035 INFO             PET0 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220320 062518.035 INFO             PET0 model1:  PE=0 SSI=0 SSIPE=0
20220320 062518.035 INFO             PET0 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220320 062518.035 INFO             PET0 model1:  PE=1 SSI=0 SSIPE=1
20220320 062518.035 INFO             PET0 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220320 062518.035 INFO             PET0 model1:  PE=2 SSI=0 SSIPE=2
20220320 062518.035 INFO             PET0 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220320 062518.035 INFO             PET0 model1:  PE=3 SSI=0 SSIPE=3
20220320 062518.036 INFO             PET0 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220320 062518.036 INFO             PET0 model1:  PE=4 SSI=0 SSIPE=4
20220320 062518.036 INFO             PET0 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220320 062518.036 INFO             PET0 model1:  PE=5 SSI=0 SSIPE=5
20220320 062518.036 INFO             PET0 model1: --- VMK::log() end ---------------------------------------
20220320 062518.038 INFO             PET0 Entering 'user1_run'
20220320 062518.038 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220320 062518.796 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220320 062519.463 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220320 062520.132 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220320 062520.800 INFO             PET0  user1_run: on SSIPE:            0  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20220320 062521.467 INFO             PET0 Exiting 'user1_run'
20220320 062521.573 INFO             PET0 Entering 'user2_run'
20220320 062521.573 INFO             PET0 model2: --- VMK::log() start -------------------------------------
20220320 062521.573 INFO             PET0 model2: vm located at: 0x292c8c0
20220320 062521.573 INFO             PET0 model2: petCount=1 localPet=0 mypthid=46940161916672 currentSsiPe=0
20220320 062521.573 INFO             PET0 model2: Current system level affinity pinning for local PET:
20220320 062521.573 INFO             PET0 model2:  SSIPE=0
20220320 062521.573 INFO             PET0 model2: Current system level OMP_NUM_THREADS setting for local PET: 6
20220320 062521.573 INFO             PET0 model2: ssiCount=1 localSsi=0
20220320 062521.573 INFO             PET0 model2: mpionly=0 threadsflag=0
20220320 062521.573 INFO             PET0 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220320 062521.573 INFO             PET0 model2: PET=0 lpid=0 tid=0 pid=0 peCount=6 accCount=0
20220320 062521.573 INFO             PET0 model2:  PE=0 SSI=0 SSIPE=0
20220320 062521.573 INFO             PET0 model2:  PE=1 SSI=0 SSIPE=1
20220320 062521.573 INFO             PET0 model2:  PE=2 SSI=0 SSIPE=2
20220320 062521.573 INFO             PET0 model2:  PE=3 SSI=0 SSIPE=3
20220320 062521.573 INFO             PET0 model2:  PE=4 SSI=0 SSIPE=4
20220320 062521.573 INFO             PET0 model2:  PE=5 SSI=0 SSIPE=5
20220320 062521.573 INFO             PET0 model2: --- VMK::log() end ---------------------------------------
20220320 062521.573 INFO             PET0  user2_run: ssiLocalDeCount=           6
20220320 062521.573 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220320 062521.573 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220320 062521.573 INFO             PET0  user2_run: OpenMP thread:           4  on SSIPE:            4  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220320 062521.574 INFO             PET0  user2_run: OpenMP thread:           3  on SSIPE:            3  Testing data for localDe =           3  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220320 062521.574 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220320 062521.574 INFO             PET0  user2_run: OpenMP thread:           5  on SSIPE:            5  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220320 062522.251 INFO             PET0  user2_run: OpenMP thread:           5  on SSIPE:            5  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220320 062522.251 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220320 062522.251 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220320 062522.251 INFO             PET0  user2_run: OpenMP thread:           4  on SSIPE:            4  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220320 062522.251 INFO             PET0  user2_run: OpenMP thread:           3  on SSIPE:            3  Testing data for localDe =           3  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220320 062522.251 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220320 062522.924 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220320 062522.924 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220320 062522.924 INFO             PET0  user2_run: OpenMP thread:           4  on SSIPE:            4  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220320 062522.924 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220320 062522.924 INFO             PET0  user2_run: OpenMP thread:           3  on SSIPE:            3  Testing data for localDe =           3  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220320 062522.924 INFO             PET0  user2_run: OpenMP thread:           5  on SSIPE:            5  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220320 062523.597 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220320 062523.597 INFO             PET0  user2_run: OpenMP thread:           4  on SSIPE:            4  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220320 062523.597 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220320 062523.597 INFO             PET0  user2_run: OpenMP thread:           3  on SSIPE:            3  Testing data for localDe =           3  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220320 062523.597 INFO             PET0  user2_run: OpenMP thread:           5  on SSIPE:            5  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220320 062523.597 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220320 062524.267 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:            2  Testing data for localDe =           2  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20220320 062524.267 INFO             PET0  user2_run: OpenMP thread:           4  on SSIPE:            4  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20220320 062524.267 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:            0  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20220320 062524.267 INFO             PET0  user2_run: OpenMP thread:           3  on SSIPE:            3  Testing data for localDe =           3  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20220320 062524.267 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:            1  Testing data for localDe =           1  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20220320 062524.267 INFO             PET0  user2_run: OpenMP thread:           5  on SSIPE:            5  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20220320 062524.938 INFO             PET0  user2_run: All data correct.
20220320 062524.938 INFO             PET0 Exiting 'user2_run'
20220320 062525.032 INFO             PET0  NUMBER_OF_PROCESSORS           6
20220320 062525.032 INFO             PET0  PASS  System Test ESMF_FieldSharedDeSSISTest, ESMF_FieldSharedDeSSISTest.F90, line 276
20220320 062525.032 INFO             PET0 Finalizing ESMF
20220320 062518.030 INFO             PET1 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220320 062518.030 INFO             PET1 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220320 062518.030 INFO             PET1 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220320 062518.030 INFO             PET1 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220320 062518.030 INFO             PET1 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220320 062518.030 INFO             PET1 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220320 062518.030 INFO             PET1 Running with ESMF Version   : v8.3.0b09-102-gabaf3ef548
20220320 062518.030 INFO             PET1 ESMF library build date/time: "Mar 20 2022" "06:03:36"
20220320 062518.030 INFO             PET1 ESMF library build location : /glade/scratch/rlong/esmf-testing/gfortran_10.1.0_mpt_O_jedwards_pio_update2
20220320 062518.030 INFO             PET1 ESMF_COMM                   : mpt
20220320 062518.030 INFO             PET1 ESMF_MOAB                   : enabled
20220320 062518.030 INFO             PET1 ESMF_LAPACK                 : enabled
20220320 062518.030 INFO             PET1 ESMF_NETCDF                 : enabled
20220320 062518.030 INFO             PET1 ESMF_PNETCDF                : disabled
20220320 062518.030 INFO             PET1 ESMF_PIO                    : enabled
20220320 062518.030 INFO             PET1 ESMF_YAMLCPP                : enabled
20220320 062518.030 INFO             PET1 --- VMK::logSystem() start -------------------------------
20220320 062518.030 INFO             PET1 esmfComm=mpt
20220320 062518.030 INFO             PET1 isPthreadsEnabled=1
20220320 062518.030 INFO             PET1 isOpenMPEnabled=1
20220320 062518.030 INFO             PET1 isOpenACCEnabled=0
20220320 062518.030 INFO             PET1 isSsiSharedMemoryEnabled=1
20220320 062518.030 INFO             PET1 ssiCount=1 peCount=6
20220320 062518.030 INFO             PET1 PE=0 SSI=0 SSIPE=0
20220320 062518.030 INFO             PET1 PE=1 SSI=0 SSIPE=1
20220320 062518.030 INFO             PET1 PE=2 SSI=0 SSIPE=2
20220320 062518.030 INFO             PET1 PE=3 SSI=0 SSIPE=3
20220320 062518.030 INFO             PET1 PE=4 SSI=0 SSIPE=4
20220320 062518.030 INFO             PET1 PE=5 SSI=0 SSIPE=5
20220320 062518.030 INFO             PET1 --- VMK::logSystem() MPI Control Variables ---------------
20220320 062518.030 INFO             PET1 index=   0                                        MPI_ADJUST_ALLGATHER : Algorithm for MPI_Allgather
20220320 062518.030 INFO             PET1 index=   1                                       MPI_ADJUST_ALLGATHERV : Algorithm for MPI_Allgatherv
20220320 062518.030 INFO             PET1 index=   2                                        MPI_ADJUST_ALLREDUCE : Algorithm for MPI_Allreduce
20220320 062518.030 INFO             PET1 index=   3                                         MPI_ADJUST_ALLTOALL : Algorithm for MPI_Alltoall
20220320 062518.030 INFO             PET1 index=   4                                        MPI_ADJUST_ALLTOALLV : Algorithm for MPI_Alltoallv
20220320 062518.030 INFO             PET1 index=   5                                        MPI_ADJUST_ALLTOALLW : Algorithm for MPI_Alltoallw
20220320 062518.030 INFO             PET1 index=   6                                          MPI_ADJUST_BARRIER : Algorithm for MPI_Barrier
20220320 062518.030 INFO             PET1 index=   7                                            MPI_ADJUST_BCAST : Algorithm for MPI_Bcast
20220320 062518.030 INFO             PET1 index=   8                                           MPI_ADJUST_EXSCAN : Algorithm for MPI_Exscan
20220320 062518.030 INFO             PET1 index=   9                                           MPI_ADJUST_GATHER : Algorithm for MPI_Gather
20220320 062518.031 INFO             PET1 index=  10                                          MPI_ADJUST_GATHERV : Algorithm for MPI_Gatherv
20220320 062518.034 INFO             PET1 index=  11                                   MPI_ADJUST_REDUCE_SCATTER : Algorithm for MPI_Reduce_scatter
20220320 062518.034 INFO             PET1 index=  12                                           MPI_ADJUST_REDUCE : Algorithm for MPI_Reduce
20220320 062518.034 INFO             PET1 index=  13                                             MPI_ADJUST_SCAN : Algorithm for MPI_Scan
20220320 062518.034 INFO             PET1 index=  14                                          MPI_ADJUST_SCATTER : Algorithm for MPI_Scatter
20220320 062518.034 INFO             PET1 index=  15                                         MPI_ADJUST_SCATTERV : Algorithm for MPI_Scatterv
20220320 062518.034 INFO             PET1 index=  16                                                   MPI_ARRAY : Alternative array name for communicating with Array Services
20220320 062518.034 INFO             PET1 index=  17                                          MPI_ASYNC_PROGRESS : Enables asynchronous progress of non-blocking operations
20220320 062518.034 INFO             PET1 index=  18                                              MPI_BUFFER_MAX : Threshold for blocking message size resulting in the use of zero-copy
20220320 062518.034 INFO             PET1 index=  19                                              MPI_BUFS_LIMIT : Max number of internal buffers a single transfer can use
20220320 062518.034 INFO             PET1 index=  20                                           MPI_BUFS_PER_PROC : Number of private message buffers per process for send and receive of long messages
20220320 062518.034 INFO             PET1 index=  21                                              MPI_CHECK_ARGS : Enables checking of MPI function arguments
20220320 062518.034 INFO             PET1 index=  22                                             MPI_CLOCKSOURCE : Time sources to use to support the MPI_Wtime() and MPI_Wtick() functions
20220320 062518.034 INFO             PET1 index=  23                                           MPI_KCOPY_ENABLED : Uses the Linux kernel to do large transfers between local ranks
20220320 062518.034 INFO             PET1 index=  24                                           MPI_COLL_A2A_FRAG : Average send size up to which MPT will use optimizations in MPI_Alltoall() and its variants
20220320 062518.034 INFO             PET1 index=  25                                        MPI_COLL_CLUSTER_OPT : Controls whether node leaders are used to accelerate collectives
20220320 062518.034 INFO             PET1 index=  26                                            MPI_COLL_GATHERV : Average send size up to which MPT will use optimizations in MPI_Gatherv()
20220320 062518.034 INFO             PET1 index=  27                                            MPI_COLL_LEADERS : Controls whether MPT uses enhanced node leaders to improve performance of collectives
20220320 062518.034 INFO             PET1 index=  28                                     MPI_COLL_NUMA_THRESHOLD : Number of NUMA nodes per host beyond which MPT will enable special NUMA performance optimizations for collective operations
20220320 062518.034 INFO             PET1 index=  29                                                MPI_COLL_OPT : Controls whether optimized collectives are enabled
20220320 062518.034 INFO             PET1 index=  30                                        MPI_COLL_OPT_VERBOSE : Enables display of optimized collective communication algorithms that are active for every communicator
20220320 062518.034 INFO             PET1 index=  31                                             MPI_COLL_PREREG : Enables pre-registration of data buffers for collectives with IB Region cache for potential amortization across subsequent calls
20220320 062518.034 INFO             PET1 index=  32                                         MPI_COLL_RED_RB_MIN : Threshold under which a doubling Allreduce is used
20220320 062518.034 INFO             PET1 index=  33                                       MPI_COLL_REPRODUCIBLE : Controls whether MPT does reductions in a reproducible manner to avoid ordering issues with floating point calculations
20220320 062518.034 INFO             PET1 index=  34                                               MPI_COLL_SYNC : Enables usage of collective algorithms suitable for tighlty synchronized applications
20220320 062518.034 INFO             PET1 index=  35                                                MPI_COMM_MAX : Max number of communicators that can be used in an MPI program
20220320 062518.034 INFO             PET1 index=  36                                        MPI_CONTEXT_MULTIPLE : Controls whether MPT will attempt to allocate separate network resources for each MPI Communicator that is created
20220320 062518.034 INFO             PET1 index=  37                                                MPI_COREDUMP : Controls which ranks of an MPI job can dump core on receipt of a core-dumping signal
20220320 062518.034 INFO             PET1 index=  38                                       MPI_COREDUMP_DEBUGGER : Controls the debugger MPT uses to display stacktraces of ranks upon receipt of a core-dumping signal
20220320 062518.034 INFO             PET1 index=  39                                         MPI_CUDA_BUFFER_MAX : Max message size that controls whether MPT uses default message buffering or CUDA IPC to do zero-copy transfer
20220320 062518.034 INFO             PET1 index=  40                          MPI_DEFAULT_SINGLE_COPY_BUFFER_MAX : Controls zero-copy message criteria for shared memory
20220320 062518.034 INFO             PET1 index=  41                                 MPI_DEFAULT_SINGLE_COPY_OFF : Enables buffering always, even for messages that qualify for single copy
20220320 062518.034 INFO             PET1 index=  42                                                     MPI_DIR : Sets the working directory on a host when using Array Services
20220320 062518.034 INFO             PET1 index=  43                                        MPI_DISPLAY_SETTINGS : Causes MPT to display the default and current settings of the environment variables controlling it
20220320 062518.034 INFO             PET1 index=  44                                             MPI_DSM_CPULIST : Specifies a list of CPUs on which to run an MPI application
20220320 062518.034 INFO             PET1 index=  45                                          MPI_DSM_DISTRIBUTE : Enables NUMA-aware CPU pinning of MPI Ranks
20220320 062518.034 INFO             PET1 index=  46                                                 MPI_DSM_OFF : Turns off nonuniform memory access (NUMA) optimization in the MPI library
20220320 062518.034 INFO             PET1 index=  47                                             MPI_DSM_VERBOSE : Causes MPT to print information about process placement
20220320 062518.034 INFO             PET1 index=  48                                            MPI_GATHER_RANKS : Causes MPT to attempt to collect all the workers on a host under a single shepherd,
20220320 062518.034 INFO             PET1 index=  49                                     MPI_GRAPH_OPTIMIZATIONS : Controls the number of optimization loops for MPI_Graph_create
20220320 062518.034 INFO             PET1 index=  50                                               MPI_GROUP_MAX : Determines the maximum number of groups that can simultaneously exist for any single MPI process
20220320 062518.034 INFO             PET1 index=  51                                               MPI_INIT_LATE : If set and MPT is launching under SLURM or MPI_SHEPHERD mode, then MPT will delay all initialization until MPI_Init() is called
20220320 062518.034 INFO             PET1 index=  52                                          MPI_LAUNCH_TIMEOUT : Number of seconds to wait before timing out when starting up remote jobs
20220320 062518.034 INFO             PET1 index=  53                                        MPI_MAPPED_HEAP_SIZE : Max amount of heap in bytes per MPI process that is memory mapped between processes under the same shepherd
20220320 062518.034 INFO             PET1 index=  54                                       MPI_MAPPED_STACK_SIZE : Amount of stack in bytes that is memory mapped per MPI process
20220320 062518.034 INFO             PET1 index=  55                                               MPI_MEM_ALIGN : Alignment for memory allocations to help eliminate precision errors related to special CPU instructions for reductions and floating point calculations
20220320 062518.034 INFO             PET1 index=  56                                              MPI_MEMMAP_OFF : Turns off the memory mapping feature used in zero-copy transfers and MPI one-sided communication
20220320 062518.034 INFO             PET1 index=  57                                           MPI_MEMORY_REPORT : Specifies whether to print information about MPT's internal memory usage
20220320 062518.034 INFO             PET1 index=  58                                      MPI_MEMORY_REPORT_FILE : If set this is the file to print memory report information to
20220320 062518.034 INFO             PET1 index=  59                                                 MPI_MSG_MEM : Controls the total amount of memory used by MPT on temporary message headers
20220320 062518.034 INFO             PET1 index=  60                                                     MPI_NAP : Controls the way in which ranks wait for events to occur
20220320 062518.034 INFO             PET1 index=  61                                              MPI_NUM_QUICKS : Limits the number of outstanding quick RDMA transfers per rank
20220320 062518.034 INFO             PET1 index=  62                                         MPI_OMP_NUM_THREADS : Represents the value of the OMP_NUM_THREADS environment variable for each host-program specification on the mpirun command line
20220320 062518.034 INFO             PET1 index=  63                                          MPI_OPENMP_INTEROP : Causes the placement of MPI processes to better accommodate the OpenMP threads associated with each process
20220320 062518.034 INFO             PET1 index=  64                                                    MPI_PMIX : Causes MPT to use the PMIx interface instead of PMI2 when working with SLURM
20220320 062518.034 INFO             PET1 index=  65                                           MPI_PREFAULT_HEAP : Amount of xpmem-attached memory that should be prefaulted
20220320 062518.034 INFO             PET1 index=  66                                               MPI_QUERYABLE : Enables use of the mpt_query command to query a MPI job about its activities and statistics
20220320 062518.034 INFO             PET1 index=  67                                           MPI_REQUEST_DEBUG : Enables extra checking of the use of MPI requests
20220320 062518.034 INFO             PET1 index=  68                                             MPI_REQUEST_MAX : Determines the number of MPI_Requests preallocated by MPT to track simultaneous nonblocking sends and receives within each MPI process
20220320 062518.034 INFO             PET1 index=  69                                              MPI_RESET_PATH : Causes MPT to set the PATH environment variable of MPI processess using a system default instead of the mpirun process environment setting
20220320 062518.034 INFO             PET1 index=  70                                                MPI_SHEPHERD : Causes MPT to use a completely separate mpt_shepherd program which does a fork+exec of each rank
20220320 062518.034 INFO             PET1 index=  71                                                 MPI_SIGTRAP : Specifies if MPT's signal handler should override any existing signal handlers for signals SIGSEGV, SIGQUIT, SIGILL, SIGABRT, SIGBUS, and SIGFPE
20220320 062518.034 INFO             PET1 index=  72                                                   MPI_STATS : Enables printing of MPI internal statistics
20220320 062518.034 INFO             PET1 index=  73                                              MPI_STATS_FILE : Name of the file that contains information from MPI_STATS and other statistics counters
20220320 062518.034 INFO             PET1 index=  74                                           MPI_STATUS_SIGNAL : Enables MPT to receive a signal which will prompt each rank to print status information to stderr
20220320 062518.034 INFO             PET1 index=  75                                          MPI_SYNC_THRESHOLD : Transfer size in bytes up to which MPT will attempt to send the data immediately in MPI_Ssend and MPI_Issend
20220320 062518.034 INFO             PET1 index=  76                                             MPI_SYSLOG_COPY : Enables messages about MPT internal errors and system failures to get copied to the system log
20220320 062518.034 INFO             PET1 index=  77                                              MPI_TYPE_DEPTH : Maximum number of nesting levels for derived data types
20220320 062518.034 INFO             PET1 index=  78                                                MPI_TYPE_MAX : Maximum number of data types that can simultaneously exist for any single MPI process
20220320 062518.034 INFO             PET1 index=  79                                        MPI_UNBUFFERED_STDIO : Disables buffering of stdio and stderr output
20220320 062518.034 INFO             PET1 index=  80                                                MPI_UNIVERSE : Additional hosts on which MPI processes may be launched with MPI_Comm_spawn and MPI_Comm_spawn_multiple functions
20220320 062518.034 INFO             PET1 index=  81                                           MPI_UNIVERSE_SIZE : Controls number of ranks when running in spawn capable mode
20220320 062518.034 INFO             PET1 index=  82                                          MPI_UNWEIGHTED_OLD : Enables pre-2.13 ABI of MPT for MPI_UNWEIGHTED
20220320 062518.034 INFO             PET1 index=  83                                                MPI_USE_CUDA : Enables CUDA support and GPU buffers for all operations
20220320 062518.034 INFO             PET1 index=  84                                             MPI_USING_VTUNE : Enables compatibility with Intel Vtune for MPT's classic SGI ABI
20220320 062518.034 INFO             PET1 index=  85                                                 MPI_VERBOSE : Enables display of information like interconnect device usage and environment variables set to non-default values
20220320 062518.034 INFO             PET1 index=  86                                                MPI_VERBOSE2 : Allows additional MPT diagnostic information to be printed in the standard output stream
20220320 062518.034 INFO             PET1 index=  87                                          MPI_WATCHDOG_TIMER : Minutes to wait before an MPI process that is unable to contact another process aborts the application
20220320 062518.034 INFO             PET1 index=  88                                               MPI_WILDCARDS : When set to false allows MPT to make some optimizations in MPI_Recv() and MPI_Probe() if MPI_ANY_SOURCE is not used
20220320 062518.034 INFO             PET1 index=  89                                                MPI_WIN_MODE : Selects RDMA capabilities of hardware v/s normal send/receive based emulation for one-sided communication
20220320 062518.034 INFO             PET1 index=  90                                               MPI_WORLD_MAP : Rank to host mapping
20220320 062518.034 INFO             PET1 index=  91                                           MPI_XPMEM_ENABLED : Determines whether HPE XPMEM is used or not
20220320 062518.034 INFO             PET1 index=  92                                            MPIO_DIRECT_READ : Enables Direct IO in ROMIO
20220320 062518.034 INFO             PET1 index=  93                                           MPIO_DIRECT_WRITE : Enables Direct IO in ROMIO
20220320 062518.034 INFO             PET1 index=  94                                 MPIO_DIRECT_READ_CHUNK_SIZE : Read chunk size when Direct IO is enabled in ROMIO
20220320 062518.034 INFO             PET1 index=  95                                MPIO_DIRECT_WRITE_CHUNK_SIZE : Write chunk size when Direct IO is enabled in ROMIO
20220320 062518.034 INFO             PET1 index=  96                                 MPIO_LUSTRE_WRITE_AGGMETHOD : Write aggregrate method for ROMIO Lustre filesystem
20220320 062518.034 INFO             PET1 index=  97                                   MPIO_LUSTRE_GCYC_MIN_ITER : Minimum number of iterations for group-cyclic aggregator
20220320 062518.034 INFO             PET1 index=  98                                    profiled_recv_request_id : identity of the request-of-interest
20220320 062518.034 INFO             PET1 --- VMK::logSystem() end ---------------------------------
20220320 062518.034 INFO             PET1 main: --- VMK::log() start -------------------------------------
20220320 062518.034 INFO             PET1 main: vm located at: 0x18a7fe0
20220320 062518.034 INFO             PET1 main: petCount=6 localPet=1 mypthid=46940161916672 currentSsiPe=1
20220320 062518.034 INFO             PET1 main: Current system level affinity pinning for local PET:
20220320 062518.034 INFO             PET1 main:  SSIPE=1
20220320 062518.034 INFO             PET1 main: Current system level OMP_NUM_THREADS setting for local PET: 36
20220320 062518.034 INFO             PET1 main: ssiCount=1 localSsi=0
20220320 062518.034 INFO             PET1 main: mpionly=1 threadsflag=0
20220320 062518.034 INFO             PET1 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220320 062518.034 INFO             PET1 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220320 062518.034 INFO             PET1 main:  PE=0 SSI=0 SSIPE=0
20220320 062518.034 INFO             PET1 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220320 062518.034 INFO             PET1 main:  PE=1 SSI=0 SSIPE=1
20220320 062518.034 INFO             PET1 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220320 062518.034 INFO             PET1 main:  PE=2 SSI=0 SSIPE=2
20220320 062518.034 INFO             PET1 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220320 062518.034 INFO             PET1 main:  PE=3 SSI=0 SSIPE=3
20220320 062518.034 INFO             PET1 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220320 062518.034 INFO             PET1 main:  PE=4 SSI=0 SSIPE=4
20220320 062518.034 INFO             PET1 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220320 062518.034 INFO             PET1 main:  PE=5 SSI=0 SSIPE=5
20220320 062518.034 INFO             PET1 main: --- VMK::log() end ---------------------------------------
20220320 062518.035 INFO             PET1 Executing 'userm1_setvm'
20220320 062518.035 INFO             PET1 Executing 'userm1_register'
20220320 062518.035 INFO             PET1 Executing 'userm2_setvm'
20220320 062518.035 DEBUG            PET1 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220320 062518.035 DEBUG            PET1 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220320 062518.035 INFO             PET1 model1: --- VMK::log() start -------------------------------------
20220320 062518.035 INFO             PET1 model1: vm located at: 0x292b6e0
20220320 062518.035 INFO             PET1 model1: petCount=6 localPet=1 mypthid=46940161916672 currentSsiPe=1
20220320 062518.035 INFO             PET1 model1: Current system level affinity pinning for local PET:
20220320 062518.035 INFO             PET1 model1:  SSIPE=1
20220320 062518.035 INFO             PET1 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220320 062518.035 INFO             PET1 model1: ssiCount=1 localSsi=0
20220320 062518.035 INFO             PET1 model1: mpionly=1 threadsflag=0
20220320 062518.035 INFO             PET1 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220320 062518.035 INFO             PET1 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220320 062518.035 INFO             PET1 model1:  PE=0 SSI=0 SSIPE=0
20220320 062518.035 INFO             PET1 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220320 062518.035 INFO             PET1 model1:  PE=1 SSI=0 SSIPE=1
20220320 062518.035 INFO             PET1 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220320 062518.035 INFO             PET1 model1:  PE=2 SSI=0 SSIPE=2
20220320 062518.035 INFO             PET1 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220320 062518.035 INFO             PET1 model1:  PE=3 SSI=0 SSIPE=3
20220320 062518.036 INFO             PET1 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220320 062518.036 INFO             PET1 model1:  PE=4 SSI=0 SSIPE=4
20220320 062518.036 INFO             PET1 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220320 062518.036 INFO             PET1 model1:  PE=5 SSI=0 SSIPE=5
20220320 062518.036 INFO             PET1 model1: --- VMK::log() end ---------------------------------------
20220320 062518.038 INFO             PET1 Entering 'user1_run'
20220320 062518.038 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220320 062518.814 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220320 062519.501 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220320 062520.188 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220320 062520.873 INFO             PET1  user1_run: on SSIPE:            1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20220320 062521.557 INFO             PET1 Exiting 'user1_run'
20220320 062525.032 INFO             PET1  NUMBER_OF_PROCESSORS           6
20220320 062525.033 INFO             PET1  PASS  System Test ESMF_FieldSharedDeSSISTest, ESMF_FieldSharedDeSSISTest.F90, line 276
20220320 062525.033 INFO             PET1 Finalizing ESMF
20220320 062518.027 INFO             PET2 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220320 062518.028 INFO             PET2 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220320 062518.028 INFO             PET2 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220320 062518.028 INFO             PET2 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220320 062518.028 INFO             PET2 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220320 062518.028 INFO             PET2 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220320 062518.028 INFO             PET2 Running with ESMF Version   : v8.3.0b09-102-gabaf3ef548
20220320 062518.028 INFO             PET2 ESMF library build date/time: "Mar 20 2022" "06:03:36"
20220320 062518.028 INFO             PET2 ESMF library build location : /glade/scratch/rlong/esmf-testing/gfortran_10.1.0_mpt_O_jedwards_pio_update2
20220320 062518.028 INFO             PET2 ESMF_COMM                   : mpt
20220320 062518.028 INFO             PET2 ESMF_MOAB                   : enabled
20220320 062518.028 INFO             PET2 ESMF_LAPACK                 : enabled
20220320 062518.028 INFO             PET2 ESMF_NETCDF                 : enabled
20220320 062518.028 INFO             PET2 ESMF_PNETCDF                : disabled
20220320 062518.028 INFO             PET2 ESMF_PIO                    : enabled
20220320 062518.028 INFO             PET2 ESMF_YAMLCPP                : enabled
20220320 062518.030 INFO             PET2 --- VMK::logSystem() start -------------------------------
20220320 062518.030 INFO             PET2 esmfComm=mpt
20220320 062518.030 INFO             PET2 isPthreadsEnabled=1
20220320 062518.030 INFO             PET2 isOpenMPEnabled=1
20220320 062518.030 INFO             PET2 isOpenACCEnabled=0
20220320 062518.030 INFO             PET2 isSsiSharedMemoryEnabled=1
20220320 062518.030 INFO             PET2 ssiCount=1 peCount=6
20220320 062518.030 INFO             PET2 PE=0 SSI=0 SSIPE=0
20220320 062518.030 INFO             PET2 PE=1 SSI=0 SSIPE=1
20220320 062518.030 INFO             PET2 PE=2 SSI=0 SSIPE=2
20220320 062518.030 INFO             PET2 PE=3 SSI=0 SSIPE=3
20220320 062518.030 INFO             PET2 PE=4 SSI=0 SSIPE=4
20220320 062518.030 INFO             PET2 PE=5 SSI=0 SSIPE=5
20220320 062518.030 INFO             PET2 --- VMK::logSystem() MPI Control Variables ---------------
20220320 062518.030 INFO             PET2 index=   0                                        MPI_ADJUST_ALLGATHER : Algorithm for MPI_Allgather
20220320 062518.030 INFO             PET2 index=   1                                       MPI_ADJUST_ALLGATHERV : Algorithm for MPI_Allgatherv
20220320 062518.030 INFO             PET2 index=   2                                        MPI_ADJUST_ALLREDUCE : Algorithm for MPI_Allreduce
20220320 062518.030 INFO             PET2 index=   3                                         MPI_ADJUST_ALLTOALL : Algorithm for MPI_Alltoall
20220320 062518.030 INFO             PET2 index=   4                                        MPI_ADJUST_ALLTOALLV : Algorithm for MPI_Alltoallv
20220320 062518.030 INFO             PET2 index=   5                                        MPI_ADJUST_ALLTOALLW : Algorithm for MPI_Alltoallw
20220320 062518.030 INFO             PET2 index=   6                                          MPI_ADJUST_BARRIER : Algorithm for MPI_Barrier
20220320 062518.030 INFO             PET2 index=   7                                            MPI_ADJUST_BCAST : Algorithm for MPI_Bcast
20220320 062518.030 INFO             PET2 index=   8                                           MPI_ADJUST_EXSCAN : Algorithm for MPI_Exscan
20220320 062518.030 INFO             PET2 index=   9                                           MPI_ADJUST_GATHER : Algorithm for MPI_Gather
20220320 062518.031 INFO             PET2 index=  10                                          MPI_ADJUST_GATHERV : Algorithm for MPI_Gatherv
20220320 062518.032 INFO             PET2 index=  11                                   MPI_ADJUST_REDUCE_SCATTER : Algorithm for MPI_Reduce_scatter
20220320 062518.032 INFO             PET2 index=  12                                           MPI_ADJUST_REDUCE : Algorithm for MPI_Reduce
20220320 062518.032 INFO             PET2 index=  13                                             MPI_ADJUST_SCAN : Algorithm for MPI_Scan
20220320 062518.032 INFO             PET2 index=  14                                          MPI_ADJUST_SCATTER : Algorithm for MPI_Scatter
20220320 062518.032 INFO             PET2 index=  15                                         MPI_ADJUST_SCATTERV : Algorithm for MPI_Scatterv
20220320 062518.032 INFO             PET2 index=  16                                                   MPI_ARRAY : Alternative array name for communicating with Array Services
20220320 062518.032 INFO             PET2 index=  17                                          MPI_ASYNC_PROGRESS : Enables asynchronous progress of non-blocking operations
20220320 062518.032 INFO             PET2 index=  18                                              MPI_BUFFER_MAX : Threshold for blocking message size resulting in the use of zero-copy
20220320 062518.032 INFO             PET2 index=  19                                              MPI_BUFS_LIMIT : Max number of internal buffers a single transfer can use
20220320 062518.032 INFO             PET2 index=  20                                           MPI_BUFS_PER_PROC : Number of private message buffers per process for send and receive of long messages
20220320 062518.032 INFO             PET2 index=  21                                              MPI_CHECK_ARGS : Enables checking of MPI function arguments
20220320 062518.032 INFO             PET2 index=  22                                             MPI_CLOCKSOURCE : Time sources to use to support the MPI_Wtime() and MPI_Wtick() functions
20220320 062518.032 INFO             PET2 index=  23                                           MPI_KCOPY_ENABLED : Uses the Linux kernel to do large transfers between local ranks
20220320 062518.032 INFO             PET2 index=  24                                           MPI_COLL_A2A_FRAG : Average send size up to which MPT will use optimizations in MPI_Alltoall() and its variants
20220320 062518.032 INFO             PET2 index=  25                                        MPI_COLL_CLUSTER_OPT : Controls whether node leaders are used to accelerate collectives
20220320 062518.032 INFO             PET2 index=  26                                            MPI_COLL_GATHERV : Average send size up to which MPT will use optimizations in MPI_Gatherv()
20220320 062518.032 INFO             PET2 index=  27                                            MPI_COLL_LEADERS : Controls whether MPT uses enhanced node leaders to improve performance of collectives
20220320 062518.032 INFO             PET2 index=  28                                     MPI_COLL_NUMA_THRESHOLD : Number of NUMA nodes per host beyond which MPT will enable special NUMA performance optimizations for collective operations
20220320 062518.032 INFO             PET2 index=  29                                                MPI_COLL_OPT : Controls whether optimized collectives are enabled
20220320 062518.032 INFO             PET2 index=  30                                        MPI_COLL_OPT_VERBOSE : Enables display of optimized collective communication algorithms that are active for every communicator
20220320 062518.032 INFO             PET2 index=  31                                             MPI_COLL_PREREG : Enables pre-registration of data buffers for collectives with IB Region cache for potential amortization across subsequent calls
20220320 062518.032 INFO             PET2 index=  32                                         MPI_COLL_RED_RB_MIN : Threshold under which a doubling Allreduce is used
20220320 062518.032 INFO             PET2 index=  33                                       MPI_COLL_REPRODUCIBLE : Controls whether MPT does reductions in a reproducible manner to avoid ordering issues with floating point calculations
20220320 062518.032 INFO             PET2 index=  34                                               MPI_COLL_SYNC : Enables usage of collective algorithms suitable for tighlty synchronized applications
20220320 062518.032 INFO             PET2 index=  35                                                MPI_COMM_MAX : Max number of communicators that can be used in an MPI program
20220320 062518.032 INFO             PET2 index=  36                                        MPI_CONTEXT_MULTIPLE : Controls whether MPT will attempt to allocate separate network resources for each MPI Communicator that is created
20220320 062518.032 INFO             PET2 index=  37                                                MPI_COREDUMP : Controls which ranks of an MPI job can dump core on receipt of a core-dumping signal
20220320 062518.032 INFO             PET2 index=  38                                       MPI_COREDUMP_DEBUGGER : Controls the debugger MPT uses to display stacktraces of ranks upon receipt of a core-dumping signal
20220320 062518.032 INFO             PET2 index=  39                                         MPI_CUDA_BUFFER_MAX : Max message size that controls whether MPT uses default message buffering or CUDA IPC to do zero-copy transfer
20220320 062518.032 INFO             PET2 index=  40                          MPI_DEFAULT_SINGLE_COPY_BUFFER_MAX : Controls zero-copy message criteria for shared memory
20220320 062518.032 INFO             PET2 index=  41                                 MPI_DEFAULT_SINGLE_COPY_OFF : Enables buffering always, even for messages that qualify for single copy
20220320 062518.032 INFO             PET2 index=  42                                                     MPI_DIR : Sets the working directory on a host when using Array Services
20220320 062518.032 INFO             PET2 index=  43                                        MPI_DISPLAY_SETTINGS : Causes MPT to display the default and current settings of the environment variables controlling it
20220320 062518.032 INFO             PET2 index=  44                                             MPI_DSM_CPULIST : Specifies a list of CPUs on which to run an MPI application
20220320 062518.032 INFO             PET2 index=  45                                          MPI_DSM_DISTRIBUTE : Enables NUMA-aware CPU pinning of MPI Ranks
20220320 062518.032 INFO             PET2 index=  46                                                 MPI_DSM_OFF : Turns off nonuniform memory access (NUMA) optimization in the MPI library
20220320 062518.032 INFO             PET2 index=  47                                             MPI_DSM_VERBOSE : Causes MPT to print information about process placement
20220320 062518.032 INFO             PET2 index=  48                                            MPI_GATHER_RANKS : Causes MPT to attempt to collect all the workers on a host under a single shepherd,
20220320 062518.033 INFO             PET2 index=  49                                     MPI_GRAPH_OPTIMIZATIONS : Controls the number of optimization loops for MPI_Graph_create
20220320 062518.033 INFO             PET2 index=  50                                               MPI_GROUP_MAX : Determines the maximum number of groups that can simultaneously exist for any single MPI process
20220320 062518.033 INFO             PET2 index=  51                                               MPI_INIT_LATE : If set and MPT is launching under SLURM or MPI_SHEPHERD mode, then MPT will delay all initialization until MPI_Init() is called
20220320 062518.033 INFO             PET2 index=  52                                          MPI_LAUNCH_TIMEOUT : Number of seconds to wait before timing out when starting up remote jobs
20220320 062518.033 INFO             PET2 index=  53                                        MPI_MAPPED_HEAP_SIZE : Max amount of heap in bytes per MPI process that is memory mapped between processes under the same shepherd
20220320 062518.033 INFO             PET2 index=  54                                       MPI_MAPPED_STACK_SIZE : Amount of stack in bytes that is memory mapped per MPI process
20220320 062518.033 INFO             PET2 index=  55                                               MPI_MEM_ALIGN : Alignment for memory allocations to help eliminate precision errors related to special CPU instructions for reductions and floating point calculations
20220320 062518.033 INFO             PET2 index=  56                                              MPI_MEMMAP_OFF : Turns off the memory mapping feature used in zero-copy transfers and MPI one-sided communication
20220320 062518.033 INFO             PET2 index=  57                                           MPI_MEMORY_REPORT : Specifies whether to print information about MPT's internal memory usage
20220320 062518.033 INFO             PET2 index=  58                                      MPI_MEMORY_REPORT_FILE : If set this is the file to print memory report information to
20220320 062518.033 INFO             PET2 index=  59                                                 MPI_MSG_MEM : Controls the total amount of memory used by MPT on temporary message headers
20220320 062518.033 INFO             PET2 index=  60                                                     MPI_NAP : Controls the way in which ranks wait for events to occur
20220320 062518.033 INFO             PET2 index=  61                                              MPI_NUM_QUICKS : Limits the number of outstanding quick RDMA transfers per rank
20220320 062518.033 INFO             PET2 index=  62                                         MPI_OMP_NUM_THREADS : Represents the value of the OMP_NUM_THREADS environment variable for each host-program specification on the mpirun command line
20220320 062518.033 INFO             PET2 index=  63                                          MPI_OPENMP_INTEROP : Causes the placement of MPI processes to better accommodate the OpenMP threads associated with each process
20220320 062518.033 INFO             PET2 index=  64                                                    MPI_PMIX : Causes MPT to use the PMIx interface instead of PMI2 when working with SLURM
20220320 062518.033 INFO             PET2 index=  65                                           MPI_PREFAULT_HEAP : Amount of xpmem-attached memory that should be prefaulted
20220320 062518.033 INFO             PET2 index=  66                                               MPI_QUERYABLE : Enables use of the mpt_query command to query a MPI job about its activities and statistics
20220320 062518.033 INFO             PET2 index=  67                                           MPI_REQUEST_DEBUG : Enables extra checking of the use of MPI requests
20220320 062518.033 INFO             PET2 index=  68                                             MPI_REQUEST_MAX : Determines the number of MPI_Requests preallocated by MPT to track simultaneous nonblocking sends and receives within each MPI process
20220320 062518.033 INFO             PET2 index=  69                                              MPI_RESET_PATH : Causes MPT to set the PATH environment variable of MPI processess using a system default instead of the mpirun process environment setting
20220320 062518.033 INFO             PET2 index=  70                                                MPI_SHEPHERD : Causes MPT to use a completely separate mpt_shepherd program which does a fork+exec of each rank
20220320 062518.033 INFO             PET2 index=  71                                                 MPI_SIGTRAP : Specifies if MPT's signal handler should override any existing signal handlers for signals SIGSEGV, SIGQUIT, SIGILL, SIGABRT, SIGBUS, and SIGFPE
20220320 062518.033 INFO             PET2 index=  72                                                   MPI_STATS : Enables printing of MPI internal statistics
20220320 062518.033 INFO             PET2 index=  73                                              MPI_STATS_FILE : Name of the file that contains information from MPI_STATS and other statistics counters
20220320 062518.033 INFO             PET2 index=  74                                           MPI_STATUS_SIGNAL : Enables MPT to receive a signal which will prompt each rank to print status information to stderr
20220320 062518.033 INFO             PET2 index=  75                                          MPI_SYNC_THRESHOLD : Transfer size in bytes up to which MPT will attempt to send the data immediately in MPI_Ssend and MPI_Issend
20220320 062518.033 INFO             PET2 index=  76                                             MPI_SYSLOG_COPY : Enables messages about MPT internal errors and system failures to get copied to the system log
20220320 062518.033 INFO             PET2 index=  77                                              MPI_TYPE_DEPTH : Maximum number of nesting levels for derived data types
20220320 062518.033 INFO             PET2 index=  78                                                MPI_TYPE_MAX : Maximum number of data types that can simultaneously exist for any single MPI process
20220320 062518.033 INFO             PET2 index=  79                                        MPI_UNBUFFERED_STDIO : Disables buffering of stdio and stderr output
20220320 062518.033 INFO             PET2 index=  80                                                MPI_UNIVERSE : Additional hosts on which MPI processes may be launched with MPI_Comm_spawn and MPI_Comm_spawn_multiple functions
20220320 062518.033 INFO             PET2 index=  81                                           MPI_UNIVERSE_SIZE : Controls number of ranks when running in spawn capable mode
20220320 062518.033 INFO             PET2 index=  82                                          MPI_UNWEIGHTED_OLD : Enables pre-2.13 ABI of MPT for MPI_UNWEIGHTED
20220320 062518.033 INFO             PET2 index=  83                                                MPI_USE_CUDA : Enables CUDA support and GPU buffers for all operations
20220320 062518.033 INFO             PET2 index=  84                                             MPI_USING_VTUNE : Enables compatibility with Intel Vtune for MPT's classic SGI ABI
20220320 062518.033 INFO             PET2 index=  85                                                 MPI_VERBOSE : Enables display of information like interconnect device usage and environment variables set to non-default values
20220320 062518.033 INFO             PET2 index=  86                                                MPI_VERBOSE2 : Allows additional MPT diagnostic information to be printed in the standard output stream
20220320 062518.033 INFO             PET2 index=  87                                          MPI_WATCHDOG_TIMER : Minutes to wait before an MPI process that is unable to contact another process aborts the application
20220320 062518.033 INFO             PET2 index=  88                                               MPI_WILDCARDS : When set to false allows MPT to make some optimizations in MPI_Recv() and MPI_Probe() if MPI_ANY_SOURCE is not used
20220320 062518.033 INFO             PET2 index=  89                                                MPI_WIN_MODE : Selects RDMA capabilities of hardware v/s normal send/receive based emulation for one-sided communication
20220320 062518.033 INFO             PET2 index=  90                                               MPI_WORLD_MAP : Rank to host mapping
20220320 062518.033 INFO             PET2 index=  91                                           MPI_XPMEM_ENABLED : Determines whether HPE XPMEM is used or not
20220320 062518.033 INFO             PET2 index=  92                                            MPIO_DIRECT_READ : Enables Direct IO in ROMIO
20220320 062518.033 INFO             PET2 index=  93                                           MPIO_DIRECT_WRITE : Enables Direct IO in ROMIO
20220320 062518.033 INFO             PET2 index=  94                                 MPIO_DIRECT_READ_CHUNK_SIZE : Read chunk size when Direct IO is enabled in ROMIO
20220320 062518.033 INFO             PET2 index=  95                                MPIO_DIRECT_WRITE_CHUNK_SIZE : Write chunk size when Direct IO is enabled in ROMIO
20220320 062518.033 INFO             PET2 index=  96                                 MPIO_LUSTRE_WRITE_AGGMETHOD : Write aggregrate method for ROMIO Lustre filesystem
20220320 062518.033 INFO             PET2 index=  97                                   MPIO_LUSTRE_GCYC_MIN_ITER : Minimum number of iterations for group-cyclic aggregator
20220320 062518.033 INFO             PET2 index=  98                                    profiled_recv_request_id : identity of the request-of-interest
20220320 062518.033 INFO             PET2 --- VMK::logSystem() end ---------------------------------
20220320 062518.033 INFO             PET2 main: --- VMK::log() start -------------------------------------
20220320 062518.033 INFO             PET2 main: vm located at: 0x18a7fe0
20220320 062518.033 INFO             PET2 main: petCount=6 localPet=2 mypthid=46940161916672 currentSsiPe=2
20220320 062518.033 INFO             PET2 main: Current system level affinity pinning for local PET:
20220320 062518.033 INFO             PET2 main:  SSIPE=2
20220320 062518.033 INFO             PET2 main: Current system level OMP_NUM_THREADS setting for local PET: 36
20220320 062518.033 INFO             PET2 main: ssiCount=1 localSsi=0
20220320 062518.033 INFO             PET2 main: mpionly=1 threadsflag=0
20220320 062518.033 INFO             PET2 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220320 062518.033 INFO             PET2 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220320 062518.033 INFO             PET2 main:  PE=0 SSI=0 SSIPE=0
20220320 062518.033 INFO             PET2 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220320 062518.033 INFO             PET2 main:  PE=1 SSI=0 SSIPE=1
20220320 062518.033 INFO             PET2 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220320 062518.033 INFO             PET2 main:  PE=2 SSI=0 SSIPE=2
20220320 062518.033 INFO             PET2 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220320 062518.033 INFO             PET2 main:  PE=3 SSI=0 SSIPE=3
20220320 062518.033 INFO             PET2 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220320 062518.033 INFO             PET2 main:  PE=4 SSI=0 SSIPE=4
20220320 062518.033 INFO             PET2 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220320 062518.033 INFO             PET2 main:  PE=5 SSI=0 SSIPE=5
20220320 062518.033 INFO             PET2 main: --- VMK::log() end ---------------------------------------
20220320 062518.035 INFO             PET2 Executing 'userm1_setvm'
20220320 062518.035 INFO             PET2 Executing 'userm1_register'
20220320 062518.035 INFO             PET2 Executing 'userm2_setvm'
20220320 062518.035 DEBUG            PET2 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220320 062518.035 DEBUG            PET2 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220320 062518.035 INFO             PET2 model1: --- VMK::log() start -------------------------------------
20220320 062518.035 INFO             PET2 model1: vm located at: 0x292c7c0
20220320 062518.035 INFO             PET2 model1: petCount=6 localPet=2 mypthid=46940161916672 currentSsiPe=2
20220320 062518.035 INFO             PET2 model1: Current system level affinity pinning for local PET:
20220320 062518.035 INFO             PET2 model1:  SSIPE=2
20220320 062518.035 INFO             PET2 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220320 062518.035 INFO             PET2 model1: ssiCount=1 localSsi=0
20220320 062518.035 INFO             PET2 model1: mpionly=1 threadsflag=0
20220320 062518.035 INFO             PET2 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220320 062518.035 INFO             PET2 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220320 062518.035 INFO             PET2 model1:  PE=0 SSI=0 SSIPE=0
20220320 062518.035 INFO             PET2 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220320 062518.035 INFO             PET2 model1:  PE=1 SSI=0 SSIPE=1
20220320 062518.035 INFO             PET2 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220320 062518.035 INFO             PET2 model1:  PE=2 SSI=0 SSIPE=2
20220320 062518.035 INFO             PET2 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220320 062518.036 INFO             PET2 model1:  PE=3 SSI=0 SSIPE=3
20220320 062518.036 INFO             PET2 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220320 062518.036 INFO             PET2 model1:  PE=4 SSI=0 SSIPE=4
20220320 062518.036 INFO             PET2 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220320 062518.036 INFO             PET2 model1:  PE=5 SSI=0 SSIPE=5
20220320 062518.036 INFO             PET2 model1: --- VMK::log() end ---------------------------------------
20220320 062518.038 INFO             PET2 Entering 'user1_run'
20220320 062518.038 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220320 062518.815 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220320 062519.503 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220320 062520.192 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220320 062520.880 INFO             PET2  user1_run: on SSIPE:            2  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20220320 062521.570 INFO             PET2 Exiting 'user1_run'
20220320 062525.032 INFO             PET2  NUMBER_OF_PROCESSORS           6
20220320 062525.033 INFO             PET2  PASS  System Test ESMF_FieldSharedDeSSISTest, ESMF_FieldSharedDeSSISTest.F90, line 276
20220320 062525.033 INFO             PET2 Finalizing ESMF
20220320 062518.029 INFO             PET3 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220320 062518.029 INFO             PET3 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220320 062518.029 INFO             PET3 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220320 062518.029 INFO             PET3 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220320 062518.029 INFO             PET3 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220320 062518.029 INFO             PET3 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220320 062518.029 INFO             PET3 Running with ESMF Version   : v8.3.0b09-102-gabaf3ef548
20220320 062518.029 INFO             PET3 ESMF library build date/time: "Mar 20 2022" "06:03:36"
20220320 062518.029 INFO             PET3 ESMF library build location : /glade/scratch/rlong/esmf-testing/gfortran_10.1.0_mpt_O_jedwards_pio_update2
20220320 062518.029 INFO             PET3 ESMF_COMM                   : mpt
20220320 062518.029 INFO             PET3 ESMF_MOAB                   : enabled
20220320 062518.029 INFO             PET3 ESMF_LAPACK                 : enabled
20220320 062518.029 INFO             PET3 ESMF_NETCDF                 : enabled
20220320 062518.029 INFO             PET3 ESMF_PNETCDF                : disabled
20220320 062518.029 INFO             PET3 ESMF_PIO                    : enabled
20220320 062518.029 INFO             PET3 ESMF_YAMLCPP                : enabled
20220320 062518.030 INFO             PET3 --- VMK::logSystem() start -------------------------------
20220320 062518.030 INFO             PET3 esmfComm=mpt
20220320 062518.030 INFO             PET3 isPthreadsEnabled=1
20220320 062518.030 INFO             PET3 isOpenMPEnabled=1
20220320 062518.030 INFO             PET3 isOpenACCEnabled=0
20220320 062518.030 INFO             PET3 isSsiSharedMemoryEnabled=1
20220320 062518.030 INFO             PET3 ssiCount=1 peCount=6
20220320 062518.030 INFO             PET3 PE=0 SSI=0 SSIPE=0
20220320 062518.030 INFO             PET3 PE=1 SSI=0 SSIPE=1
20220320 062518.030 INFO             PET3 PE=2 SSI=0 SSIPE=2
20220320 062518.030 INFO             PET3 PE=3 SSI=0 SSIPE=3
20220320 062518.030 INFO             PET3 PE=4 SSI=0 SSIPE=4
20220320 062518.030 INFO             PET3 PE=5 SSI=0 SSIPE=5
20220320 062518.030 INFO             PET3 --- VMK::logSystem() MPI Control Variables ---------------
20220320 062518.030 INFO             PET3 index=   0                                        MPI_ADJUST_ALLGATHER : Algorithm for MPI_Allgather
20220320 062518.030 INFO             PET3 index=   1                                       MPI_ADJUST_ALLGATHERV : Algorithm for MPI_Allgatherv
20220320 062518.030 INFO             PET3 index=   2                                        MPI_ADJUST_ALLREDUCE : Algorithm for MPI_Allreduce
20220320 062518.030 INFO             PET3 index=   3                                         MPI_ADJUST_ALLTOALL : Algorithm for MPI_Alltoall
20220320 062518.030 INFO             PET3 index=   4                                        MPI_ADJUST_ALLTOALLV : Algorithm for MPI_Alltoallv
20220320 062518.030 INFO             PET3 index=   5                                        MPI_ADJUST_ALLTOALLW : Algorithm for MPI_Alltoallw
20220320 062518.030 INFO             PET3 index=   6                                          MPI_ADJUST_BARRIER : Algorithm for MPI_Barrier
20220320 062518.030 INFO             PET3 index=   7                                            MPI_ADJUST_BCAST : Algorithm for MPI_Bcast
20220320 062518.030 INFO             PET3 index=   8                                           MPI_ADJUST_EXSCAN : Algorithm for MPI_Exscan
20220320 062518.030 INFO             PET3 index=   9                                           MPI_ADJUST_GATHER : Algorithm for MPI_Gather
20220320 062518.031 INFO             PET3 index=  10                                          MPI_ADJUST_GATHERV : Algorithm for MPI_Gatherv
20220320 062518.032 INFO             PET3 index=  11                                   MPI_ADJUST_REDUCE_SCATTER : Algorithm for MPI_Reduce_scatter
20220320 062518.032 INFO             PET3 index=  12                                           MPI_ADJUST_REDUCE : Algorithm for MPI_Reduce
20220320 062518.032 INFO             PET3 index=  13                                             MPI_ADJUST_SCAN : Algorithm for MPI_Scan
20220320 062518.032 INFO             PET3 index=  14                                          MPI_ADJUST_SCATTER : Algorithm for MPI_Scatter
20220320 062518.032 INFO             PET3 index=  15                                         MPI_ADJUST_SCATTERV : Algorithm for MPI_Scatterv
20220320 062518.032 INFO             PET3 index=  16                                                   MPI_ARRAY : Alternative array name for communicating with Array Services
20220320 062518.032 INFO             PET3 index=  17                                          MPI_ASYNC_PROGRESS : Enables asynchronous progress of non-blocking operations
20220320 062518.032 INFO             PET3 index=  18                                              MPI_BUFFER_MAX : Threshold for blocking message size resulting in the use of zero-copy
20220320 062518.032 INFO             PET3 index=  19                                              MPI_BUFS_LIMIT : Max number of internal buffers a single transfer can use
20220320 062518.032 INFO             PET3 index=  20                                           MPI_BUFS_PER_PROC : Number of private message buffers per process for send and receive of long messages
20220320 062518.032 INFO             PET3 index=  21                                              MPI_CHECK_ARGS : Enables checking of MPI function arguments
20220320 062518.032 INFO             PET3 index=  22                                             MPI_CLOCKSOURCE : Time sources to use to support the MPI_Wtime() and MPI_Wtick() functions
20220320 062518.033 INFO             PET3 index=  23                                           MPI_KCOPY_ENABLED : Uses the Linux kernel to do large transfers between local ranks
20220320 062518.033 INFO             PET3 index=  24                                           MPI_COLL_A2A_FRAG : Average send size up to which MPT will use optimizations in MPI_Alltoall() and its variants
20220320 062518.033 INFO             PET3 index=  25                                        MPI_COLL_CLUSTER_OPT : Controls whether node leaders are used to accelerate collectives
20220320 062518.033 INFO             PET3 index=  26                                            MPI_COLL_GATHERV : Average send size up to which MPT will use optimizations in MPI_Gatherv()
20220320 062518.033 INFO             PET3 index=  27                                            MPI_COLL_LEADERS : Controls whether MPT uses enhanced node leaders to improve performance of collectives
20220320 062518.033 INFO             PET3 index=  28                                     MPI_COLL_NUMA_THRESHOLD : Number of NUMA nodes per host beyond which MPT will enable special NUMA performance optimizations for collective operations
20220320 062518.033 INFO             PET3 index=  29                                                MPI_COLL_OPT : Controls whether optimized collectives are enabled
20220320 062518.033 INFO             PET3 index=  30                                        MPI_COLL_OPT_VERBOSE : Enables display of optimized collective communication algorithms that are active for every communicator
20220320 062518.033 INFO             PET3 index=  31                                             MPI_COLL_PREREG : Enables pre-registration of data buffers for collectives with IB Region cache for potential amortization across subsequent calls
20220320 062518.033 INFO             PET3 index=  32                                         MPI_COLL_RED_RB_MIN : Threshold under which a doubling Allreduce is used
20220320 062518.033 INFO             PET3 index=  33                                       MPI_COLL_REPRODUCIBLE : Controls whether MPT does reductions in a reproducible manner to avoid ordering issues with floating point calculations
20220320 062518.033 INFO             PET3 index=  34                                               MPI_COLL_SYNC : Enables usage of collective algorithms suitable for tighlty synchronized applications
20220320 062518.033 INFO             PET3 index=  35                                                MPI_COMM_MAX : Max number of communicators that can be used in an MPI program
20220320 062518.033 INFO             PET3 index=  36                                        MPI_CONTEXT_MULTIPLE : Controls whether MPT will attempt to allocate separate network resources for each MPI Communicator that is created
20220320 062518.033 INFO             PET3 index=  37                                                MPI_COREDUMP : Controls which ranks of an MPI job can dump core on receipt of a core-dumping signal
20220320 062518.033 INFO             PET3 index=  38                                       MPI_COREDUMP_DEBUGGER : Controls the debugger MPT uses to display stacktraces of ranks upon receipt of a core-dumping signal
20220320 062518.033 INFO             PET3 index=  39                                         MPI_CUDA_BUFFER_MAX : Max message size that controls whether MPT uses default message buffering or CUDA IPC to do zero-copy transfer
20220320 062518.033 INFO             PET3 index=  40                          MPI_DEFAULT_SINGLE_COPY_BUFFER_MAX : Controls zero-copy message criteria for shared memory
20220320 062518.033 INFO             PET3 index=  41                                 MPI_DEFAULT_SINGLE_COPY_OFF : Enables buffering always, even for messages that qualify for single copy
20220320 062518.033 INFO             PET3 index=  42                                                     MPI_DIR : Sets the working directory on a host when using Array Services
20220320 062518.033 INFO             PET3 index=  43                                        MPI_DISPLAY_SETTINGS : Causes MPT to display the default and current settings of the environment variables controlling it
20220320 062518.033 INFO             PET3 index=  44                                             MPI_DSM_CPULIST : Specifies a list of CPUs on which to run an MPI application
20220320 062518.033 INFO             PET3 index=  45                                          MPI_DSM_DISTRIBUTE : Enables NUMA-aware CPU pinning of MPI Ranks
20220320 062518.033 INFO             PET3 index=  46                                                 MPI_DSM_OFF : Turns off nonuniform memory access (NUMA) optimization in the MPI library
20220320 062518.033 INFO             PET3 index=  47                                             MPI_DSM_VERBOSE : Causes MPT to print information about process placement
20220320 062518.033 INFO             PET3 index=  48                                            MPI_GATHER_RANKS : Causes MPT to attempt to collect all the workers on a host under a single shepherd,
20220320 062518.033 INFO             PET3 index=  49                                     MPI_GRAPH_OPTIMIZATIONS : Controls the number of optimization loops for MPI_Graph_create
20220320 062518.033 INFO             PET3 index=  50                                               MPI_GROUP_MAX : Determines the maximum number of groups that can simultaneously exist for any single MPI process
20220320 062518.033 INFO             PET3 index=  51                                               MPI_INIT_LATE : If set and MPT is launching under SLURM or MPI_SHEPHERD mode, then MPT will delay all initialization until MPI_Init() is called
20220320 062518.033 INFO             PET3 index=  52                                          MPI_LAUNCH_TIMEOUT : Number of seconds to wait before timing out when starting up remote jobs
20220320 062518.033 INFO             PET3 index=  53                                        MPI_MAPPED_HEAP_SIZE : Max amount of heap in bytes per MPI process that is memory mapped between processes under the same shepherd
20220320 062518.033 INFO             PET3 index=  54                                       MPI_MAPPED_STACK_SIZE : Amount of stack in bytes that is memory mapped per MPI process
20220320 062518.033 INFO             PET3 index=  55                                               MPI_MEM_ALIGN : Alignment for memory allocations to help eliminate precision errors related to special CPU instructions for reductions and floating point calculations
20220320 062518.033 INFO             PET3 index=  56                                              MPI_MEMMAP_OFF : Turns off the memory mapping feature used in zero-copy transfers and MPI one-sided communication
20220320 062518.033 INFO             PET3 index=  57                                           MPI_MEMORY_REPORT : Specifies whether to print information about MPT's internal memory usage
20220320 062518.033 INFO             PET3 index=  58                                      MPI_MEMORY_REPORT_FILE : If set this is the file to print memory report information to
20220320 062518.033 INFO             PET3 index=  59                                                 MPI_MSG_MEM : Controls the total amount of memory used by MPT on temporary message headers
20220320 062518.033 INFO             PET3 index=  60                                                     MPI_NAP : Controls the way in which ranks wait for events to occur
20220320 062518.033 INFO             PET3 index=  61                                              MPI_NUM_QUICKS : Limits the number of outstanding quick RDMA transfers per rank
20220320 062518.033 INFO             PET3 index=  62                                         MPI_OMP_NUM_THREADS : Represents the value of the OMP_NUM_THREADS environment variable for each host-program specification on the mpirun command line
20220320 062518.033 INFO             PET3 index=  63                                          MPI_OPENMP_INTEROP : Causes the placement of MPI processes to better accommodate the OpenMP threads associated with each process
20220320 062518.033 INFO             PET3 index=  64                                                    MPI_PMIX : Causes MPT to use the PMIx interface instead of PMI2 when working with SLURM
20220320 062518.033 INFO             PET3 index=  65                                           MPI_PREFAULT_HEAP : Amount of xpmem-attached memory that should be prefaulted
20220320 062518.033 INFO             PET3 index=  66                                               MPI_QUERYABLE : Enables use of the mpt_query command to query a MPI job about its activities and statistics
20220320 062518.033 INFO             PET3 index=  67                                           MPI_REQUEST_DEBUG : Enables extra checking of the use of MPI requests
20220320 062518.033 INFO             PET3 index=  68                                             MPI_REQUEST_MAX : Determines the number of MPI_Requests preallocated by MPT to track simultaneous nonblocking sends and receives within each MPI process
20220320 062518.033 INFO             PET3 index=  69                                              MPI_RESET_PATH : Causes MPT to set the PATH environment variable of MPI processess using a system default instead of the mpirun process environment setting
20220320 062518.033 INFO             PET3 index=  70                                                MPI_SHEPHERD : Causes MPT to use a completely separate mpt_shepherd program which does a fork+exec of each rank
20220320 062518.033 INFO             PET3 index=  71                                                 MPI_SIGTRAP : Specifies if MPT's signal handler should override any existing signal handlers for signals SIGSEGV, SIGQUIT, SIGILL, SIGABRT, SIGBUS, and SIGFPE
20220320 062518.033 INFO             PET3 index=  72                                                   MPI_STATS : Enables printing of MPI internal statistics
20220320 062518.033 INFO             PET3 index=  73                                              MPI_STATS_FILE : Name of the file that contains information from MPI_STATS and other statistics counters
20220320 062518.033 INFO             PET3 index=  74                                           MPI_STATUS_SIGNAL : Enables MPT to receive a signal which will prompt each rank to print status information to stderr
20220320 062518.033 INFO             PET3 index=  75                                          MPI_SYNC_THRESHOLD : Transfer size in bytes up to which MPT will attempt to send the data immediately in MPI_Ssend and MPI_Issend
20220320 062518.033 INFO             PET3 index=  76                                             MPI_SYSLOG_COPY : Enables messages about MPT internal errors and system failures to get copied to the system log
20220320 062518.033 INFO             PET3 index=  77                                              MPI_TYPE_DEPTH : Maximum number of nesting levels for derived data types
20220320 062518.033 INFO             PET3 index=  78                                                MPI_TYPE_MAX : Maximum number of data types that can simultaneously exist for any single MPI process
20220320 062518.033 INFO             PET3 index=  79                                        MPI_UNBUFFERED_STDIO : Disables buffering of stdio and stderr output
20220320 062518.033 INFO             PET3 index=  80                                                MPI_UNIVERSE : Additional hosts on which MPI processes may be launched with MPI_Comm_spawn and MPI_Comm_spawn_multiple functions
20220320 062518.033 INFO             PET3 index=  81                                           MPI_UNIVERSE_SIZE : Controls number of ranks when running in spawn capable mode
20220320 062518.033 INFO             PET3 index=  82                                          MPI_UNWEIGHTED_OLD : Enables pre-2.13 ABI of MPT for MPI_UNWEIGHTED
20220320 062518.033 INFO             PET3 index=  83                                                MPI_USE_CUDA : Enables CUDA support and GPU buffers for all operations
20220320 062518.033 INFO             PET3 index=  84                                             MPI_USING_VTUNE : Enables compatibility with Intel Vtune for MPT's classic SGI ABI
20220320 062518.033 INFO             PET3 index=  85                                                 MPI_VERBOSE : Enables display of information like interconnect device usage and environment variables set to non-default values
20220320 062518.033 INFO             PET3 index=  86                                                MPI_VERBOSE2 : Allows additional MPT diagnostic information to be printed in the standard output stream
20220320 062518.033 INFO             PET3 index=  87                                          MPI_WATCHDOG_TIMER : Minutes to wait before an MPI process that is unable to contact another process aborts the application
20220320 062518.033 INFO             PET3 index=  88                                               MPI_WILDCARDS : When set to false allows MPT to make some optimizations in MPI_Recv() and MPI_Probe() if MPI_ANY_SOURCE is not used
20220320 062518.034 INFO             PET3 index=  89                                                MPI_WIN_MODE : Selects RDMA capabilities of hardware v/s normal send/receive based emulation for one-sided communication
20220320 062518.034 INFO             PET3 index=  90                                               MPI_WORLD_MAP : Rank to host mapping
20220320 062518.034 INFO             PET3 index=  91                                           MPI_XPMEM_ENABLED : Determines whether HPE XPMEM is used or not
20220320 062518.034 INFO             PET3 index=  92                                            MPIO_DIRECT_READ : Enables Direct IO in ROMIO
20220320 062518.034 INFO             PET3 index=  93                                           MPIO_DIRECT_WRITE : Enables Direct IO in ROMIO
20220320 062518.034 INFO             PET3 index=  94                                 MPIO_DIRECT_READ_CHUNK_SIZE : Read chunk size when Direct IO is enabled in ROMIO
20220320 062518.034 INFO             PET3 index=  95                                MPIO_DIRECT_WRITE_CHUNK_SIZE : Write chunk size when Direct IO is enabled in ROMIO
20220320 062518.034 INFO             PET3 index=  96                                 MPIO_LUSTRE_WRITE_AGGMETHOD : Write aggregrate method for ROMIO Lustre filesystem
20220320 062518.034 INFO             PET3 index=  97                                   MPIO_LUSTRE_GCYC_MIN_ITER : Minimum number of iterations for group-cyclic aggregator
20220320 062518.034 INFO             PET3 index=  98                                    profiled_recv_request_id : identity of the request-of-interest
20220320 062518.034 INFO             PET3 --- VMK::logSystem() end ---------------------------------
20220320 062518.034 INFO             PET3 main: --- VMK::log() start -------------------------------------
20220320 062518.034 INFO             PET3 main: vm located at: 0x18a8010
20220320 062518.034 INFO             PET3 main: petCount=6 localPet=3 mypthid=46940161916672 currentSsiPe=3
20220320 062518.034 INFO             PET3 main: Current system level affinity pinning for local PET:
20220320 062518.034 INFO             PET3 main:  SSIPE=3
20220320 062518.034 INFO             PET3 main: Current system level OMP_NUM_THREADS setting for local PET: 36
20220320 062518.034 INFO             PET3 main: ssiCount=1 localSsi=0
20220320 062518.034 INFO             PET3 main: mpionly=1 threadsflag=0
20220320 062518.034 INFO             PET3 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220320 062518.034 INFO             PET3 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220320 062518.034 INFO             PET3 main:  PE=0 SSI=0 SSIPE=0
20220320 062518.034 INFO             PET3 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220320 062518.034 INFO             PET3 main:  PE=1 SSI=0 SSIPE=1
20220320 062518.034 INFO             PET3 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220320 062518.034 INFO             PET3 main:  PE=2 SSI=0 SSIPE=2
20220320 062518.034 INFO             PET3 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220320 062518.034 INFO             PET3 main:  PE=3 SSI=0 SSIPE=3
20220320 062518.034 INFO             PET3 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220320 062518.034 INFO             PET3 main:  PE=4 SSI=0 SSIPE=4
20220320 062518.034 INFO             PET3 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220320 062518.034 INFO             PET3 main:  PE=5 SSI=0 SSIPE=5
20220320 062518.034 INFO             PET3 main: --- VMK::log() end ---------------------------------------
20220320 062518.035 INFO             PET3 Executing 'userm1_setvm'
20220320 062518.035 INFO             PET3 Executing 'userm1_register'
20220320 062518.035 INFO             PET3 Executing 'userm2_setvm'
20220320 062518.035 DEBUG            PET3 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220320 062518.035 DEBUG            PET3 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220320 062518.035 INFO             PET3 model1: --- VMK::log() start -------------------------------------
20220320 062518.035 INFO             PET3 model1: vm located at: 0x292c880
20220320 062518.035 INFO             PET3 model1: petCount=6 localPet=3 mypthid=46940161916672 currentSsiPe=3
20220320 062518.035 INFO             PET3 model1: Current system level affinity pinning for local PET:
20220320 062518.035 INFO             PET3 model1:  SSIPE=3
20220320 062518.035 INFO             PET3 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220320 062518.035 INFO             PET3 model1: ssiCount=1 localSsi=0
20220320 062518.035 INFO             PET3 model1: mpionly=1 threadsflag=0
20220320 062518.035 INFO             PET3 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220320 062518.035 INFO             PET3 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220320 062518.035 INFO             PET3 model1:  PE=0 SSI=0 SSIPE=0
20220320 062518.035 INFO             PET3 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220320 062518.035 INFO             PET3 model1:  PE=1 SSI=0 SSIPE=1
20220320 062518.035 INFO             PET3 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220320 062518.035 INFO             PET3 model1:  PE=2 SSI=0 SSIPE=2
20220320 062518.035 INFO             PET3 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220320 062518.036 INFO             PET3 model1:  PE=3 SSI=0 SSIPE=3
20220320 062518.036 INFO             PET3 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220320 062518.036 INFO             PET3 model1:  PE=4 SSI=0 SSIPE=4
20220320 062518.036 INFO             PET3 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220320 062518.036 INFO             PET3 model1:  PE=5 SSI=0 SSIPE=5
20220320 062518.036 INFO             PET3 model1: --- VMK::log() end ---------------------------------------
20220320 062518.038 INFO             PET3 Entering 'user1_run'
20220320 062518.038 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220320 062518.814 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220320 062519.504 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220320 062520.193 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220320 062520.882 INFO             PET3  user1_run: on SSIPE:            3  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20220320 062521.571 INFO             PET3 Exiting 'user1_run'
20220320 062525.032 INFO             PET3  NUMBER_OF_PROCESSORS           6
20220320 062525.033 INFO             PET3  PASS  System Test ESMF_FieldSharedDeSSISTest, ESMF_FieldSharedDeSSISTest.F90, line 276
20220320 062525.033 INFO             PET3 Finalizing ESMF
20220320 062518.028 INFO             PET4 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220320 062518.028 INFO             PET4 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220320 062518.028 INFO             PET4 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220320 062518.028 INFO             PET4 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220320 062518.028 INFO             PET4 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220320 062518.028 INFO             PET4 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220320 062518.028 INFO             PET4 Running with ESMF Version   : v8.3.0b09-102-gabaf3ef548
20220320 062518.028 INFO             PET4 ESMF library build date/time: "Mar 20 2022" "06:03:36"
20220320 062518.028 INFO             PET4 ESMF library build location : /glade/scratch/rlong/esmf-testing/gfortran_10.1.0_mpt_O_jedwards_pio_update2
20220320 062518.028 INFO             PET4 ESMF_COMM                   : mpt
20220320 062518.028 INFO             PET4 ESMF_MOAB                   : enabled
20220320 062518.028 INFO             PET4 ESMF_LAPACK                 : enabled
20220320 062518.028 INFO             PET4 ESMF_NETCDF                 : enabled
20220320 062518.028 INFO             PET4 ESMF_PNETCDF                : disabled
20220320 062518.028 INFO             PET4 ESMF_PIO                    : enabled
20220320 062518.028 INFO             PET4 ESMF_YAMLCPP                : enabled
20220320 062518.030 INFO             PET4 --- VMK::logSystem() start -------------------------------
20220320 062518.030 INFO             PET4 esmfComm=mpt
20220320 062518.030 INFO             PET4 isPthreadsEnabled=1
20220320 062518.030 INFO             PET4 isOpenMPEnabled=1
20220320 062518.030 INFO             PET4 isOpenACCEnabled=0
20220320 062518.030 INFO             PET4 isSsiSharedMemoryEnabled=1
20220320 062518.030 INFO             PET4 ssiCount=1 peCount=6
20220320 062518.030 INFO             PET4 PE=0 SSI=0 SSIPE=0
20220320 062518.030 INFO             PET4 PE=1 SSI=0 SSIPE=1
20220320 062518.030 INFO             PET4 PE=2 SSI=0 SSIPE=2
20220320 062518.030 INFO             PET4 PE=3 SSI=0 SSIPE=3
20220320 062518.030 INFO             PET4 PE=4 SSI=0 SSIPE=4
20220320 062518.030 INFO             PET4 PE=5 SSI=0 SSIPE=5
20220320 062518.030 INFO             PET4 --- VMK::logSystem() MPI Control Variables ---------------
20220320 062518.030 INFO             PET4 index=   0                                        MPI_ADJUST_ALLGATHER : Algorithm for MPI_Allgather
20220320 062518.030 INFO             PET4 index=   1                                       MPI_ADJUST_ALLGATHERV : Algorithm for MPI_Allgatherv
20220320 062518.030 INFO             PET4 index=   2                                        MPI_ADJUST_ALLREDUCE : Algorithm for MPI_Allreduce
20220320 062518.030 INFO             PET4 index=   3                                         MPI_ADJUST_ALLTOALL : Algorithm for MPI_Alltoall
20220320 062518.030 INFO             PET4 index=   4                                        MPI_ADJUST_ALLTOALLV : Algorithm for MPI_Alltoallv
20220320 062518.030 INFO             PET4 index=   5                                        MPI_ADJUST_ALLTOALLW : Algorithm for MPI_Alltoallw
20220320 062518.030 INFO             PET4 index=   6                                          MPI_ADJUST_BARRIER : Algorithm for MPI_Barrier
20220320 062518.030 INFO             PET4 index=   7                                            MPI_ADJUST_BCAST : Algorithm for MPI_Bcast
20220320 062518.030 INFO             PET4 index=   8                                           MPI_ADJUST_EXSCAN : Algorithm for MPI_Exscan
20220320 062518.030 INFO             PET4 index=   9                                           MPI_ADJUST_GATHER : Algorithm for MPI_Gather
20220320 062518.030 INFO             PET4 index=  10                                          MPI_ADJUST_GATHERV : Algorithm for MPI_Gatherv
20220320 062518.032 INFO             PET4 index=  11                                   MPI_ADJUST_REDUCE_SCATTER : Algorithm for MPI_Reduce_scatter
20220320 062518.033 INFO             PET4 index=  12                                           MPI_ADJUST_REDUCE : Algorithm for MPI_Reduce
20220320 062518.033 INFO             PET4 index=  13                                             MPI_ADJUST_SCAN : Algorithm for MPI_Scan
20220320 062518.033 INFO             PET4 index=  14                                          MPI_ADJUST_SCATTER : Algorithm for MPI_Scatter
20220320 062518.033 INFO             PET4 index=  15                                         MPI_ADJUST_SCATTERV : Algorithm for MPI_Scatterv
20220320 062518.033 INFO             PET4 index=  16                                                   MPI_ARRAY : Alternative array name for communicating with Array Services
20220320 062518.033 INFO             PET4 index=  17                                          MPI_ASYNC_PROGRESS : Enables asynchronous progress of non-blocking operations
20220320 062518.033 INFO             PET4 index=  18                                              MPI_BUFFER_MAX : Threshold for blocking message size resulting in the use of zero-copy
20220320 062518.033 INFO             PET4 index=  19                                              MPI_BUFS_LIMIT : Max number of internal buffers a single transfer can use
20220320 062518.033 INFO             PET4 index=  20                                           MPI_BUFS_PER_PROC : Number of private message buffers per process for send and receive of long messages
20220320 062518.033 INFO             PET4 index=  21                                              MPI_CHECK_ARGS : Enables checking of MPI function arguments
20220320 062518.033 INFO             PET4 index=  22                                             MPI_CLOCKSOURCE : Time sources to use to support the MPI_Wtime() and MPI_Wtick() functions
20220320 062518.033 INFO             PET4 index=  23                                           MPI_KCOPY_ENABLED : Uses the Linux kernel to do large transfers between local ranks
20220320 062518.033 INFO             PET4 index=  24                                           MPI_COLL_A2A_FRAG : Average send size up to which MPT will use optimizations in MPI_Alltoall() and its variants
20220320 062518.033 INFO             PET4 index=  25                                        MPI_COLL_CLUSTER_OPT : Controls whether node leaders are used to accelerate collectives
20220320 062518.033 INFO             PET4 index=  26                                            MPI_COLL_GATHERV : Average send size up to which MPT will use optimizations in MPI_Gatherv()
20220320 062518.033 INFO             PET4 index=  27                                            MPI_COLL_LEADERS : Controls whether MPT uses enhanced node leaders to improve performance of collectives
20220320 062518.033 INFO             PET4 index=  28                                     MPI_COLL_NUMA_THRESHOLD : Number of NUMA nodes per host beyond which MPT will enable special NUMA performance optimizations for collective operations
20220320 062518.033 INFO             PET4 index=  29                                                MPI_COLL_OPT : Controls whether optimized collectives are enabled
20220320 062518.033 INFO             PET4 index=  30                                        MPI_COLL_OPT_VERBOSE : Enables display of optimized collective communication algorithms that are active for every communicator
20220320 062518.033 INFO             PET4 index=  31                                             MPI_COLL_PREREG : Enables pre-registration of data buffers for collectives with IB Region cache for potential amortization across subsequent calls
20220320 062518.033 INFO             PET4 index=  32                                         MPI_COLL_RED_RB_MIN : Threshold under which a doubling Allreduce is used
20220320 062518.033 INFO             PET4 index=  33                                       MPI_COLL_REPRODUCIBLE : Controls whether MPT does reductions in a reproducible manner to avoid ordering issues with floating point calculations
20220320 062518.033 INFO             PET4 index=  34                                               MPI_COLL_SYNC : Enables usage of collective algorithms suitable for tighlty synchronized applications
20220320 062518.033 INFO             PET4 index=  35                                                MPI_COMM_MAX : Max number of communicators that can be used in an MPI program
20220320 062518.033 INFO             PET4 index=  36                                        MPI_CONTEXT_MULTIPLE : Controls whether MPT will attempt to allocate separate network resources for each MPI Communicator that is created
20220320 062518.033 INFO             PET4 index=  37                                                MPI_COREDUMP : Controls which ranks of an MPI job can dump core on receipt of a core-dumping signal
20220320 062518.033 INFO             PET4 index=  38                                       MPI_COREDUMP_DEBUGGER : Controls the debugger MPT uses to display stacktraces of ranks upon receipt of a core-dumping signal
20220320 062518.033 INFO             PET4 index=  39                                         MPI_CUDA_BUFFER_MAX : Max message size that controls whether MPT uses default message buffering or CUDA IPC to do zero-copy transfer
20220320 062518.033 INFO             PET4 index=  40                          MPI_DEFAULT_SINGLE_COPY_BUFFER_MAX : Controls zero-copy message criteria for shared memory
20220320 062518.033 INFO             PET4 index=  41                                 MPI_DEFAULT_SINGLE_COPY_OFF : Enables buffering always, even for messages that qualify for single copy
20220320 062518.033 INFO             PET4 index=  42                                                     MPI_DIR : Sets the working directory on a host when using Array Services
20220320 062518.033 INFO             PET4 index=  43                                        MPI_DISPLAY_SETTINGS : Causes MPT to display the default and current settings of the environment variables controlling it
20220320 062518.033 INFO             PET4 index=  44                                             MPI_DSM_CPULIST : Specifies a list of CPUs on which to run an MPI application
20220320 062518.033 INFO             PET4 index=  45                                          MPI_DSM_DISTRIBUTE : Enables NUMA-aware CPU pinning of MPI Ranks
20220320 062518.033 INFO             PET4 index=  46                                                 MPI_DSM_OFF : Turns off nonuniform memory access (NUMA) optimization in the MPI library
20220320 062518.033 INFO             PET4 index=  47                                             MPI_DSM_VERBOSE : Causes MPT to print information about process placement
20220320 062518.033 INFO             PET4 index=  48                                            MPI_GATHER_RANKS : Causes MPT to attempt to collect all the workers on a host under a single shepherd,
20220320 062518.033 INFO             PET4 index=  49                                     MPI_GRAPH_OPTIMIZATIONS : Controls the number of optimization loops for MPI_Graph_create
20220320 062518.033 INFO             PET4 index=  50                                               MPI_GROUP_MAX : Determines the maximum number of groups that can simultaneously exist for any single MPI process
20220320 062518.033 INFO             PET4 index=  51                                               MPI_INIT_LATE : If set and MPT is launching under SLURM or MPI_SHEPHERD mode, then MPT will delay all initialization until MPI_Init() is called
20220320 062518.033 INFO             PET4 index=  52                                          MPI_LAUNCH_TIMEOUT : Number of seconds to wait before timing out when starting up remote jobs
20220320 062518.033 INFO             PET4 index=  53                                        MPI_MAPPED_HEAP_SIZE : Max amount of heap in bytes per MPI process that is memory mapped between processes under the same shepherd
20220320 062518.033 INFO             PET4 index=  54                                       MPI_MAPPED_STACK_SIZE : Amount of stack in bytes that is memory mapped per MPI process
20220320 062518.033 INFO             PET4 index=  55                                               MPI_MEM_ALIGN : Alignment for memory allocations to help eliminate precision errors related to special CPU instructions for reductions and floating point calculations
20220320 062518.033 INFO             PET4 index=  56                                              MPI_MEMMAP_OFF : Turns off the memory mapping feature used in zero-copy transfers and MPI one-sided communication
20220320 062518.034 INFO             PET4 index=  57                                           MPI_MEMORY_REPORT : Specifies whether to print information about MPT's internal memory usage
20220320 062518.034 INFO             PET4 index=  58                                      MPI_MEMORY_REPORT_FILE : If set this is the file to print memory report information to
20220320 062518.034 INFO             PET4 index=  59                                                 MPI_MSG_MEM : Controls the total amount of memory used by MPT on temporary message headers
20220320 062518.034 INFO             PET4 index=  60                                                     MPI_NAP : Controls the way in which ranks wait for events to occur
20220320 062518.034 INFO             PET4 index=  61                                              MPI_NUM_QUICKS : Limits the number of outstanding quick RDMA transfers per rank
20220320 062518.034 INFO             PET4 index=  62                                         MPI_OMP_NUM_THREADS : Represents the value of the OMP_NUM_THREADS environment variable for each host-program specification on the mpirun command line
20220320 062518.034 INFO             PET4 index=  63                                          MPI_OPENMP_INTEROP : Causes the placement of MPI processes to better accommodate the OpenMP threads associated with each process
20220320 062518.034 INFO             PET4 index=  64                                                    MPI_PMIX : Causes MPT to use the PMIx interface instead of PMI2 when working with SLURM
20220320 062518.034 INFO             PET4 index=  65                                           MPI_PREFAULT_HEAP : Amount of xpmem-attached memory that should be prefaulted
20220320 062518.034 INFO             PET4 index=  66                                               MPI_QUERYABLE : Enables use of the mpt_query command to query a MPI job about its activities and statistics
20220320 062518.034 INFO             PET4 index=  67                                           MPI_REQUEST_DEBUG : Enables extra checking of the use of MPI requests
20220320 062518.034 INFO             PET4 index=  68                                             MPI_REQUEST_MAX : Determines the number of MPI_Requests preallocated by MPT to track simultaneous nonblocking sends and receives within each MPI process
20220320 062518.034 INFO             PET4 index=  69                                              MPI_RESET_PATH : Causes MPT to set the PATH environment variable of MPI processess using a system default instead of the mpirun process environment setting
20220320 062518.034 INFO             PET4 index=  70                                                MPI_SHEPHERD : Causes MPT to use a completely separate mpt_shepherd program which does a fork+exec of each rank
20220320 062518.034 INFO             PET4 index=  71                                                 MPI_SIGTRAP : Specifies if MPT's signal handler should override any existing signal handlers for signals SIGSEGV, SIGQUIT, SIGILL, SIGABRT, SIGBUS, and SIGFPE
20220320 062518.034 INFO             PET4 index=  72                                                   MPI_STATS : Enables printing of MPI internal statistics
20220320 062518.034 INFO             PET4 index=  73                                              MPI_STATS_FILE : Name of the file that contains information from MPI_STATS and other statistics counters
20220320 062518.034 INFO             PET4 index=  74                                           MPI_STATUS_SIGNAL : Enables MPT to receive a signal which will prompt each rank to print status information to stderr
20220320 062518.034 INFO             PET4 index=  75                                          MPI_SYNC_THRESHOLD : Transfer size in bytes up to which MPT will attempt to send the data immediately in MPI_Ssend and MPI_Issend
20220320 062518.034 INFO             PET4 index=  76                                             MPI_SYSLOG_COPY : Enables messages about MPT internal errors and system failures to get copied to the system log
20220320 062518.034 INFO             PET4 index=  77                                              MPI_TYPE_DEPTH : Maximum number of nesting levels for derived data types
20220320 062518.034 INFO             PET4 index=  78                                                MPI_TYPE_MAX : Maximum number of data types that can simultaneously exist for any single MPI process
20220320 062518.034 INFO             PET4 index=  79                                        MPI_UNBUFFERED_STDIO : Disables buffering of stdio and stderr output
20220320 062518.034 INFO             PET4 index=  80                                                MPI_UNIVERSE : Additional hosts on which MPI processes may be launched with MPI_Comm_spawn and MPI_Comm_spawn_multiple functions
20220320 062518.034 INFO             PET4 index=  81                                           MPI_UNIVERSE_SIZE : Controls number of ranks when running in spawn capable mode
20220320 062518.034 INFO             PET4 index=  82                                          MPI_UNWEIGHTED_OLD : Enables pre-2.13 ABI of MPT for MPI_UNWEIGHTED
20220320 062518.034 INFO             PET4 index=  83                                                MPI_USE_CUDA : Enables CUDA support and GPU buffers for all operations
20220320 062518.034 INFO             PET4 index=  84                                             MPI_USING_VTUNE : Enables compatibility with Intel Vtune for MPT's classic SGI ABI
20220320 062518.034 INFO             PET4 index=  85                                                 MPI_VERBOSE : Enables display of information like interconnect device usage and environment variables set to non-default values
20220320 062518.034 INFO             PET4 index=  86                                                MPI_VERBOSE2 : Allows additional MPT diagnostic information to be printed in the standard output stream
20220320 062518.034 INFO             PET4 index=  87                                          MPI_WATCHDOG_TIMER : Minutes to wait before an MPI process that is unable to contact another process aborts the application
20220320 062518.034 INFO             PET4 index=  88                                               MPI_WILDCARDS : When set to false allows MPT to make some optimizations in MPI_Recv() and MPI_Probe() if MPI_ANY_SOURCE is not used
20220320 062518.034 INFO             PET4 index=  89                                                MPI_WIN_MODE : Selects RDMA capabilities of hardware v/s normal send/receive based emulation for one-sided communication
20220320 062518.034 INFO             PET4 index=  90                                               MPI_WORLD_MAP : Rank to host mapping
20220320 062518.034 INFO             PET4 index=  91                                           MPI_XPMEM_ENABLED : Determines whether HPE XPMEM is used or not
20220320 062518.034 INFO             PET4 index=  92                                            MPIO_DIRECT_READ : Enables Direct IO in ROMIO
20220320 062518.034 INFO             PET4 index=  93                                           MPIO_DIRECT_WRITE : Enables Direct IO in ROMIO
20220320 062518.034 INFO             PET4 index=  94                                 MPIO_DIRECT_READ_CHUNK_SIZE : Read chunk size when Direct IO is enabled in ROMIO
20220320 062518.034 INFO             PET4 index=  95                                MPIO_DIRECT_WRITE_CHUNK_SIZE : Write chunk size when Direct IO is enabled in ROMIO
20220320 062518.034 INFO             PET4 index=  96                                 MPIO_LUSTRE_WRITE_AGGMETHOD : Write aggregrate method for ROMIO Lustre filesystem
20220320 062518.034 INFO             PET4 index=  97                                   MPIO_LUSTRE_GCYC_MIN_ITER : Minimum number of iterations for group-cyclic aggregator
20220320 062518.034 INFO             PET4 index=  98                                    profiled_recv_request_id : identity of the request-of-interest
20220320 062518.034 INFO             PET4 --- VMK::logSystem() end ---------------------------------
20220320 062518.034 INFO             PET4 main: --- VMK::log() start -------------------------------------
20220320 062518.034 INFO             PET4 main: vm located at: 0x18a8080
20220320 062518.034 INFO             PET4 main: petCount=6 localPet=4 mypthid=46940161916672 currentSsiPe=4
20220320 062518.034 INFO             PET4 main: Current system level affinity pinning for local PET:
20220320 062518.034 INFO             PET4 main:  SSIPE=4
20220320 062518.034 INFO             PET4 main: Current system level OMP_NUM_THREADS setting for local PET: 36
20220320 062518.034 INFO             PET4 main: ssiCount=1 localSsi=0
20220320 062518.034 INFO             PET4 main: mpionly=1 threadsflag=0
20220320 062518.034 INFO             PET4 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220320 062518.034 INFO             PET4 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220320 062518.034 INFO             PET4 main:  PE=0 SSI=0 SSIPE=0
20220320 062518.034 INFO             PET4 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220320 062518.034 INFO             PET4 main:  PE=1 SSI=0 SSIPE=1
20220320 062518.034 INFO             PET4 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220320 062518.034 INFO             PET4 main:  PE=2 SSI=0 SSIPE=2
20220320 062518.034 INFO             PET4 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220320 062518.034 INFO             PET4 main:  PE=3 SSI=0 SSIPE=3
20220320 062518.034 INFO             PET4 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220320 062518.034 INFO             PET4 main:  PE=4 SSI=0 SSIPE=4
20220320 062518.034 INFO             PET4 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220320 062518.034 INFO             PET4 main:  PE=5 SSI=0 SSIPE=5
20220320 062518.034 INFO             PET4 main: --- VMK::log() end ---------------------------------------
20220320 062518.035 INFO             PET4 Executing 'userm1_setvm'
20220320 062518.035 INFO             PET4 Executing 'userm1_register'
20220320 062518.035 INFO             PET4 Executing 'userm2_setvm'
20220320 062518.035 DEBUG            PET4 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220320 062518.035 DEBUG            PET4 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220320 062518.035 INFO             PET4 model1: --- VMK::log() start -------------------------------------
20220320 062518.035 INFO             PET4 model1: vm located at: 0x292c900
20220320 062518.035 INFO             PET4 model1: petCount=6 localPet=4 mypthid=46940161916672 currentSsiPe=4
20220320 062518.035 INFO             PET4 model1: Current system level affinity pinning for local PET:
20220320 062518.035 INFO             PET4 model1:  SSIPE=4
20220320 062518.035 INFO             PET4 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220320 062518.035 INFO             PET4 model1: ssiCount=1 localSsi=0
20220320 062518.035 INFO             PET4 model1: mpionly=1 threadsflag=0
20220320 062518.035 INFO             PET4 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220320 062518.035 INFO             PET4 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220320 062518.035 INFO             PET4 model1:  PE=0 SSI=0 SSIPE=0
20220320 062518.035 INFO             PET4 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220320 062518.035 INFO             PET4 model1:  PE=1 SSI=0 SSIPE=1
20220320 062518.035 INFO             PET4 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220320 062518.035 INFO             PET4 model1:  PE=2 SSI=0 SSIPE=2
20220320 062518.035 INFO             PET4 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220320 062518.036 INFO             PET4 model1:  PE=3 SSI=0 SSIPE=3
20220320 062518.036 INFO             PET4 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220320 062518.036 INFO             PET4 model1:  PE=4 SSI=0 SSIPE=4
20220320 062518.036 INFO             PET4 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220320 062518.036 INFO             PET4 model1:  PE=5 SSI=0 SSIPE=5
20220320 062518.036 INFO             PET4 model1: --- VMK::log() end ---------------------------------------
20220320 062518.038 INFO             PET4 Entering 'user1_run'
20220320 062518.038 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220320 062518.810 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220320 062519.496 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220320 062520.182 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220320 062520.868 INFO             PET4  user1_run: on SSIPE:            4  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20220320 062521.554 INFO             PET4 Exiting 'user1_run'
20220320 062525.032 INFO             PET4  NUMBER_OF_PROCESSORS           6
20220320 062525.033 INFO             PET4  PASS  System Test ESMF_FieldSharedDeSSISTest, ESMF_FieldSharedDeSSISTest.F90, line 276
20220320 062525.033 INFO             PET4 Finalizing ESMF
20220320 062518.028 INFO             PET5 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220320 062518.028 INFO             PET5 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20220320 062518.028 INFO             PET5 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20220320 062518.028 INFO             PET5 !!! FOR PRODUCTION RUNS, USE:                      !!!
20220320 062518.028 INFO             PET5 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20220320 062518.028 INFO             PET5 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20220320 062518.028 INFO             PET5 Running with ESMF Version   : v8.3.0b09-102-gabaf3ef548
20220320 062518.028 INFO             PET5 ESMF library build date/time: "Mar 20 2022" "06:03:36"
20220320 062518.028 INFO             PET5 ESMF library build location : /glade/scratch/rlong/esmf-testing/gfortran_10.1.0_mpt_O_jedwards_pio_update2
20220320 062518.028 INFO             PET5 ESMF_COMM                   : mpt
20220320 062518.028 INFO             PET5 ESMF_MOAB                   : enabled
20220320 062518.028 INFO             PET5 ESMF_LAPACK                 : enabled
20220320 062518.028 INFO             PET5 ESMF_NETCDF                 : enabled
20220320 062518.028 INFO             PET5 ESMF_PNETCDF                : disabled
20220320 062518.028 INFO             PET5 ESMF_PIO                    : enabled
20220320 062518.028 INFO             PET5 ESMF_YAMLCPP                : enabled
20220320 062518.030 INFO             PET5 --- VMK::logSystem() start -------------------------------
20220320 062518.030 INFO             PET5 esmfComm=mpt
20220320 062518.030 INFO             PET5 isPthreadsEnabled=1
20220320 062518.030 INFO             PET5 isOpenMPEnabled=1
20220320 062518.030 INFO             PET5 isOpenACCEnabled=0
20220320 062518.030 INFO             PET5 isSsiSharedMemoryEnabled=1
20220320 062518.030 INFO             PET5 ssiCount=1 peCount=6
20220320 062518.030 INFO             PET5 PE=0 SSI=0 SSIPE=0
20220320 062518.030 INFO             PET5 PE=1 SSI=0 SSIPE=1
20220320 062518.030 INFO             PET5 PE=2 SSI=0 SSIPE=2
20220320 062518.030 INFO             PET5 PE=3 SSI=0 SSIPE=3
20220320 062518.030 INFO             PET5 PE=4 SSI=0 SSIPE=4
20220320 062518.030 INFO             PET5 PE=5 SSI=0 SSIPE=5
20220320 062518.030 INFO             PET5 --- VMK::logSystem() MPI Control Variables ---------------
20220320 062518.030 INFO             PET5 index=   0                                        MPI_ADJUST_ALLGATHER : Algorithm for MPI_Allgather
20220320 062518.030 INFO             PET5 index=   1                                       MPI_ADJUST_ALLGATHERV : Algorithm for MPI_Allgatherv
20220320 062518.030 INFO             PET5 index=   2                                        MPI_ADJUST_ALLREDUCE : Algorithm for MPI_Allreduce
20220320 062518.030 INFO             PET5 index=   3                                         MPI_ADJUST_ALLTOALL : Algorithm for MPI_Alltoall
20220320 062518.030 INFO             PET5 index=   4                                        MPI_ADJUST_ALLTOALLV : Algorithm for MPI_Alltoallv
20220320 062518.030 INFO             PET5 index=   5                                        MPI_ADJUST_ALLTOALLW : Algorithm for MPI_Alltoallw
20220320 062518.030 INFO             PET5 index=   6                                          MPI_ADJUST_BARRIER : Algorithm for MPI_Barrier
20220320 062518.030 INFO             PET5 index=   7                                            MPI_ADJUST_BCAST : Algorithm for MPI_Bcast
20220320 062518.030 INFO             PET5 index=   8                                           MPI_ADJUST_EXSCAN : Algorithm for MPI_Exscan
20220320 062518.030 INFO             PET5 index=   9                                           MPI_ADJUST_GATHER : Algorithm for MPI_Gather
20220320 062518.030 INFO             PET5 index=  10                                          MPI_ADJUST_GATHERV : Algorithm for MPI_Gatherv
20220320 062518.033 INFO             PET5 index=  11                                   MPI_ADJUST_REDUCE_SCATTER : Algorithm for MPI_Reduce_scatter
20220320 062518.033 INFO             PET5 index=  12                                           MPI_ADJUST_REDUCE : Algorithm for MPI_Reduce
20220320 062518.033 INFO             PET5 index=  13                                             MPI_ADJUST_SCAN : Algorithm for MPI_Scan
20220320 062518.033 INFO             PET5 index=  14                                          MPI_ADJUST_SCATTER : Algorithm for MPI_Scatter
20220320 062518.033 INFO             PET5 index=  15                                         MPI_ADJUST_SCATTERV : Algorithm for MPI_Scatterv
20220320 062518.033 INFO             PET5 index=  16                                                   MPI_ARRAY : Alternative array name for communicating with Array Services
20220320 062518.033 INFO             PET5 index=  17                                          MPI_ASYNC_PROGRESS : Enables asynchronous progress of non-blocking operations
20220320 062518.033 INFO             PET5 index=  18                                              MPI_BUFFER_MAX : Threshold for blocking message size resulting in the use of zero-copy
20220320 062518.033 INFO             PET5 index=  19                                              MPI_BUFS_LIMIT : Max number of internal buffers a single transfer can use
20220320 062518.033 INFO             PET5 index=  20                                           MPI_BUFS_PER_PROC : Number of private message buffers per process for send and receive of long messages
20220320 062518.033 INFO             PET5 index=  21                                              MPI_CHECK_ARGS : Enables checking of MPI function arguments
20220320 062518.033 INFO             PET5 index=  22                                             MPI_CLOCKSOURCE : Time sources to use to support the MPI_Wtime() and MPI_Wtick() functions
20220320 062518.033 INFO             PET5 index=  23                                           MPI_KCOPY_ENABLED : Uses the Linux kernel to do large transfers between local ranks
20220320 062518.033 INFO             PET5 index=  24                                           MPI_COLL_A2A_FRAG : Average send size up to which MPT will use optimizations in MPI_Alltoall() and its variants
20220320 062518.033 INFO             PET5 index=  25                                        MPI_COLL_CLUSTER_OPT : Controls whether node leaders are used to accelerate collectives
20220320 062518.033 INFO             PET5 index=  26                                            MPI_COLL_GATHERV : Average send size up to which MPT will use optimizations in MPI_Gatherv()
20220320 062518.033 INFO             PET5 index=  27                                            MPI_COLL_LEADERS : Controls whether MPT uses enhanced node leaders to improve performance of collectives
20220320 062518.033 INFO             PET5 index=  28                                     MPI_COLL_NUMA_THRESHOLD : Number of NUMA nodes per host beyond which MPT will enable special NUMA performance optimizations for collective operations
20220320 062518.033 INFO             PET5 index=  29                                                MPI_COLL_OPT : Controls whether optimized collectives are enabled
20220320 062518.033 INFO             PET5 index=  30                                        MPI_COLL_OPT_VERBOSE : Enables display of optimized collective communication algorithms that are active for every communicator
20220320 062518.033 INFO             PET5 index=  31                                             MPI_COLL_PREREG : Enables pre-registration of data buffers for collectives with IB Region cache for potential amortization across subsequent calls
20220320 062518.033 INFO             PET5 index=  32                                         MPI_COLL_RED_RB_MIN : Threshold under which a doubling Allreduce is used
20220320 062518.033 INFO             PET5 index=  33                                       MPI_COLL_REPRODUCIBLE : Controls whether MPT does reductions in a reproducible manner to avoid ordering issues with floating point calculations
20220320 062518.033 INFO             PET5 index=  34                                               MPI_COLL_SYNC : Enables usage of collective algorithms suitable for tighlty synchronized applications
20220320 062518.033 INFO             PET5 index=  35                                                MPI_COMM_MAX : Max number of communicators that can be used in an MPI program
20220320 062518.033 INFO             PET5 index=  36                                        MPI_CONTEXT_MULTIPLE : Controls whether MPT will attempt to allocate separate network resources for each MPI Communicator that is created
20220320 062518.033 INFO             PET5 index=  37                                                MPI_COREDUMP : Controls which ranks of an MPI job can dump core on receipt of a core-dumping signal
20220320 062518.033 INFO             PET5 index=  38                                       MPI_COREDUMP_DEBUGGER : Controls the debugger MPT uses to display stacktraces of ranks upon receipt of a core-dumping signal
20220320 062518.033 INFO             PET5 index=  39                                         MPI_CUDA_BUFFER_MAX : Max message size that controls whether MPT uses default message buffering or CUDA IPC to do zero-copy transfer
20220320 062518.033 INFO             PET5 index=  40                          MPI_DEFAULT_SINGLE_COPY_BUFFER_MAX : Controls zero-copy message criteria for shared memory
20220320 062518.033 INFO             PET5 index=  41                                 MPI_DEFAULT_SINGLE_COPY_OFF : Enables buffering always, even for messages that qualify for single copy
20220320 062518.034 INFO             PET5 index=  42                                                     MPI_DIR : Sets the working directory on a host when using Array Services
20220320 062518.034 INFO             PET5 index=  43                                        MPI_DISPLAY_SETTINGS : Causes MPT to display the default and current settings of the environment variables controlling it
20220320 062518.034 INFO             PET5 index=  44                                             MPI_DSM_CPULIST : Specifies a list of CPUs on which to run an MPI application
20220320 062518.034 INFO             PET5 index=  45                                          MPI_DSM_DISTRIBUTE : Enables NUMA-aware CPU pinning of MPI Ranks
20220320 062518.034 INFO             PET5 index=  46                                                 MPI_DSM_OFF : Turns off nonuniform memory access (NUMA) optimization in the MPI library
20220320 062518.034 INFO             PET5 index=  47                                             MPI_DSM_VERBOSE : Causes MPT to print information about process placement
20220320 062518.034 INFO             PET5 index=  48                                            MPI_GATHER_RANKS : Causes MPT to attempt to collect all the workers on a host under a single shepherd,
20220320 062518.034 INFO             PET5 index=  49                                     MPI_GRAPH_OPTIMIZATIONS : Controls the number of optimization loops for MPI_Graph_create
20220320 062518.034 INFO             PET5 index=  50                                               MPI_GROUP_MAX : Determines the maximum number of groups that can simultaneously exist for any single MPI process
20220320 062518.034 INFO             PET5 index=  51                                               MPI_INIT_LATE : If set and MPT is launching under SLURM or MPI_SHEPHERD mode, then MPT will delay all initialization until MPI_Init() is called
20220320 062518.034 INFO             PET5 index=  52                                          MPI_LAUNCH_TIMEOUT : Number of seconds to wait before timing out when starting up remote jobs
20220320 062518.034 INFO             PET5 index=  53                                        MPI_MAPPED_HEAP_SIZE : Max amount of heap in bytes per MPI process that is memory mapped between processes under the same shepherd
20220320 062518.034 INFO             PET5 index=  54                                       MPI_MAPPED_STACK_SIZE : Amount of stack in bytes that is memory mapped per MPI process
20220320 062518.034 INFO             PET5 index=  55                                               MPI_MEM_ALIGN : Alignment for memory allocations to help eliminate precision errors related to special CPU instructions for reductions and floating point calculations
20220320 062518.034 INFO             PET5 index=  56                                              MPI_MEMMAP_OFF : Turns off the memory mapping feature used in zero-copy transfers and MPI one-sided communication
20220320 062518.034 INFO             PET5 index=  57                                           MPI_MEMORY_REPORT : Specifies whether to print information about MPT's internal memory usage
20220320 062518.034 INFO             PET5 index=  58                                      MPI_MEMORY_REPORT_FILE : If set this is the file to print memory report information to
20220320 062518.034 INFO             PET5 index=  59                                                 MPI_MSG_MEM : Controls the total amount of memory used by MPT on temporary message headers
20220320 062518.034 INFO             PET5 index=  60                                                     MPI_NAP : Controls the way in which ranks wait for events to occur
20220320 062518.034 INFO             PET5 index=  61                                              MPI_NUM_QUICKS : Limits the number of outstanding quick RDMA transfers per rank
20220320 062518.034 INFO             PET5 index=  62                                         MPI_OMP_NUM_THREADS : Represents the value of the OMP_NUM_THREADS environment variable for each host-program specification on the mpirun command line
20220320 062518.034 INFO             PET5 index=  63                                          MPI_OPENMP_INTEROP : Causes the placement of MPI processes to better accommodate the OpenMP threads associated with each process
20220320 062518.034 INFO             PET5 index=  64                                                    MPI_PMIX : Causes MPT to use the PMIx interface instead of PMI2 when working with SLURM
20220320 062518.034 INFO             PET5 index=  65                                           MPI_PREFAULT_HEAP : Amount of xpmem-attached memory that should be prefaulted
20220320 062518.034 INFO             PET5 index=  66                                               MPI_QUERYABLE : Enables use of the mpt_query command to query a MPI job about its activities and statistics
20220320 062518.034 INFO             PET5 index=  67                                           MPI_REQUEST_DEBUG : Enables extra checking of the use of MPI requests
20220320 062518.034 INFO             PET5 index=  68                                             MPI_REQUEST_MAX : Determines the number of MPI_Requests preallocated by MPT to track simultaneous nonblocking sends and receives within each MPI process
20220320 062518.034 INFO             PET5 index=  69                                              MPI_RESET_PATH : Causes MPT to set the PATH environment variable of MPI processess using a system default instead of the mpirun process environment setting
20220320 062518.034 INFO             PET5 index=  70                                                MPI_SHEPHERD : Causes MPT to use a completely separate mpt_shepherd program which does a fork+exec of each rank
20220320 062518.034 INFO             PET5 index=  71                                                 MPI_SIGTRAP : Specifies if MPT's signal handler should override any existing signal handlers for signals SIGSEGV, SIGQUIT, SIGILL, SIGABRT, SIGBUS, and SIGFPE
20220320 062518.034 INFO             PET5 index=  72                                                   MPI_STATS : Enables printing of MPI internal statistics
20220320 062518.034 INFO             PET5 index=  73                                              MPI_STATS_FILE : Name of the file that contains information from MPI_STATS and other statistics counters
20220320 062518.034 INFO             PET5 index=  74                                           MPI_STATUS_SIGNAL : Enables MPT to receive a signal which will prompt each rank to print status information to stderr
20220320 062518.034 INFO             PET5 index=  75                                          MPI_SYNC_THRESHOLD : Transfer size in bytes up to which MPT will attempt to send the data immediately in MPI_Ssend and MPI_Issend
20220320 062518.034 INFO             PET5 index=  76                                             MPI_SYSLOG_COPY : Enables messages about MPT internal errors and system failures to get copied to the system log
20220320 062518.034 INFO             PET5 index=  77                                              MPI_TYPE_DEPTH : Maximum number of nesting levels for derived data types
20220320 062518.034 INFO             PET5 index=  78                                                MPI_TYPE_MAX : Maximum number of data types that can simultaneously exist for any single MPI process
20220320 062518.034 INFO             PET5 index=  79                                        MPI_UNBUFFERED_STDIO : Disables buffering of stdio and stderr output
20220320 062518.034 INFO             PET5 index=  80                                                MPI_UNIVERSE : Additional hosts on which MPI processes may be launched with MPI_Comm_spawn and MPI_Comm_spawn_multiple functions
20220320 062518.034 INFO             PET5 index=  81                                           MPI_UNIVERSE_SIZE : Controls number of ranks when running in spawn capable mode
20220320 062518.034 INFO             PET5 index=  82                                          MPI_UNWEIGHTED_OLD : Enables pre-2.13 ABI of MPT for MPI_UNWEIGHTED
20220320 062518.034 INFO             PET5 index=  83                                                MPI_USE_CUDA : Enables CUDA support and GPU buffers for all operations
20220320 062518.034 INFO             PET5 index=  84                                             MPI_USING_VTUNE : Enables compatibility with Intel Vtune for MPT's classic SGI ABI
20220320 062518.034 INFO             PET5 index=  85                                                 MPI_VERBOSE : Enables display of information like interconnect device usage and environment variables set to non-default values
20220320 062518.034 INFO             PET5 index=  86                                                MPI_VERBOSE2 : Allows additional MPT diagnostic information to be printed in the standard output stream
20220320 062518.034 INFO             PET5 index=  87                                          MPI_WATCHDOG_TIMER : Minutes to wait before an MPI process that is unable to contact another process aborts the application
20220320 062518.034 INFO             PET5 index=  88                                               MPI_WILDCARDS : When set to false allows MPT to make some optimizations in MPI_Recv() and MPI_Probe() if MPI_ANY_SOURCE is not used
20220320 062518.034 INFO             PET5 index=  89                                                MPI_WIN_MODE : Selects RDMA capabilities of hardware v/s normal send/receive based emulation for one-sided communication
20220320 062518.034 INFO             PET5 index=  90                                               MPI_WORLD_MAP : Rank to host mapping
20220320 062518.034 INFO             PET5 index=  91                                           MPI_XPMEM_ENABLED : Determines whether HPE XPMEM is used or not
20220320 062518.034 INFO             PET5 index=  92                                            MPIO_DIRECT_READ : Enables Direct IO in ROMIO
20220320 062518.034 INFO             PET5 index=  93                                           MPIO_DIRECT_WRITE : Enables Direct IO in ROMIO
20220320 062518.034 INFO             PET5 index=  94                                 MPIO_DIRECT_READ_CHUNK_SIZE : Read chunk size when Direct IO is enabled in ROMIO
20220320 062518.034 INFO             PET5 index=  95                                MPIO_DIRECT_WRITE_CHUNK_SIZE : Write chunk size when Direct IO is enabled in ROMIO
20220320 062518.034 INFO             PET5 index=  96                                 MPIO_LUSTRE_WRITE_AGGMETHOD : Write aggregrate method for ROMIO Lustre filesystem
20220320 062518.034 INFO             PET5 index=  97                                   MPIO_LUSTRE_GCYC_MIN_ITER : Minimum number of iterations for group-cyclic aggregator
20220320 062518.034 INFO             PET5 index=  98                                    profiled_recv_request_id : identity of the request-of-interest
20220320 062518.034 INFO             PET5 --- VMK::logSystem() end ---------------------------------
20220320 062518.034 INFO             PET5 main: --- VMK::log() start -------------------------------------
20220320 062518.034 INFO             PET5 main: vm located at: 0x18a8120
20220320 062518.034 INFO             PET5 main: petCount=6 localPet=5 mypthid=46940161916672 currentSsiPe=5
20220320 062518.034 INFO             PET5 main: Current system level affinity pinning for local PET:
20220320 062518.034 INFO             PET5 main:  SSIPE=5
20220320 062518.034 INFO             PET5 main: Current system level OMP_NUM_THREADS setting for local PET: 36
20220320 062518.034 INFO             PET5 main: ssiCount=1 localSsi=0
20220320 062518.034 INFO             PET5 main: mpionly=1 threadsflag=0
20220320 062518.034 INFO             PET5 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220320 062518.034 INFO             PET5 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220320 062518.034 INFO             PET5 main:  PE=0 SSI=0 SSIPE=0
20220320 062518.034 INFO             PET5 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220320 062518.034 INFO             PET5 main:  PE=1 SSI=0 SSIPE=1
20220320 062518.034 INFO             PET5 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220320 062518.034 INFO             PET5 main:  PE=2 SSI=0 SSIPE=2
20220320 062518.034 INFO             PET5 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220320 062518.034 INFO             PET5 main:  PE=3 SSI=0 SSIPE=3
20220320 062518.034 INFO             PET5 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220320 062518.034 INFO             PET5 main:  PE=4 SSI=0 SSIPE=4
20220320 062518.034 INFO             PET5 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220320 062518.034 INFO             PET5 main:  PE=5 SSI=0 SSIPE=5
20220320 062518.034 INFO             PET5 main: --- VMK::log() end ---------------------------------------
20220320 062518.035 INFO             PET5 Executing 'userm1_setvm'
20220320 062518.035 INFO             PET5 Executing 'userm1_register'
20220320 062518.035 INFO             PET5 Executing 'userm2_setvm'
20220320 062518.035 DEBUG            PET5 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220320 062518.035 DEBUG            PET5 vmkt_create()#198 Pthread: service=1 (0: actual PET, 1: service thread) - PTHREAD_STACK_MIN: 16384 bytes stack_size: 4194304 bytes
20220320 062518.035 INFO             PET5 model1: --- VMK::log() start -------------------------------------
20220320 062518.035 INFO             PET5 model1: vm located at: 0x292c9a0
20220320 062518.035 INFO             PET5 model1: petCount=6 localPet=5 mypthid=46940161916672 currentSsiPe=5
20220320 062518.035 INFO             PET5 model1: Current system level affinity pinning for local PET:
20220320 062518.035 INFO             PET5 model1:  SSIPE=5
20220320 062518.035 INFO             PET5 model1: Current system level OMP_NUM_THREADS setting for local PET: 1
20220320 062518.035 INFO             PET5 model1: ssiCount=1 localSsi=0
20220320 062518.035 INFO             PET5 model1: mpionly=1 threadsflag=0
20220320 062518.035 INFO             PET5 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20220320 062518.035 INFO             PET5 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20220320 062518.035 INFO             PET5 model1:  PE=0 SSI=0 SSIPE=0
20220320 062518.035 INFO             PET5 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20220320 062518.035 INFO             PET5 model1:  PE=1 SSI=0 SSIPE=1
20220320 062518.035 INFO             PET5 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20220320 062518.035 INFO             PET5 model1:  PE=2 SSI=0 SSIPE=2
20220320 062518.036 INFO             PET5 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20220320 062518.036 INFO             PET5 model1:  PE=3 SSI=0 SSIPE=3
20220320 062518.036 INFO             PET5 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20220320 062518.036 INFO             PET5 model1:  PE=4 SSI=0 SSIPE=4
20220320 062518.036 INFO             PET5 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20220320 062518.036 INFO             PET5 model1:  PE=5 SSI=0 SSIPE=5
20220320 062518.036 INFO             PET5 model1: --- VMK::log() end ---------------------------------------
20220320 062518.038 INFO             PET5 Entering 'user1_run'
20220320 062518.038 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220320 062518.816 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220320 062519.502 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220320 062520.188 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220320 062520.874 INFO             PET5  user1_run: on SSIPE:            5  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20220320 062521.561 INFO             PET5 Exiting 'user1_run'
20220320 062525.032 INFO             PET5  NUMBER_OF_PROCESSORS           6
20220320 062525.033 INFO             PET5  PASS  System Test ESMF_FieldSharedDeSSISTest, ESMF_FieldSharedDeSSISTest.F90, line 276
20220320 062525.033 INFO             PET5 Finalizing ESMF
